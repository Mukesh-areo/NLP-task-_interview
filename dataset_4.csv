Title,Abstract,First Claim,Target
Geophysical deep learning,A method can include selecting a type of geophysical data; selecting a type of algorithm; generating synthetic geophysical data based at least in part on the algorithm; training a deep learning framework based at least in part on the synthetic geophysical data to generate a trained deep learning framework; receiving acquired geophysical data for a geologic environment; implementing the trained deep learning framework to generate interpretation results for the acquired geophysical data; and outputting the interpretation results.,"1. A method comprising:
selecting a type of geophysical data;
selecting a type of algorithm;
generating synthetic geophysical data of the selected type of geophysical data based at least in part on the algorithm;
training a deep learning classifier framework based at least in part on the synthetic geophysical data to generate a trained deep learning classifier framework;
receiving acquired geophysical data for a geologic environment;
implementing the trained deep learning classifier framework to generate interpretation results for the acquired geophysical data; and
outputting the interpretation results, wherein the interpretation results characterize structural features indicative of hydrocarbons in the geologic environment.",Deep learning
Deep learning for algorithm portfolios,"Automated feature construction for algorithm portfolios in machine learning is provided. A gray scale image is generated from a text representing a problem instance. The gray scale image is rescaled or reshaped to a predefined size that is smaller than an initial size of the gray scale image. The rescaled gray scale image represents features of the problem instance. The rescaled gray scale image is input as features to a machine learning-based convolutional neural network. Based on the rescaled gray scale image, the machine learning-based convolutional neural network is automatically trained to learn to automatically determine one or more problem solvers from a portfolio of problem solvers suited for solving the problem instance.","1. A computer-implemented method of automated feature construction for algorithm portfolios in machine learning, comprising:
receiving, by one or more processors, a problem instance represented as text describing a problem to be solved by computer-implemented problem solver;
generating, by one or more of the processors, a gray scale image from the text by converting the text into the gray scale image that corresponds to the text;
rescaling, by one or more of the processors, the gray scale image to a predefined size that is smaller than an initial size of the gray scale image, into a rescaled gray scale image, the rescaled gray scale image representing features of the problem instance;
inputting, by one or more of the processors, the rescaled gray scale image as features to a machine learning-based convolutional neural network; and
training based on the rescaled gray scale image, by one or more of the processors, the machine learning-based convolutional neural network to learn to automatically determine one or more problem solvers from a portfolio of problem solvers suited for solving the problem instance,
wherein the receiving, generating, rescaling and inputting are performed for a plurality of problem instances for the training.",Deep learning
Modular deep learning model,"The technology described herein uses a modular model to process speech. A deep learning based acoustic model comprises a stack of different types of neural network layers. The sub-modules of a deep learning based acoustic model can be used to represent distinct non-phonetic acoustic factors, such as accent origins (e.g. native, non-native), speech channels (e.g. mobile, bluetooth, desktop etc.), speech application scenario (e.g. voice search, short message dictation etc.), and speaker variation (e.g. individual speakers or clustered speakers), etc. The technology described herein uses certain sub-modules in a first context and a second group of sub-modules in a second context.","1. An automatic speech recognition (ASR) system for machine understanding of acoustic information, the system comprising:
one or more processors that receive acoustic information comprising speech from a user received by a computing device associated with the user;
a computer-storage media storing an acoustic model configured to identify acoustic units within an acoustic information,
wherein the acoustic model (AM) comprises a modular deep learning model comprising multiple hidden layers, the multiple hidden layers including multiple modular layers comprising multiple context-specific sub-modules, wherein each modular layer is associated with a different context, wherein only a first context-specific sub-module associated with an active context in an individual modular layer processes the acoustic information during ASR, wherein the modular deep learning model also includes one or more common layers between individual modular layers;
a control module comprising one or more computer-storage media storing computer-usable instructions which, when executed by one or more processors, operates to use one or more external signals to determine the active context of the acoustic information when performing ASR;
a model configuration component comprising one or more computer-storage media storing computer-usable instructions which, when executed by one or more processors, operates to configure the acoustic model by activating and deactivating different context-specific sub-modules in the modular deep learning model during ASR based on the active context determined by the control module; and
one or more processors that determine recognized speech for the acoustic information, the recognized speech comprising one or more of words, entities, or phrases, by processing acoustic features of the acoustic information using the modular deep learning model.",Deep learning
Deep learning processing of video,"A method and system for processing multiple frames of a video by a neural network are provided. Two frames of a video may be analyzed to determine if at least a portion of the layer-by-layer processing by a neural network can be skipped or terminated. Processing of a first frame of the video is performed by the neural network. A next frame of the video is processed by the neural network, such that processing of fewer layers (or sets of operations) of the neural network is performed if the first frame and the second frame are substantially similar.","1. A method comprising:
performing a first processing of a first frame of a video by a neural network, wherein the first processing comprises application of a first layer and a second layer of the neural network; and
performing a second processing of a second frame of the video by the neural network, wherein application of the second layer of the neural network is skipped in the second processing if the first frame and the second frame are substantially similar.",Deep learning
System and method for deep learning and wireless network optimization using deep learning,"A neural network is trained using deep reinforcement learning (DRL) techniques for adjusting cell parameters of a wireless network by generating a plurality of experience tuples, and updating the neural network based on the generated experience tuples. The trained neural network may be used to select actions to adjust the cell parameters. Each experience tuple includes a cell identifier, a first state, a second state, an action applied to the cell that moves the cell from the first state to the second state, a local reward, and a global reward. The neural network is updated based on whether or not each action is acceptable, which is determined based on the global reward and the local reward associated with each action.","1. A method comprising:
initializing a neural network with a set of weight values, the neural network being used to determine actions that adjust one or more settings of cells associated with base stations in a wireless network, each base station providing communication services to user equipments (UEs) within one or more cells; and
training the neural network by using a deep reinforcement learning (DRL) process, the DRL processing comprising:
generating a first plurality of experience tuples for a plurality of cells in the wireless network, each experience tuple comprising a cell identifier that identifies a cell, a first state of the cell, a second state of the cell, an action that causes the cell to transit from the first state to the second state, and a reward value for taking the action, wherein a state of a cell comprises a setting of a base station providing a coverage area of the cell, and a reward value is calculated using a cost function based on measurement reports received from UEs in the wireless network, wherein each experience tuple can be a DRL-generated experience tuple in which a respective action is selected by a DRL agent based on the neural network according to a DRL technique or an expert-generated experience tuple in which the respective action is provided based on expert experience, and wherein whether an action is selected by the DRL agent based on the neural network or provided based on the expert experience is determined based on a first criterion; and
selecting a second plurality of experience tuples from the first plurality of experience tuples; and
updating the set of weight values of the neural network according to reward values in the second plurality of experience tuples.",Deep learning
Deep learning automated dermatopathology,"Techniques for classifying a human cutaneous tissue specimen are presented. The techniques may include obtaining a computer readable image of the human tissue sample and preprocessing the image. The techniques may include applying a trained deep learning model to the image to label each of a plurality of image pixels with at least one probability representing a particular diagnosis, such that a labeled plurality of image pixels is obtained. The techniques can also include applying a trained discriminative classifier to contiguous regions of pixels defined at least in part by the labeled plurality of image pixels to obtain a specimen level diagnosis, where the specimen level diagnosis includes at least one of: basal cell carcinoma, dermal nevus, or seborrheic keratosis. The techniques can include outputting the specimen level diagnosis.","1. An at least partially computer implemented method of classifying a human cutaneous tissue specimen, the method comprising:
obtaining a computer readable image of the human tissue sample;
preprocessing the image;
applying a trained deep learning model to the image to label each of a plurality of image pixels with at least one probability representing a particular diagnosis, such that a labeled plurality of image pixels is obtained;
applying a trained discriminative classifier to contiguous regions of pixels defined at least in part by the labeled plurality of image pixels to obtain a specimen level diagnosis, wherein the specimen level diagnosis comprises at least one of:
basal cell carcinoma, dermal nevus, or seborrheic keratosis; and
outputting the specimen level diagnosis.",Deep learning
Quantum deep learning,"Boltzmann machines are trained using an objective function that is evaluated by sampling quantum states that approximate a Gibbs state. Classical processing is used to produce the objective function, and the approximate Gibbs state is based on weights and biases that are refined using the sample results. In some examples, amplitude estimation is used. A combined classical/quantum computer produces suitable weights and biases for classification of shapes and other applications.","1. A method of efficiently training a Boltzmann machine, comprising:
with a classical computer, receiving a specification of a Boltzmann machine, an objective function, and associated training data, and
in a quantum computer, determining at least one gradient of the objective function by:
based on the specification of the Boltzmann machine, preparing a plurality of qubits to represent a Gibbs distribution;
adding a qubit to the plurality of qubits and applying a rotation operator so that the added qubit has amplitudes of first and second states based on a joint probability distribution of hidden and visible unit values;
producing the at least one gradient of the objective function by sampling the states of each of the plurality of qubits; and
based on the at least one gradient of the objective function, specifying at least one visible bias, at least one hidden bias or at least one weight of the Boltzmann machine so as to produce a trained Boltzmann machine.",Deep learning
Deep learning application distribution,"In one embodiment, a method includes training a deep neural network using a first set of network characteristics corresponding to a first time and a second set of network characteristics corresponding to a second time, generating, using the deep neural network, a predictive set of network characteristics corresponding to a future time, and assigning a task of a distributed application to a processing unit based on the predictive set of network characteristics.","1. A method, comprising:
receiving, at a controller in a local area network (LAN), a first set of network characteristics and a second set of network characteristics that each comprise respective network measurements associated with a plurality of devices in the LAN that execute a distributed application, wherein the first set of network characteristics correspond to a first time and the second set of network characteristics correspond to a second time;
training, by the controller, a deep neural network by using the first set of network characteristics and the second set of network characteristics as inputs for the deep neural network, wherein the deep neural network implements multimodal learning using a plurality of deep neural network layers;
generating, by the controller and using the deep neural network, a predictive set of network characteristics of the distributed application corresponding to a future time based on interrelationships identified between features generated from the first set of network characteristics and the second set of network characteristics, wherein the predictive set of network characteristics are indicative of availability of network bandwidth, network latency, or network throughput of one device of the plurality of devices in the LAN at the future time; and
causing, by the controller, an assignment of a task of the distributed application to one or more devices in the LAN based on the predictive set of network characteristics.",Deep learning
Backpressure for accelerated deep learning,"Techniques in advanced deep learning provide improvements in one or more of accuracy, performance, and energy efficiency. An array of processing elements performs flow-based computations on wavelets of data. Each processing element comprises a respective compute element and a respective routing element. Each compute element comprises virtual input queues. Each router enables communication via wavelets with at least nearest neighbors in a 2D mesh. Routing is controlled by respective virtual channel specifiers in each wavelet and routing configuration information in each router. Each router comprises data queues. The virtual input queues of the compute element and the data queues of the router are managed in accordance with the virtual channels. Backpressure information, per each of the virtual channels, is generated, communicated, and used to prevent overrun of the virtual input queues and the data queues.","1. A method comprising:
managing a plurality of virtual input queues of a first processing element having a first coupling to a fabric, each virtual input queue enabled to store a respective number of fabric packets received via the fabric, the first coupling associated with a plurality of fabric virtual channels each associated with one of the virtual input queues;
managing a plurality of backpressure indicators, each backpressure indicator associated with a respective one of the fabric virtual channels and enabled to selectively indicate one of a stall state and a ready state, each respective backpressure indicator set to the ready state when the virtual input queue associated with the fabric virtual channel associated with the respective backpressure indicator holds less than a respective threshold number of fabric packets and otherwise set to the stall state;
transmitting the backpressure indicators via the first coupling to the fabric and through routing paths enabled by the fabric and determinable at least in part by referencing information identified by fabric virtual channel identifiers respectively associated with each of the fabric virtual channels and wherein each of the fabric packets comprises a respective instance of one of the fabric virtual channel identifiers;
receiving the backpressure indicators via the fabric at a second processing element, the second processing element comprising a second coupling to the fabric, the second coupling associated with the fabric virtual channels; and
in the second processing element and responsive to ones of the received backpressure indicators in the stall state, stalling transmission associated with the fabric virtual channels associated with the ones of the received backpressure indicators in the stall state, the stalling transmission being with respect to fabric packets from the second processing element destined for the first processing element.",Deep learning
Text recognition and localization with deep learning,"Approaches provide for identifying text represented in image data as well as determining a location or region of the image data that includes the text represented in the image data. For example, a camera of a computing device can be used to capture a live camera view of one or more items. The live camera view can be presented to the user on a display screen of the computing device. An application executing on the computing device or at least in communication with the computing device can analyze the image data of the live camera view to identify text represented in the image data as well as determine locations or regions of the image that include the representations. For example, one such recognition approach includes a region proposal process to generate a plurality of candidate bounding boxes, a region filtering process to determine a subset of the plurality of candidate bounding boxes, a region refining process to refine the bounding box coordinates to more accurately fit the identified text, a text recognizer process to recognize words in the refined bounding boxes, and a post-processing process to suppress overlapping bounding boxes to generate a final set of bounding boxes.","1. A computer-implemented method, comprising:
obtaining image data that includes representations of text;
determining a plurality of regions of interest, a first set of the plurality of regions of interest including the representations of text and a second set of the plurality of regions of interest including potential representations of text;
using a first trained neural network to identify the first set of the plurality of regions of interest;
using a second trained neural network to update a position of each region of the first set of the plurality of regions of interest to include respective text representations within a predetermined deviation;
using a third trained neural network on the first set of the plurality of regions of interest to recognize words associated with respective regions based at least in part on the respective text representations;
receiving a selection of one of a recognized word from the list of recognized words;
causing a query to be executed against a data source, the query including the recognized word;
receiving, in response to the query, result information for a set of items, the set of items determined by comparing the word to a library of words, each word in the library of words associated with at least one item; and
generating a list of recognized words.",Deep learning
Image steganalysis based on deep learning,"The present invention provides a method for detecting image steganography based on deep learning, which comprises: filtering images having steganographic class label or true class label in a training set with a high-pass filter to obtain a training set including steganographic class residual images and true class residual images; training a deep network model on said training set to obtain a trained deep model for steganalysis; filtering the image to be detected with said high-pass filter to obtain a residual image to be detected; detecting said residual image to be detected on said deep model so as to determine whether said residual image to be detected is a steganographic image. The method for detecting image steganography in the present invention can create an automatic blind steganalysis model through feature learning and can identify steganographic images accurately.","1. A method for detecting image steganography based on deep learning, characterized by comprising:
filtering images having steganographic class label or true class label in a training set with a high-pass filter to obtain a training set including steganographic class residual images and true class residual images;
training a deep network model on said training set to obtain a trained deep network for steganalysis;
filtering the image to be detected with said high-pass filter to obtain a residual image to be detected;
detecting said residual image to be detected on said trained deep network so as to determine whether said residual image to be detected is a steganographic image.",Deep learning
Systems and methods for deep learning microscopy,"A microscopy method includes a trained deep neural network that is executed by software using one or more processors of a computing device, the trained deep neural network trained with a training set of images comprising co-registered pairs of high-resolution microscopy images or image patches of a sample and their corresponding low-resolution microscopy images or image patches of the same sample. A microscopy input image of a sample to be imaged is input to the trained deep neural network which rapidly outputs an output image of the sample, the output image having improved one or more of spatial resolution, depth-of-field, signal-to-noise ratio, and/or image contrast.","1. A microscopy method comprising:
providing a trained deep neural network that is executed by software using one or more processors of a computing device, the trained deep neural network trained with a training set of non-fluorescence images comprising co-registered pairs of high-resolution microscopy images or image patches of a sample and their corresponding low-resolution microscopy images or image patches of the same sample;
inputting a non-fluorescence microscopy input image of a second sample to the trained deep neural network;
outputting an output image of the second sample from the trained deep neural network, the output image having improved one or more of spatial resolution, depth-of-field, signal-to-noise ratio, and/or image contrast; and
wherein the non-fluorescence microscopy input image comprises one of a bright-field microscopy image, a holographic microscopy image, a dark-field microscopy image, and an optical coherence tomography (OCT) image.",Deep learning
Multi-scale deep learning system,"A system for identifying objects in an image is provided. The system identifies segments of an image that may contain objects. For each segment, the system generates a segment score by inputting to a multi-scale neural network windows of multiple scales that include the segment that have been resampled to a fixed window size. A multi-scale neural network includes a feature extracting convolutional neural network (â€œfeCNNâ€) for each scale and a classifier that inputs each feature of each feCNN. The segment score indicates whether the segment contains an object. The system generates a pixel score for pixels of the image. The pixel score for a pixel indicates that that pixel is within an object based on the segment scores of segments that contain that pixel. The system then identifies the object based on the pixel scores of neighboring pixels.","1. A method performed by a computing system, the method comprising, for each of a plurality of segments of an image:
identifying a bounding box for the segment;
for each of a plurality of window sizes,
extracting a window of that window size that includes the bounding box;
resampling the extracted window to a resampled window of a fixed window size; and
providing the resampled window of the fixed window size for the segment as input to a feature extracting convolutional neural network (â€œfeCNNâ€) for that window size;
executing the feCNNs to extract features for each resampled window; and
executing a classifier with the extracted features of the feCNNs as inputs to generate a segment probability that the segment relates to an object of interest.",Deep learning
Image matting using deep learning,"Methods and systems are provided for generating mattes for input images. A neural network system can be trained where the training includes training a first neural network that generates mattes for input images where the input images are synthetic composite images. Such a neural network system can further be trained where the training includes training a second neural network that generates refined mattes from the mattes produced by the first neural network. Such a trained neural network system can be used to input an image and trimap pair for which the trained system will output a matte. Such a matte can be used to extract an object from the input image. Upon extracting the object, a user can manipulate the object, for example, to composite the object onto a new background.","1. A computer-implemented method for training a neural network system to generate mattes for images, the method comprising:
inputting, into a first neural network, a training image and trimap pair, the trimap indicating a blended region in which one or more pixels include an unknown combination of foreground and background colors;
determining, by the first neural network, a percentage of foreground color for each of the one or more pixels in the blended region, the first neural network using color, structure, and texture information to determine percentages of foreground color in association with the one or more pixels;
generating, by the first neural network, a training matte for the training image using the percentages of foreground color for each of the one or more pixels in the blended region;
identifying, by the first neural network, error associated with the training matte using a loss function; and
adjusting the first neural network based on the identified error associated with the training matte.",Deep learning
Video frame synthesis with deep learning,"The present disclosure provides systems and methods that leverage machine-learned models (e.g., neural networks) to provide video frame synthesis. In particular, the systems and methods of the present disclosure can include or otherwise leverage a machine-learned video frame synthesis model to allow for video frames to be synthesized from videos. In one particular example, the video frame synthesis model can include a convolutional neural network having a voxel flow layer and provides one or more synthesized video frames as part of slow-motion video.","1. A computer-implemented method for video frame synthesis, the method comprising:
receiving, by one or more computing devices, a video;
inputting, by the one or more computing devices, a first set of sequential frame data descriptive of the video into a machine-learned video frame synthesis model, wherein the machine-learned video frame synthesis model comprises at least one convolutional neural network having a voxel flow layer, the at least one convolutional neural network is at least partially trained using unsupervised learning to predict a voxel flow field comprising a per-pixel, 3D optical flow vector across space and time;
receiving, by the one or more computing devices, one or more synthesized frames from the video, the one or more synthesized frames output by the machine-learned video frame synthesis model; and
providing, by the one or more computing devices, information regarding the one or more synthesized frames.",Deep learning
Deep learning for tooth detection and evaluation,"A machine learning model is trained to define bounding shapes around teeth in images. The machine learning model is trained by receiving a training dataset comprising a plurality of images, each image of the plurality of images comprising a face and a provided bounding shape around teeth in the image. The training dataset is input into an untrained machine learning model. The untrained machine learning model is trained based on the training dataset to generate a trained machine learning model that defines bounding shapes around teeth in images, wherein for an input image the trained machine learning model is to output a mask that defines a bounding shape around teeth of the input image, wherein the mask indicates, for each pixel of the input image, whether that pixel is inside of a defined bounding shape or is outside of the defined bounding shape.","1. A method comprising:
inputting a training dataset into an untrained machine learning model;
training the untrained machine learning model based on the training dataset to generate a trained machine learning model that defines bounding shapes around teeth depicted in images of faces of patients;
receiving an image of a face of a patient, the image including a depiction of teeth of the patient;
processing the image of the face using the trained machine learning model;
associating a first bounding shape with a first plurality of teeth of the teeth depicted in the image of the face of the patient using the trained machine learning model, wherein the first bounding shape comprises an object bounding the first plurality of teeth of the patient;
determining, for each pixel in the image of the face of the patient, whether the pixel is associated with a first region inside of the first bounding shape or a second region outside of the first bounding shape;
generating a mask for the image of the face of the patient, wherein each entry in the mask is associated with a specific pixel in the image and indicates for the specific pixel whether the specific pixel is associated with the first region inside of the first bounding shape or the second region outside of the first bounding shape; and
cropping the image of the face of the patient based on the mask to remove the second region outside of the first bounding shape, wherein the cropped image comprises depictions of the first plurality of teeth.",Deep learning
Automated measurement based on deep learning,"A framework for automated measurement. In accordance with one aspect, the framework detects a centerline point of a structure of interest in an image. A centerline of the structure of interest may be traced based on the detected centerline point. A trained segmentation learning structure may be used to generate one or more contours of the structure of interest along the centerline. One or more measurements may then be extracted from the one or more contours.","1. A system for automated measurement, comprising:
a non-transitory memory device for storing computer readable program code; and
a processor device in communication with the memory device, the processor being operative with the computer readable program code to perform steps including
receiving an image of an aorta,
detecting a centerline point of the aorta in the image,
tracing a centerline of the aorta using the detected centerline point as a starting point,
generating, by a trained segmentation learning structure, one or more contours of cross-sections of the aorta along the centerline, wherein the trained segmentation learning structure comprises a deep learning architecture trained by two-dimensional cross-sectional images of the aorta,
extracting one or more measurements from the one or more contours, and
presenting the one or more measurements in a graphical user interface.",Deep learning
Approximate synchronization for parallel deep learning,"Techniques facilitating synchronization of processing engines for parallel deep learning are provided. In one example, a first processing component associated with a processor and processing components can: generate first output data based on input data associated with a machine learning process, wherein the processing components are communicatively coupled with an assignment component via a network; transmit the first output data to a second processing component of the processing components, wherein the first processing component and the second processing component comprise a first group of the processing components and the first group of the processing components is determined by the assignment component based on a first defined criterion; receive communication data generated by the second processing component; and generate second output data based on the communication data, wherein the second output data is an updated version of the first output data stored in the memory of the first processing component.","1. A computer program product for deep learning, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a processing component to cause the processing component to:
generate, by the processing component, first output data based on first input data associated with machine learning and received by the processing component and one or more other processing components;
transmit, by the processing component, the first output data to a first processing component from the one or more other processing components, wherein the first processing component is determined by the processing component or the one or more other processing components, wherein the processing component and the first processing component are operated in synchronization for the deep learning, and wherein the first output data is stored in a memory operatively coupled to the processing component;
generate, by the processing component, second output data based on the first output data and communication data that is generated by the first processing component; and
transmit, by the processing component, the second output data to a second processing component from the processing component, wherein the second processing component is determined by the processing component or the one or more other processing components, and wherein the processing component and the second processing component are operated in synchronization for the deep learning.",Deep learning
Tomographic reconstruction based on deep learning,"The present approach relates to the use of machine learning and deep learning systems suitable for solving large-scale, space-variant tomographic reconstruction and/or correction problems. In certain embodiments, a tomographic transform of measured data obtained from a tomography scanner is used as an input to a neural network. In accordance with certain aspects of the present approach, the tomographic transform operation(s) is performed separate from or outside the neural network such that the result of the tomographic transform operation is instead provided as an input to the neural network. In addition, in certain embodiments, one or more layers of the neural network may be provided as wavelet filter banks.","1. A method, comprising:
obtaining measured data from a tomography scanner;
calculating one or more tomographic transforms of the measured data, wherein the one or more tomographic transforms comprise at least one of a backprojection, a weighted backprojection, a reprojection, a plurality of diagonal elements of a Fisher information matrix, a variance image, a noise correlation image, or a polynomial of the Fisher information matrix;
providing inputs to a trained neural network, wherein the inputs comprise the one or more tomographic transforms and an input reconstructed image, and the input reconstructed image comprises a filtered backprojection image with a special filter kernel, wherein the filter coefficients are selected that preserve the original sinogram after a reprojection; and
obtaining one or more outputs from the trained neural network based on the inputs.",Deep learning
Scalable deep learning video analytics,"In one embodiment, a method includes receiving training data, the training data including training video data representing video of a location in a quiescent state, training a neural network using the training data to obtain a plurality of metrics, receiving current data, the current data including current video data representing video of the location at a current time period, generating a reconstruction error based on the plurality of metrics and the current video data in the embedded space, and generating, in response to determining that the reconstruction error is greater than a threshold, a notification indicative of the location being in a non-quiescent state.","1. A method comprising:
receiving training data, the training data including training video data representing video of a location in a quiescent state;
training a neural network using the training data to obtain a plurality of metrics;
receiving current data, the current data including current video data representing video of the location at a current time period;
generating a reconstruction error based on the plurality of metrics and the current video data; and
generating, in response to determining that the reconstruction error is greater than a threshold, a notification indicative of the location being in a non-quiescent state.",Deep learning
Deep learning bias detection in text,"In one embodiment, a method includes obtaining text from a user, applying the text to a deep learning neural network to generate a plurality of bias coordinates defining a point in an embedded space, and, in response to determining that at least one of the plurality of bias coordinates exceeds a threshold, providing an indication of bias to the user.","1. A method comprising:
obtaining text from a user;
applying the text to a deep learning neural network to generate a plurality of bias coordinates defining a point in an embedded space, wherein the embedded space is a multi-dimensional vector space defined by the plurality of bias coordinates, each dimension of the embedded space corresponding to a respective bias and a size of each bias coordinate corresponding to an amount of bias; and
in response to determining that at least one of the plurality of bias coordinates satisfies a bias threshold, providing an indication of bias to the user.",Deep learning
Active view planning by deep learning,"A system and method that identifies an object and a viewpoint from an image with a probability that satisfies a predefined criterion is described. An active view planning application receives a first image, performs recognition on the first image to determine an object, a viewpoint and a probability of recognition, determines a first expected gain in the probability of recognition when a first action is taken and a second expected gain in the probability of recognition when a second action is taken, and identifies a next action from the first action and the second action based on an increase in expected gains.","1. A computer-implemented method comprising:
receiving, by a computing device, a first image;
performing, by the computing device, recognition on the first image using a deep neural network;
determining, by the computing device, a probability of recognition for an object based on performing the recognition on the first image, the probability of recognition for the object identifying an extent of certainty about the image including the object and being captured at a first viewpoint;
determining, by the computing device, whether the probability of recognition for the object satisfies a predetermined threshold;
responsive to determining that the probability of recognition for the object does not satisfy the predetermined threshold, determining, by the computing device, a first expected gain in the probability of recognition when a first action is taken and a second expected gain in the probability of recognition when a second action is taken, the first action and the second action belonging to a set of actions describing receiving a second image for increasing the probability of recognition; and
identifying a next action from the first action and the second action based on an increase in expected gains.",Deep learning
Deep learning-based store realograms,"Systems and techniques are provided for tracking inventory items in an area of real space. A plurality of cameras, or other sensors, produce respective sequences of images in corresponding fields of view in the real space. The field of view of each camera overlaps with the field of view of at least one other camera. The system is coupled to the plurality of cameras and uses the sequences of images produced by at least two cameras in the plurality of cameras to identify inventory events. The inventory event includes an item identifier, a location and a timestamp. A plurality of cells having coordinates in the area of real space are stored as a data set in the memory. The processing system calculates scores at a scoring time, for inventory items having locations matching particular cells using respective counts of inventory events.","1. A system for tracking inventory items in an area of real space, comprising:
a plurality of sensors, sensors in the plurality of sensors producing respective sequences of images of corresponding fields of view in the real space, the field of view of each sensor overlapping with the field of view of at least one other sensor in the plurality of sensors; and
a processing system coupled to the plurality of sensors, the processing system including logic that uses the sequences of images produced by at least two sensors in the plurality of sensors to identify inventory events, and in response to the inventory events, to track locations of inventory items in the area of real space, logic to match locations of inventory items with coordinates of cells in a plurality of cells having coordinates in the area of real space, and to maintain a data representing inventory items matched with cells in the plurality of cells, and logic that calculates scores at a scoring time for inventory items having locations matching particular cells, the scores based on counts of inventory events.",Deep learning
Methods for explainability of deep-learning models,"Embodiments are disclosed for health assessment and diagnosis implemented in an artificial intelligence (AI) system. In an embodiment, a method comprises: feeding a first set of input features to the AI model; obtaining a first set of raw output predictions from the model; determining a first set of impact scores for the input features fed into the model; training a neural network with the first set of impact scores as input to the network and pre-determined sentences describing the model's behavior as output; feeding a second set of input features to the AI model; obtaining a second set of raw output predictions from the model; determining a second set of impact scores based on the second set of output predictions; feeding the second set of impact scores to the neural network; and generating a sentence describing the AI model's behavior on the second set of input features.","1. A method of interpreting an artificial intelligence (AI) model prediction, comprising:
feeding a first set of input features to the AI model;
obtaining, using one or more processors, a first set of raw output predictions from the model;
determining, using the one or more processors, a first set of impact scores for one or more of the input features fed into the model;
training a neural network with the first set of impact scores as input to the network and pre-determined sentences describing the model's behavior as output;
feeding a second set of input features to the AI model;
obtaining, using one or more processors, a second set of raw output predictions from the model;
determining a second set of impact scores based on the second set of output predictions;
feeding the second set of impact scores to the neural network; and
generating a sentence describing the AI model's behavior on the second set of input features.",Deep learning
Deep learning algorithms for heartbeats detection,"A system that detects heartbeats includes a sensor or a transducer and algorithms based on deep learning. The algorithms employ techniques of artificial intelligence that enable the system to extract heartbeat features under low signal-to-noise-ratio (SNR) conditions when a user is exercising. The algorithms can be applied to various technologies for heart rate monitoring such as ultrasound Doppler, photoplethysmogram (PPG), electrocardiogram (EKG), acoustic, pressure/force sensing and laser/RF Doppler, among other types of sensing methods.","1. A method for determining a heart rate of a subject, comprising the steps of:
obtaining temporal sensor data generated by a sensor worn by the subject, the temporal sensor data represented by a plurality of frames of data samples;
generating, for each frame of the plurality of frames, a feature vector based on a frequency domain representation of the corresponding data samples of the frame;
inputting the feature vectors to a scenario classifier to determine a scenario of a plurality of scenarios for the temporal sensor data, wherein each of the plurality of scenarios corresponds to a different signal-to-noise level;
identifying a model from a plurality of trained models according to the determined scenario;
inputting the feature vectors to the model to generate a series of heartbeat determinations of the subject for each frame of the plurality of frames, wherein the model comprises a first set of parameters and associated weights that represent likelihoods of heartbeats of the subject as a function of time; and
providing the heart rate of the subject determined based on the series of heartbeat determinations.",Deep learning
Crop grading via deep learning,"Methods and systems for crop grading and crop management. One or more images of crops are obtained and one or more crop related features are at least one of identified or extracted from the one or more images. A crop health status is determined based on the one or more crop related features, an environmental context, a growth stage of the crop, and a farm cohort by using a computerized deep learning system to perform an automated growth stage analysis. One or more actions are at least one of recommended, triggered, and performed.","1. A method for generating or updating a crop blueprint, the method comprising:
obtaining one or more images of crops;
extracting one or more crop related features from the one or more images;
determining a crop health status based on the one or more crop related features, an environmental context, a growth stage of the crop, and a farm cohort by using a computerized deep learning system to perform an automated growth stage analysis;
predicting a quality of the crops within a threshold risk R based on the one or more crop related features, the threshold risk R being based on a comparison of the predicted quality and an expected quality for a corresponding growth stage; and
at least one of recommending, triggering, and performing one or more actions;
wherein the obtaining one or more images of crops is repeated at a frequency based on a rate of change of leaf color and wherein the extracting one or more crop related features from the one or more images is based on one or more machine learning methods that use deep learning to learn a meaning of a rate of change of crop color based on historical images and based on data from a crop knowledge graph.",Deep learning
Activation layers for deep learning networks,"Tasks such as object classification from image data can take advantage of a deep learning process using convolutional neural networks. These networks can include a convolutional layer followed by an activation layer, or activation unit, among other potential layers. Improved accuracy can be obtained by using a generalized linear unit (GLU) as an activation unit in such a network, where a GLU is linear for both positive and negative inputs, and is defined by a positive slope, a negative slope, and a bias. These parameters can be learned for each channel or a block of channels, and stacking those types of activation units can further improve accuracy.","1. A computer-implemented method, comprising:
training a convolutional neural network using a set of a training data, the training data including instances of data with determined classifications;
receiving a query from a client device associated with a user;
processing the query using the trained convolutional neural network to determine a classification of a data object represented in the query, the trained convolutional neural network containing at least one convolutional layer and at least one activation layer, the at least one activation layer including a generalized linear unit, the generalized linear unit having a functional form described using a pair of straight lines with three parameters including a first slope in a positive region, a second slope in a negative region, and an offset applied to the first slope and the second slope, the three parameters learnable over at least one input channel;
determining a set of features corresponding to the classification; and
providing, to the client device, information for at least a subset of the set of features.",Deep learning
Deep learning system for cuboid detection,"Systems and methods for cuboid detection and keypoint localization in images are disclosed. In one aspect, a deep cuboid detector can be used for simultaneous cuboid detection and keypoint localization in monocular images. The deep cuboid detector can include a plurality of convolutional layers and non-convolutional layers of a trained convolution neural network for determining a convolutional feature map from an input image. A region proposal network of the deep cuboid detector can determine a bounding box surrounding a cuboid in the image using the convolutional feature map. The pooling layer and regressor layers of the deep cuboid detector can implement iterative feature pooling for determining a refined bounding box and a parameterized representation of the cuboid.","1. A system for cuboid detection and keypoint localization comprising:
non-transitory memory configured to store:
executable instructions,
an image for cuboid detection, and
a cuboid detector comprising:
a plurality of convolutional layers and non-convolutional layers of a first convolutional neural network (CNN) for generating a convolutional feature map from the image,
a region proposal network (RPN) comprising a second CNN for determining, using the convolutional feature map, at least one region of interest (RoI) comprising a cuboid at a cuboid image location of the image, and
a pooling layer and at least one regressor layer for determining, using the convolutional feature map and the RoI comprising the cuboid, a refined RoI at a refined cuboid image location and a representation of the cuboid;
a hardware processor in communication with the non-transitory memory, the hardware processor programmed by the executable instructions to:
receive the image;
generate, using the plurality of convolutional layers and the non-convolutional layers of the first CNN and the image, the convolutional feature map;
determine, using the RPN, the at least one RoI comprising the cuboid at the cuboid image location of the image;
determine, using the pooling layer and the cuboid image location, a submap of the convolutional feature map corresponding to the RoI comprising the cuboid; and
determine, using the at least one regressor layer and the submap of the convolutional feature map corresponding to the RoI comprising the cuboid, the refined RoI at the refined cuboid image location and the representation of the cuboid.",Deep learning
Systems and methods for deep learning processor,"A hardware-based programmable deep learning processor (DLP) is proposed, wherein the DLP comprises with a plurality of accelerators dedicated for deep learning processing. Specifically, the DLP includes a plurality of tensor engines configured to perform operations for pattern recognition and classification based on a neural network. Each tensor engine includes one or more matrix multiplier (MatrixMul) engines each configured to perform a plurality of dense and/or sparse vector-matrix and matrix-matrix multiplication operations, one or more convolutional network (ConvNet) engines each configured to perform a plurality of efficient convolution operations on sparse or dense matrices, one or more vector floating point units (VectorFPUs) each configured to perform floating point vector operations, and a data engine configured to retrieve and store multi-dimensional data to both on-chip and external memories.","1. A hardware-based programmable deep learning processor (DLP), comprising:
an on-system memory (OSM) and one or more controllers configured to access a plurality of external memory resources via direct memory access (DMA);
a plurality of programmable tensor engines configured to perform a plurality of operations on input data to generate deep learning processing results for pattern recognition and classification based on a neural network, wherein at least one or more programmable tensor engine of the plurality of tensor engines further comprises a plurality types of hardware engines to accelerate the operations on data at one or more layers of the neural network, wherein the types of hardware engines include:
one or more matrix multiplier engines configured to perform a plurality of dense and/or sparse vector-matrix and matrix-matrix multiplication operations, and wherein the one or more of the matrix multiplier engines is configured to reduce the number of times the input data and a weight matrix need to be read and/or the output matrix needs to be written at the one or more layers of the neural network, wherein in a matrix-matrix multiplication operation an input matrix associated with the input data is N rows by M columns and the weight matrix associated therewith is M rows by K columns, and wherein a T row by a T column submatrix of the input matrix is multiplied by a T row by a T column submatrix of the weight matrix, and wherein the input matrix is read K/T times and wherein the weight matrix is read N/T times;
one or more convolutional network engines configured to perform a plurality of convolution operations by applying a function to increase a sparsity of the vectors and/or matrices, and wherein the one or more convolutional network engines is configured to reduce a number of computations on zero values of the vectors and/or matrices;
one or more vector floating point units configured to perform a vector operation in floating point format;
a data engine configured to prefetch the input data from the OSM and/or the external memory resources.",Deep learning
"Deep learning based self-driving car, deep learning based self-driving control device, and deep learning based self-driving control method","The present embodiments relate to a deep learning-based self-driving vehicle, a deep learning-based self-driving control device, and a deep learning-based self-driving control method, and more particularly, to a deep learning-based self-driving vehicle, a deep learning-based self-driving control device, and a deep learning-based self-driving control method which are capable of reliably performing self-driving control to a necessary degree in a necessary situation by accurately distinguishing between and recognizing a control target object referenced while the self-driving vehicle is traveling and a structure not referenced.","1. A deep learning-based self-driving control device comprising:
a deep learning algorithm execution unit configured to output a result of executing a deep learning algorithm on a monitoring signal obtained by monitoring a periphery of a vehicle;
a self-driving control unit configured to, when the vehicle is self-driving, control the self-driving of the vehicle on the basis of the result of executing the deep learning algorithm; and
a deep learning processing unit configured to change the deep learning algorithm with reference to driver driving information,
wherein the monitoring signal is input to an input layer of a deep neural network and the result of executing the deep learning algorithm corresponds to values of a plurality of output nodes of an output layer of the deep neural network,
wherein the deep neural network includes the input layer having a plurality of input nodes, the output layer, a hidden layer having a plurality of connection nodes for connecting the plurality of input nodes to the plurality of output nodes, and a plurality of weight lines for connecting the plurality of input nodes to the plurality of connection nodes and for connecting the plurality of connection nodes to the plurality of output nodes.",Deep learning
DEEP LEARNING MODEL SCHEDULING,"Systems, methods, and computer-executable instructions for determining a computation schedule for a recurrent neural network (RNN). A matrix multiplication (MM) directed-acyclic graph (DAG) is received for the RNN. Valid phased computation schedules for the RNN are generated. Each of the valid phase computation schedule includes an ordering of MM operations. For each of the plurality of valid phased computation schedules, each of the MM operations is partitioned to processor cores based on L3 cache to L2 cache data movement. The RNN is executed based on the valid phased computation schedules. A final computation schedule is stored. The final computation schedule is used for future executions of the RNN.","1. A method for determining a computation schedule for a recurrent neural network (RNN), the method comprising:
receiving a matrix multiplication (MM) directed-acyclic graph (DAG) that models for computations of the RNN;
generating a plurality of valid phased computation schedules for the RNN from the MM-DAG, wherein each of the valid phase computation schedule includes an ordering of MM operations;
partitioning, for each of the plurality of valid phased computation schedules, each of the MM operations to a plurality of processor cores based on L3 cache to L2 cache data movement;
executing, for each of the plurality of valid phased computation schedules, the RNN based on the partitioning; and
storing a final computation schedule based on the executing, wherein the final computation schedule is used for subsequent executions of the RNN, and wherein the plurality of valid phased computation schedules comprises the final computation schedule.",Deep learning
Deep learning allergen mapping,"An entry on an allergen map may be generated by a computer system where a deep learning model is trained using online content data. Allergen content data which contains geographic data may be detected from the online content data. The allergen content data may be analyzed by the computer system and tagged with a quality and intensity indicator. Based on the tagging and the geographic location, an allergen map may be generated.","1. A method comprising:
providing, from a shared pool of configurable computing resources, to at least one computer processor circuit executing an artificial neural network, a set of known online content data, wherein the artificial neural network is trained using the known online content data, and wherein the known online content data comprises a set of known allergen data;
detecting, by the at least one computer processor circuit executing the artificial neural network, from a set of incoming online content data, allergen content data, wherein the allergen content data comprises a geographic location of a smart phone with an enabled global positioning service (GPS) feature, wherein the allergen content data includes a photograph taken by the smart phone, wherein the photograph includes a flower in a background of the photograph and detected by the artificial neural network using object recognition;
analyzing, by the at least one computer processor circuit executing the artificial neural network, the allergen content data;
tagging, by the at least one computer processor circuit executing the artificial neural network and based on the analyzing, the allergen content data, wherein the tagging comprises a quality indicator and an intensity indicator, wherein the quality indicator comprises a pollen associated with the flower in the background of the photograph, wherein the intensity indicator is based on a blooming season of the flower in the background of the photograph;
generating, by the at least one computer processor circuit executing the artificial neural network and based on the tagging and the geographic location, an entry on an allergen map;
filtering, by the at least one computer processor circuit executing the artificial neural network, the allergen map based on a profile of a user to show allergen entries having a first quality indicator and a first intensity indicator, wherein the profile of the user comprises a location of the user and allergen sensitivities of the user, the allergen sensitivities including the first quality indicator and the first intensity indicator;
displaying, by the at least one computer processor circuit executing the artificial neural network, the filtered allergen map on a device of the user; and
providing, by the at least one computer processor circuit executing the artificial neural network, a recommendation to the device based on the filtered allergen map.",Deep learning
Deep learning for classification of swallows,"A method of classifying a swallow of a subject includes obtaining vibration data that is based on and indicative of a number of vibrations resulting from the swallow, and using a computer implemented deep learning classifier to classify the swallow based on the vibration data. Also, a system for classifying a swallow of a subject includes a computing device implementing a deep learning classifier. The computing device includes a processor apparatus structured and configured to receive vibration data that is based on and indicative of a number of vibrations resulting from the swallow, and use the deep learning classifier to classify the swallow based on the vibration data. The deep learning classifier may comprise a single layer or a multi-layer Deep Belief network.","1. A method of classifying a swallow of a subject, comprising:
obtaining vibration data, the vibration data being based on and indicative of a number of vibrations resulting from the swallow; and
using a computer implemented deep learning classifier to classify the swallow based on the vibration data;
wherein the vibration data is dual-axis data indicative of vibrations resulting from the swallow in a first axis and a second axis, wherein the vibration data comprises first axis vibration data indicative of the vibrations in the first axis and second axis vibration data indicative of the vibrations in the second axis, and wherein the deep learning classifier comprises a first Deep Belief network, a second Deep Belief network, and a combination network, wherein the first Deep belief network uses the first axis vibration data and the second Deep Belief network uses the second axis vibration data.",Deep learning
Sensor fusion and deep learning,Some embodiments provide a sensor data-processing system which detects and classifies objects detected in an environment via fusion of sensor data representations generated by multiple separate sensors. The sensor data-processing system can fuse sensor data representations generated by multiple sensor devices into a fused sensor data representation and can further detect and classify features in the fused sensor data representation. Feature detection can be implemented based at least in part upon utilizing a feature-detection model generated via one or more of deep learning and traditional machine learning. The sensor data-processing system can adjust sensor data processing of representations generated by sensor devices based on external factors including indications of sensor health and environmental conditions. The sensor data-processing system can be implemented in a vehicle and provide output data associated with the detected objects to a navigation system which navigates the vehicle according to the output data.,"1. An apparatus, comprising:
a sensor data-processing system comprising one or more processors and memory configured to:
preprocess a plurality of sensor data representations of an environment, generated by a plurality of separate sensor devices, to combine the plurality of sensor data representations into a fused sensor data representation of the environment;
after combining the sensor data representations, detect a feature associated with at least one object in the fused sensor data representation of the environment;
identify one or more object classifications for the feature as detected in the fused sensor data representation;
detect the feature associated with the at least one object in respective ones of the plurality of sensor data representations of the environment;
determine corresponding one or more classification confidence values for the one or more object classifications based on the feature as detected in the respective ones of the plurality of sensor data representations; and
classify the at least one object according to the one or more object classifications and the one or more classification confidence values.",Deep learning
Deep Learning Training System,"Training large neural network models by providing training input to model training machines organized as multiple replicas that asynchronously update a shared model via a global parameter server is described herein. In at least one embodiment, a system including a model module storing a portion of a model and a deep learning training module that communicates with the model module are configured for asynchronously sending updates to shared parameters associated with the model. The techniques herein describe receiving and processing a batch of data items to calculate updates. Replicas of training machines communicate asynchronously with a global parameter server to provide updates to a shared model and return updated weight values. The model may be modified to reflect the updated weight values. The techniques described herein include computation and communication optimizations that improve system efficiency and scaling of large neural networks.","1. A system comprising:
a computer-readable media storing at least two modules;
a processing unit operably coupled to the computer-readable media, the processing unit adapted to execute the at least two modules, the at least two modules comprising:
a model module configured to store a portion of a model; and
a deep learning training module configured to communicate with the model module and asynchronously sending updates to parameters shared by the model.",Deep learning
DISTRIBUTED DEEP LEARNING DEVICE AND DISTRIBUTED DEEP LEARNING SYSTEM,"A distributed deep learning device that exchanges a quantized gradient with a plurality of learning devices and performs distributed deep learning, that includes: a communicator that exchanges the quantized gradient by communication with another learning device; a gradient calculator that calculates a gradient of a current parameter; a quantization remainder adder that adds, to the gradient, a value obtained by multiplying a remainder at the time of quantizing a previous gradient by a predetermined multiplying factor; a gradient quantizer that quantizes the gradient obtained by the quantization remainder adder; a gradient restorer that restores a quantized gradient received by the communicator to a gradient of the original accuracy; a quantization remainder storage that stores a remainder at the time of quantizing; a gradient aggregator that aggregates gradients collected by the communicator and calculates an aggregated gradient; and a parameter updater that updates the parameter with the aggregated gradient.","1. A distributed deep learning device that exchanges a quantized gradient with at least one or more learning devices and performs deep learning in a distributed manner, the distributed deep learning device comprising:
a communicator that exchanges the quantized gradient by communication with another learning device;
a gradient calculator that calculates a gradient of a current parameter;
a quantization remainder adder that adds, to the gradient obtained by the gradient calculator, a value obtained by multiplying a remainder at the time of quantizing a previous gradient by a predetermined multiplying factor larger than 0 and smaller than 1;
a gradient quantizer that quantizes the gradient obtained by adding the remainder after the predetermined multiplication by the quantization remainder adder;
a gradient restorer that restores a quantized gradient received by the communicator to a gradient of an original accuracy;
a quantization remainder storage that stores a remainder at the time of quantizing the gradient in the gradient quantizer;
a gradient aggregator that aggregates gradients collected by the communicator and calculates an aggregated gradient; and
a parameter updater that updates the parameter on the basis of the gradient aggregated by the gradient aggregator.",Deep learning
Claim analysis with deep learning,"Embodiments relate to system for automatically predicting payer response to claims. In an embodiment, the system receives claim data associated with a claim. The system identifies a set of claim features of the claim data, and generates an input vector with at least a portion of the set of claim features. The system applies the input vector to a trained model. A first portion of the neural network is configured to generate an embedding representing the input vector with a lower dimensionality than the input vector. A second portion of the neural network is configured to generate a prediction of whether the claim will be denied based on the embedding. The system provides the prediction for display on a user interface of a user device. The prediction may further include denial reason codes and a response date estimation to indicate if, when, and why a claim will be denied.","1. A method of analyzing claims, the method comprising:
identifying a set of claim features of a claim;
generating an input vector comprising at least a portion of the set of claim features;
applying the input vector to a trained neural network, wherein a first portion of the neural network is to generate an embedding representing the input vector with a lower dimensionality than the input vector, and wherein a second portion of the neural network is to generate a prediction of whether the claim is to be denied based, at least in part, on the embedding; and
providing the prediction for display on a user interface of a user device, wherein:
the prediction includes a gradient-based score for at least one feature of the input vector and a probability that the claim is to be denied;
the gradient-based score indicates an extent to which the at least one feature contributes to the probability that the is to be denied; and
providing the prediction for display further comprises:
providing for display, on the user interface of the user device, the gradient-based score for the at least one feature in the input vector;
providing an interface element to allow a user to modify one or more values associated with the claim; and
responsive to determining that the user modified one or more values associated with the claim:
generating an updated input vector, the updated input vector including the modified one or more values;
applying the updated input vector to the trained neural network to generate an updated prediction; and
providing the updated prediction for display on the user interface of the user device.",Deep learning
DEEP LEARNING HARDWARE,"A network of matrix processing units (MPUs) is provided on a device, where each MPU is connected to at least one other MPU in the network, and each MPU is to perform matrix multiplication operations. Computer memory stores tensor data and a master control central processing unit (MCC) is provided on the device to receive an instruction from a host device, where the instruction includes one or more tensor operands based on the tensor data. The MCC invokes a set of operations on one or more of the MPUs based on the instruction, where the set of operations includes operations on the tensor operands. A result is generated from the set of operations, the result embodied as a tensor value.","1. An apparatus comprising:
a network of matrix processing units (MPUs), wherein each MPU is connected to at least one other MPU in the network, and each MPU is to perform matrix multiplication operations;
a memory to store tensor data; and
a master control central processing unit (MCC) to:
receive an instruction from a host device, wherein the instruction comprises one or more tensor operands based on the tensor data;
invoke a set of operations on one or more of the MPUs based on the instruction, wherein the set of operations comprises operations on the tensor operands; and
output a result of the set of operations, wherein the result comprises a tensor value.",Deep learning
DEEP LEARNING FOR CREDIT CONTROLS,Systems and methods are provided to identify abnormal transaction activity by a participant that is inconsistent with current conditions. Historical participant and external data is identified. A recurrent neural network identifies patterns in the historical participant and external data. A new transaction by the participant is received. The new transaction is compared using the patterns to the historical participant and external data. An abnormality score is generated. An alert is generated if the abnormality score exceeds a threshold.,"1. A computer implemented method for detecting a deviation from prior activity, inconsistent with current conditions, by a participant in a data transaction processing system in which data items are transacted by a hardware matching processor that matches electronic data transaction request messages for the same one of the data items based on multiple transaction parameters from different client computers over a data communication network, the method comprising:
identifying, by an activity module, in a memory, historic transaction data for the participant and historic external factor data;
identifying, by the activity module, one or more patterns in the historic transaction data and the historic external factor data using a structured neural network; the structured neural network comprising a layered plurality of interconnected processing nodes; wherein each connection of the plurality of interconnected processing nodes to another may be dynamically weighted; the one or more patterns indicative of historical normal activity by the participant in relation to the historic external factor data;
receiving, by the activity module, from the participant, an electronic data transaction request message comprising data indicative of a current transaction specifying a product, quantity, and value, and storing the received electronic data transaction request message in the memory;
calculating, by the activity module, current external factor data;
comparing, by the activity module, the data indicative of the transaction and the current external factor data with the one or more patterns;
generating, by the activity module, an abnormality score for the first electronic data transaction based on the comparison; and
generating, by the activity module, an alert when the abnormality score exceeds a first threshold.",Deep learning
Arithmetic processing device for deep learning and control method of the arithmetic processing device for deep learning,"A memory 11 stores therein first data and second data each of which has element data that forms a matrix. Arithmetic units 51 to 53 repeat, for each of a first predetermined row of the first data and a second predetermined row of the second data that are stored in the memory 11, by using the element data included in the first predetermined row and the element data included in the second predetermined row, a row portion operation based on the number of columns in the second data and performs, by using results of the row portion operations, an arithmetic operation process that acquires the operation results of the operation that uses the first data and the second data.","1. An arithmetic processing device comprising:
a processor, a memory and a product-sum calculator, the memory stores first data and second data each of which has element data that forms a matrix;
the product-sum calculator performs a row portion operation by performing, by sequentially shifting a predetermined position from a top by a predetermined number at a time, an operation that arranges, in an order of an arrangement in the first data, element data included in a first predetermined row that has a predetermined number of rows, that acquires first element data that corresponds to the element data arranged from the predetermined position to a number of columns of the second data, that multiplies each of the pieces of the acquired first element data by an associated pieces of the element data included in a second predetermined row that has the predetermined number of rows, and that sums multiplication results, and performs, by using results of the row portion operations, a convolution operation that is performed by shifting, by using the second data as weight data, arrangement positions of the second data in the first data by a predetermined number at a time, and
the processor performs an image recognition by using a result of the convolution operation.",Deep learning
Integrated deep learning and clinical image viewing and reporting,"Integrated deep learning and clinical image viewing and reporting are provided. In some embodiments, a clinical image is received. An annotated image is generated from the clinical image by application of a deep learning system. At least one clinical finding is generated from the clinical image by application of the deep learning system. The annotated image and the at least one clinical finding are provided to a user. A structured report is generated based on the annotated image and the at least one clinical finding.","1. A method comprising:
receiving a clinical image;
generating an annotated image from the clinical image by application of a deep learning system;
generating at least one clinical finding based on the clinical image by application of the deep learning system;
providing the annotated image and the at least one clinical finding to a user;
generating a structured report based on the annotated image and the at least one clinical finding.",Deep learning
Image crop suggestion and evaluation using deep-learning,"Various embodiments describe using a neural network to evaluate image crops in substantially real-time. In an example, a computer system performs unsupervised training of a first neural network based on unannotated image crops, followed by a supervised training of the first neural network based on annotated image crops. Once this first neural network is trained, the computer system inputs image crops generated from images to this trained network and receives composition scores therefrom. The computer system performs supervised training of a second neural network based on the images and the composition scores.","1. A computer-implemented method, the method comprising:
performing, by a computer system, an unsupervised training of a first neural network based on unannotated image crops from a first set of training images, the unsupervised training comprising updating parameters of the first neural network associated with generating a composition score of one or more of the unannotated image crops;
performing, by the computer system upon completion of the unsupervised training, a first supervised training of the first neural network based on a set of annotated image crops from the first set of training images further by updating the parameters of the first neural network;
generating, by the computer system, a set of additional image crops from a second set of training images, wherein the set of additional image crops is generated based on predefined crop areas;
receiving, by the computer system, from the first neural network, and upon completion of the first supervised training, a set of composition scores corresponding to the set of additional image crops, the set of composition scores based on inputting the set of additional image crops to the first neural network;
performing, by the computer system, a second supervised training of a second neural network based on the second set of training images and the set of composition scores, the second supervised training comprising updating parameters of the second neural network associated with evaluating compositions of the predefined crop areas; and
providing, by the computer system and to an image application, information about a composition of an input image crop within an input image, the composition based on at least one of: an output from the first neural network or an output from the second neural network.",Deep learning
Compression in machine learning and deep learning processing,"Embodiments are generally directed to compression in machine learning and deep learning processing. An embodiment of an apparatus for compression of untyped data includes a graphical processing unit (GPU) including a data compression pipeline, the data compression pipeline including a data port coupled with one or more shader cores, wherein the data port is to allow transfer of untyped data without format conversion, and a 3D compression/decompression unit to provide for compression of untyped data to be stored to a memory subsystem and decompression of untyped data from the memory subsystem.","1. An apparatus for compression of 3D graphics data and untyped data comprising:
a graphical processing unit (GPU) including a data compression pipeline for typed 3D graphics data and untyped data, the data compression pipeline including:
a data port coupled with one or more shader cores, wherein the data port is to convert typed 3D graphics data to a format for pixel data and is to allow transfer of untyped data without format conversion, and
a 3D compression/decompression unit to provide for compression and decompression of both typed 3D data and untyped data, wherein untyped data is to be compressed to be stored to a memory subsystem and is to be decompressed to be read from the memory subsystem;
wherein the apparatus is to utilize blocks based at least in part on data hashing to store 3D data and is to utilize fixed sequential blocks for storage of untyped data.",Deep learning
Deep-learning-based automatic skin retouching,"Embodiments disclosed herein involve techniques for automatically retouching photos. A neural network is trained to generate a skin quality map from an input photo. The input photo is separated into high and low frequency layers which are separately processed. A high frequency path automatically retouches the high frequency layer using a neural network that accepts the skin quality map as an input. A low frequency path automatically retouches the low frequency layer using a color transformation generated by a second neural network and the skin quality map. The retouched high and low frequency layers are combined to generate the final output. In some embodiments, a training set for any or all of the networks is enhanced by applying a modification to an original image from a pair of retouched photos in the training set to improve the resulting performance of trained networks over different input conditions.","1. One or more non-transitory computer storage media storing computer-useable instructions that, when used by one or more computing devices, cause the one or more computing devices to perform operations comprising:
automatically generating a skin quality map from a skin mask of an input image, wherein the skin mask identifies regions of skin in the input image, and wherein the skin quality map is a probability map comprising a probability for each analyzed region that the analyzed region needs retouching;
separating the skin mask into a high frequency layer and a low frequency layer;
providing the skin quality map as an input into a first neural network configured to perform texture synthesis on the high frequency layer to generate a retouched high frequency layer;
applying a color transformation to the low frequency layer to generate a retouched low frequency layer, wherein parameters of the color transformation are generated using a second neural network and upsampled using the skin quality map; and
combining the retouched high frequency layer and the retouched low frequency layer.",Deep learning
Deep learning system for recognizing pills in images,"A system and method is provided that utilizes deep learning, including convolutional neural networks, to identify subject objects in unconstrained user images such as unknown pills. An image of, e.g., a pill, may be captured and subsequently processed using deep learning models to identify the pill. The deep learning models may be optimized to have a small footprint (in terms of computational and memory resources) suitable for a resource-limited device such as a smartphone while retaining a high object recognition accuracy. Each such model may also be run on modified versions of the unconstrained image, for example on color, greyscale, and gradient images, to focus the models on different distinguishing features of the object.","1. A method for pill identification comprising:
obtaining, via an electronic device, an image of a subject pill to be identified;
preprocessing the image to obtain at least a coarse location of the subject pill, using at least one of a processor of the electronic device and a processor of a cloud server;
generating feature arrays corresponding to features of the image using each of a set of convolutional neural networks (CNNs) running on at least one of the processor of the electronic device and the processor of the cloud server;
determining a probable identity of the subject pill based on the feature arrays generated by the set of CNNs; and
communicating the probable identity of the subject pill for a user via a screen of the electronic device.",Deep learning
Deep learning and intelligent sensing system integration,"Disclosed herein are systems, methods, and apparatuses for deep learning and intelligent sensing system integrations. A processor may be configured to receive a plurality of images from the sensor system, identify objects in the images in an offline mode, classify the objects in the images in the offline mode, generate heat maps in the offline mode, and send instructions regarding operation of the maritime vessel based on the objects that are identified. The visual sensor may be a stereoscopic camera. The processor may be further configured to perform stereoscopy. The instructions may include a speed or a heading of, for example, a maritime vessel.","1. A system comprising:
a sensor system disposed on a maritime vessel, the sensor system including a visual sensor; and
a processor in electronic communication with the sensor system, wherein the processor is configured to:
receive a plurality of images from the sensor system;
identify objects in the images in an offline mode by assigning a label to each pixel in the images and segmenting two or more objects in the images;
classify the objects in the images in the offline mode;
generate a disparity map in the offline mode depicting a distance of matter in each pixel from the sensor system; and
send instructions regarding operation of the maritime vessel based on the objects that are identified, and wherein the instructions include a speed or a heading.",Deep learning
Image labeling for cleaning robot deep learning system,"A computer-implemented method for labeling images includes capturing, using an augmented-reality enabled device, a first set of images that include views of a first object; at the augmented-reality enabled device, for each of the first set of images, identifying the first object and generating a bounding box that is associated with the first object; receiving an input providing a first label for the first object; and at the augmented-reality enabled device, for each of at least some of the first set of images, associating the first label with the first object bound by the bounding box.","1. A computer-implemented method comprising:
receiving, at one or more data processors, a first set of images that include views of a first object;
establishing a coordinate system of a virtual space that corresponds to a real-world space in which the first object is located;
receiving a first input identifying the first object in one of the first set of images, the first input providing a first label for the first object, the first label indicative of an identity of the first object;
receiving a second input for overlaying a virtual object on the first object in the one of the first set of images; and
processing, using the one or more data processors, the other ones of the first set of images, including:
automatically overlaying the virtual object on the first object in the other ones of the first set of images,
automatically identifying the first object in the other ones of the first set of images based on the virtual object in the other ones of the first set of images, and automatically associating the first object with the first label.",Deep learning
Harmonizing composite images using deep learning,"Methods and systems are provided for generating harmonized images for input composite images. A neural network system can be trained, where the training includes training a neural network that generates harmonized images for input composite images. This training is performed based on a comparison of a training harmonized image and a reference image, where the reference image is modified to generate a training input composite image used to generate the training harmonized image. In addition, a mask of a region can be input to limit the area of the input image that is to be modified. Such a trained neural network system can be used to input a composite image and mask pair for which the trained system will output a harmonized image.","1. A computer-implemented method for training a neural network system to harmonize composite images, the method comprising:
receiving, by a neural network, a training composite image wherein the training composite image comprises a reference image with a modified region;
generating, via the neural network, a training harmonized image from the training composite image received by the neural network, wherein, to generate the training harmonized image, the neural network changes coloration of the modified region of the training composite image to harmonize the modified region with the training composite image; and
training the neural network based on a comparison of the generated training harmonized image and the reference image.",Deep learning
Deep learning enhanced code completion system,"A code completion tool uses a deep learning model to predict the likelihood of a method completing a method invocation. In one aspect, the deep learning model is a LSTM trained on features that represent the syntactic context of a method invocation derived from an abstract tree representation of the code fragment.","1. A method, comprising:
obtaining training samples to train a deep learning model, the training samples representing a plurality of ordered sequences of tokens, an ordered sequence of tokens represents a syntactic context of a method invocation;
inputting the training samples into the deep learning model;
training the deep learning model with the training samples to output, a plurality of probabilities, a probability associated with a select token from a vocabulary of tokens used in the plurality of ordered sequences of tokens, wherein during the training of the deep learning model, at least one predicted embedding vector is generated representing a training sample, the at least one predicted embedding vector obtained as the product of a last temporal hidden state of the deep learning model and a linear projection matrix; and
incorporating the deep learning model into a code completion tool for use in completing a code fragment initiating a method invocation.",Deep learning
Classifying software scripts utilizing deep learning networks,"A method includes generating a tokenized representation of a given software script, the tokenized representation comprising two or more tokens representing two or more commands in the given software script. The method also includes mapping the tokens of the tokenized representation to a vector space providing contextual representation of the tokens utilizing an embedding layer of a deep learning network, detecting sequences of the mapped tokens representing sequences of commands associated with designated types of script behavior utilizing at least one hidden layer of the deep learning network, and classifying the given software script based on the detected sequences of the mapped tokens utilizing one or more classification layers of the deep learning network. The method further includes modifying access by a given client device to the given software script responsive to classifying the given software script as a given software script type.","1. A method comprising:
generating a tokenized representation of a given software script, the tokenized representation comprising two or more tokens representing two or more commands in the given software script;
mapping the tokens of the tokenized representation to a vector space providing contextual representation of the tokens utilizing an embedding layer of a deep learning network;
detecting sequences of the mapped tokens representing sequences of commands associated with designated types of script behavior utilizing at least one hidden layer of the deep learning network;
classifying the given software script based on the detected sequences of the mapped tokens utilizing one or more classification layers of the deep learning network; and
modifying access by a given client device to the given software script responsive to classifying the given software script as a given software script type;
wherein generating the tokenized representation comprises:
generating an array comprising a set of ordered token values corresponding to an order of the tokens in the given software script, wherein a given one of the token values comprises either (i) an index value representing a known script command in a vocabulary of known script commands of a scripting language utilized by the given software script or (ii) a designated value representing an unknown script command not in the vocabulary of known script commands;
determining whether the array comprises a representation of a first type, the representation of the first type comprising at least a threshold number of consecutive instances of the designated value representing unknown script commands between a first token of the array comprising a first index value representing one of the known script commands and a second token of the array comprising a second index value representing one of the known script commands; and
responsive to determining that the array comprises the representation of the first type, converting the representation of the first type to a representing of a second type different than the first type by altering at least one of an ordering and a number of token values in the array having the designated value representing unknown script commands such that there is less than the threshold number of consecutive instances of the designated value representing unknown script commands between the first token and the second token;
wherein the method is performed by at least one processing device comprising a processor coupled to a memory.",Deep learning
MACHINE LEARNING,"Computer implemented machine learning methods are described. A co-operative learning method involves a first rule based system and a second rule based system. A rule base is generated from input data and recursion data is used to recursively update the rule base as a result of newly received input data. Rule data defining at least one rule and associated data are sent to the second system which determines whether to update its rule base using the transmitted rule data, and if so the recursion data is used to recursively determine the updated rules for its rule base. A father machine learning method for a rule based system, involves receiving time series data, determining whether the data increases or decreases the spatial density for previously existing rules, and if so then creating a new cluster and associated rule, otherwise a new cluster is not created.","1. A computer implemented method for co-operative learning by at least a first entity supporting a first rule based system and a second entity supporting a second rule based system, the method comprising:
the first entity generating a rule base from input data supplied from at least one input and having recursion data used to recursively update the rule base as a result of newly received input data;
the first entity sending rule data defining at least one rule of the rule base of the first entity and associated data, including the recursion data, to the second entity;
the second entity determining whether to update the rule base of the second entity using the transmitted rule data, and if so then using the recursion data to recursively determine the updated rules for its rule base.",Machine learning
Machine learning,"A first set of classifiers are trained to predict categories using attributes of cases. A second set of a classifiers are also trained to predict the categories using the attributes of the cases. In addition, the second set of classifiers use the predictions from at least one of the classifiers in the first set to make predictions.","1. A method of machine learning comprises:
training a first set of classifiers to predict a plurality of categories using attributes of training cases, wherein the training cases include documents, payroll data, web surfing data, user profiles, or any combination thereof; and
training a second set of classifiers to predict the plurality of categories using the attributes of the training cases and using predictions from one or more of the classifiers in the first set as inputs for training the second set of classifiers, wherein the ithoutput of the first set classifiers does not input to the ithinput of the second set classifiers;
receiving new cases for classification, wherein the new cases include one of new documents, new payroll data, new web surfing data, and new user profiles; and
classifying the new cases into the plurality of categories with the trained first and second set of classifiers so as to classify each of the new cases under at least one topic as represented by one of the plurality of categories to organize the new cases.",Machine learning
Machine learning,"An automated response system (e.g., an automated voice response system) may employ learning strategies to develop or improve automated response capabilities. Learning strategies may include using communications(e.g., utterances, text messages, etc.) of one party in a conversation (e.g., a customer service agent) to identify and categorize communications of another party in the conversation (e.g., a caller). Classifiers can be build from the categorized communications. Classifiers can be used to identify common communications patterns of a party in a conversation (e.g., an agent). Learning strategies may also include selecting communications as learning opportunities to improve automated response capabilities based on selection criteria (e.g., selection criteria chosen to ensure that the system does not learn from unreliable or insignificant examples).","1. A computer-implemented method comprising:
receiving a set of conversations between a member of a first party type and a member of a second party type, wherein each of the conversations includes a communication of a member of the first party type and a communication of a member of the second party type that is responsive to the communication of the member of the first party type;
grouping the communications of members of the first party type into a first set of clusters;
grouping the responsive communications of members of the second party type into a second set of clusters based upon the grouping of the communications of members of the first party type; and
by machine, generating a set of agent type classifiers for one or more clusters in the second set of clusters, wherein generating is a step executed by a computer processor that is a functional component of the computer, said execution being part of execution, by the computer processor, of computer-readable instructions embedded on a computer-readable storage medium.",Machine learning
"Machine learning device, machine learning system, and machine learning method","Quality judgment on a laser beam intensity distribution is performed by taking an observation condition of the laser beam into consideration. A machine learning device includes: a state observing means that acquires data indicating an intensity distribution of a laser beam and data indicating a condition for observing the laser beam, performed to generate the data indicating the intensity distribution as input data; a label acquisition means that acquires an evaluation value related to judgment of the quality of the laser beam as a label; and a learning means that performs supervised learning using a pair of the input data acquired by the state observing means and the label acquired by the label acquisition means as training data to construct a learning model for judging the quality of the laser beam.","1. A machine learning device comprising:
a state observing means that acquires data indicating an intensity distribution of a laser beam and data indicating a condition for observing the laser beam, performed to generate the data indicating the intensity distribution as input data;
a label acquisition means that acquires an evaluation value related to judgment of the quality of the laser beam as a label; and
a learning means that performs supervised learning using a pair of the input data acquired by the state observing means and the label acquired by the label acquisition means as training data to construct a learning model for judging the quality of the laser beam.",Machine learning
"Machine learning method, machine learning device, and machine learning program","A full-size training image is reduced by an image reduction unit (11) and input to an FCN (Fully Convolutional Neural Network) computation unit (13), and the FCN computation unit (13) performs calculation under a set filter coefficient and outputs a reduced label image. The reduced label image is enlarged to a full size by an image enlargement unit (14), the error calculation unit (15) calculates an error between the enlarged label image and a full-size ground truth based on a loss function, and the parameter update unit (16) updates a filter coefficient depending on the error. By repeating learning under the control of the learning control unit 17, it is possible to generate a learning model for performing optimal segmentation including an error generated at the time of image enlargement. Further, by including the image enlargement processing in the learning model, a full-size label image can be output, and the accuracy evaluation of the model can also be performed with high accuracy.","1. A machine learning method for generating a machine learning model adapted for semantic segmentation on an input image, the machine learning method comprising:
a) an image reduction step of reducing a size of an input training image; and
b) a learning execution step, the learning execution step including repeatedly performing the following:
a label image generation step of generating a label image using a learning model based on predetermined parameters from a plurality of reduced training images;
an image enlargement step of enlarging the label image to have a size corresponding to that of the input training image; and
a parameter update step of acquiring an error due to a loss function based on the enlarged label image and on a ground truth image corresponding to the training image, thereby updating the parameter depending on the error, resulting in gradual improvement of the learning model,
wherein an enlarged learning model, in which enlargement processing by the image enlargement step is added to the learning model that has undergone the improvement in the learning execution step, is acquired for a learning model adapted for performing semantic segmentation.",Machine learning
Image transformation for machine learning,"Methods, systems, and apparatus, including an apparatus for determining pixel coordinates for image transformation and memory addresses for storing the transformed image data. In some implementations, a system includes a processing unit configured to perform machine learning computations for images using a machine learning model and pixel values for the images, a storage medium configured to store the pixel values for the images, and a memory address computation unit that includes one or more hardware processors. The processor(s) are configured to receive image data for an image and determine that the dimensions of the image do not match the dimensions of the machine learning model. In response, the processor(s) determine pixel coordinates for a transformed version of the input image and, for each of the pixel coordinates, memory address(es), in the storage medium, for storing pixel value(s) that will be used to generate an input to the machine learning model.","1. A system, comprising:
a processing unit configured to perform machine learning computations for images using a machine learning model and pixel values for the images, wherein the machine learning model is configured to process images having particular dimensions;
a storage medium configured to store the pixel values for the images; and
a memory address computation unit that includes one or more hardware processors configured to:
receive image data for an image to be processed as input by the machine learning model, the image data specifying dimensions of the image, the dimensions of the image specifying a horizontal pixel dimension and a vertical pixel dimension, wherein the horizontal pixel dimension is a number of pixels along the horizontal dimension of the image and the vertical pixel dimension is a number of pixels along the vertical dimension of the image;
determine that the dimensions of the image do not match the particular dimensions of images that the machine learning model is configured to process;
in response to determining that the dimensions of the image do not match the particular dimensions, determining memory addresses for a transformed version of the image that has dimensions that match the particular dimensions, including:
determine, based on the dimensions of the image and the particular dimensions, a horizontal pixel stride and a vertical pixel stride for the image; and
determine, using the horizontal pixel stride and the vertical pixel stride, a plurality of pixel coordinates for the transformed version of the image, each pixel coordinate including a vertical coordinate and a horizontal coordinate;
for each of the plurality of pixel coordinates, determine one or more memory addresses, in the storage medium, for storing one or more pixel values that will be used to generate an input to the machine learning model for the pixel coordinate, the one or more memory addresses for each pixel coordinate being based on the vertical coordinate and the horizontal coordinate of the pixel coordinate; and
output the memory address for each pixel value of each pixel coordinate to the processing unit, wherein the processing unit uses each memory address to obtain the pixel value stored in memory at the memory address and performs one or more machine learning computations using the obtained pixel value.",Machine learning
CONTROLLER AND MACHINE LEARNING DEVICE,"A machine learning device of a controller observes, as state variables expressing a current state of an environment, teaching position compensation amount data indicating a compensation amount of a teaching position in control of a robot according to the teaching position and data indicating a disturbance value of each of the motors of the robot in the control of the robot, and acquires determination data indicating an appropriateness determination result of the disturbance value of each of the motors of the robot in the control of the robot. Then, the machine learning device learns the compensation amount of the teaching position of the robot in association with the motor disturbance value data by using the observed state variables and the determination data.","1. A controller that determines a compensation amount of a teaching position in control of a robot according to the teaching position included in teaching data, the controller comprising:
a machine learning device that learns a compensation amount of the teaching position in the control of the robot according to the teaching position, wherein
the machine learning device has
a state observation section that observes, as state variables expressing a current state of an environment, teaching position compensation amount data indicating the compensation amount of the teaching position in the control of the robot according to the teaching position, motor disturbance value data indicating a disturbance value of each of motors of the robot in the control of the robot, and teaching position data including the teaching position of the teaching data,
a determination data acquisition section that acquires determination data indicating an appropriateness determination result of the disturbance value of each of the motors of the robot in the control of the robot, and
a learning section that learns the compensation amount of the teaching position of the robot in association with the motor disturbance value data and the teaching position by using the state variables and the determination data.",Machine learning
Machine learning inventory management,"Systems and methods for machine learning inventory management. The methods comprise: capturing images by a plurality of image capture devices of different types (e.g., visual camera, 3D camera and/or thermal camera); reading item identification codes for items represented in the images; and using at least a first portion of the images and known physical appearances of a plurality of items by a machine learning algorithm to learn relationships between the items represented in the images and the item identification codes. At least a second portion of the images are used to learn various types of information that is useful for inventory management (e.g., changes in inventory amounts for display equipment, changes in equipment cleanliness, changes in inventory packaging, item misplacements, etc.).","1. A method for machine learning inventory management, comprising:
capturing images by a plurality of image capture devices of different types;
reading, by a reading device, item identification codes for items represented in the images;
using at least a first portion of the images and known physical appearances of a plurality of items by a machine learning algorithm running on a computing device to learn relationships between the items represented in the images and the item identification codes;
using, by the computing device, the machine learned relationships to provide full inventory coverage and generate models for generating predicted future inventory states and inventory decisions without human assistance; and
performing actions by the computing device to control an inventory system based on the predicted future inventory states and inventory decisions.",Machine learning
Hardware accelerated machine learning,"A machine learning hardware accelerator architecture and associated techniques are disclosed. The architecture features multiple memory banks of very wide SRAM that may be concurrently accessed by a large number of parallel operational units. Each operational unit supports an instruction set specific to machine learning, including optimizations for performing tensor operations and convolutions. Optimized addressing, an optimized shift reader and variations on a multicast network that permutes and copies data and associates with an operational unit that support those operations are also disclosed.","1. A system to perform hardware acceleration for machine learning, comprising:
multiple banks of random access memory;
a plurality of shift reader circuits;
a plurality of operation unit circuits, each of the plurality of operation unit circuits to perform a concurrent access of at least a portion of the random access memory and perform one or more logical atomic operations on data stored in at least one of a plurality of registers in respective operation unit circuits;
a plurality of multicast networks, each of the plurality of multicast networks communicatively coupled to at least two banks of random access memory included in the multiple banks of random access memory and to a respective one of the plurality of shift reader circuits;
wherein each of the plurality of multicast networks communicates a first element from a first shift reader and a second element from a second shift reader to a single one of the plurality of operation unit circuits for each clock cycle, the single one of the plurality of operation unit circuits to perform a single logical atomic operation on the first element and the second element; and
an instruction decoder circuit including an accumulator circuit, one or more registers, and a random number generator circuit, the instruction decoder circuit to:
process instructions restricted to an instruction set including machine learning primitives;
perform memory transfers; and
dispatch instructions to the plurality of operation unit circuits.",Machine learning
Churn prediction with machine learning,"Disclosed is a churn prediction system that predicts with a high level of accuracy which users will and which users will not stop opening the app over a 30-day time period. To this end a model is created using historical event data where the churn-related behavior of each user is known. New event data is then applied to the model to determine the likelihood of each user churning in the future. With these prediction scores a user is then qualified as falling into one of three classifications: low-risk, medium-risk, or high-risk of churn.","1. A computer implemented method, comprising:
receiving a device identifier and an event descriptor, wherein the device identifier is associated with a target device, and wherein the event descriptor describes an event associated with an application that occurred for the target device;
identifying historical event data describing events associated with the application that have occurred for a plurality of user devices by a cutoff date, the events including one or more open events and send events;
using the historical event data to train the machine learning model to predict a likelihood of users of the application churning after the cutoff date, the training of the machine learning model comprising, for each particular user device of the plurality of user devices:
determining a shifted cutoff date for the particular user device, the shifted cutoff date being a predetermined time before the cutoff date,
determining a first time period of historical events for the particular user device ending at the shifted cutoff date;
determining a second time period of historical events for the particular user device extending from the shifted cutoff date to the cutoff date;
identifying first event data describing events occurring for the particular user device within the first time period;
assigning the first event data of the particular user device to a positive or negative training set using second event data describing user events occurring for the particular user device within the second time period, wherein the assigning comprises:
responsive to the second activity data indicating the particular user device churned during the second time period, assigning the first activity data to the positive training set; and
responsive to the second activity data indicating the particular user device did not churn during the second time period, assigning the first activity data to the negative training set;
training the machine learning model based on the positive training set and the negative training set generated from the historical event data;
applying, using the event descriptor, the trained machine learning model for determining the likelihood of the target device churning within a future time period after the cutoff date; and
transmitting the determined likelihood of the target device churning.",Machine learning
Statistical Machine Learning,"Statistical machine learning, in which an input module receives user input that defines a hypothesis associated with a particular output. The hypothesis defines one or more starting criteria that are proposed as being correlated with the particular output, and a recommendation engine initially provides recommendations that include the particular output based on the one or more starting criteria defined by the hypothesis. An experience analytics system receives feedback data related to whether the recommendations provided based on the one or more starting criteria defined by the hypothesis were successful and modifies the hypothesis based on the feedback data. Subsequent to the experience analytics system modifying the hypothesis, the recommendation engine provides recommendations that include the particular output based on the modified hypothesis.","1. A statistical machine learning system comprising:
an input module configured to receive user input that defines a hypothesis associated with a particular output, the hypothesis defining, based on an existing business knowledge associated to a purchasing process of a product, one or more starting criteria that are proposed as being correlated with the particular output;
at least one non-transitory computer-readable storage medium coupled to one or more processors configured to store data representative of the hypothesis defined by the user input;
a recommendation engine configured to access the data representative of the hypothesis and provide, during a first period of time, a recommendation to a first plurality of customers, the recommendation comprising the particular output based on the one or more starting criteria defined by the hypothesis; and
an experience analytics system configured to perform operations comprising:
receiving, from an application system, feedback data based on the one or more starting criteria defined by the hypothesis, the feedback data comprising a percentage of customers from the first plurality of customers accepting the recommendation and indicating that the hypothesis is to be adapted based on the percentage exceeding a first threshold and based on the percentage being below a second threshold,
receiving, from the application system, relevant data indicating that one or more starting criteria are correlated with the particular output, the relevant data not being a direct result of the recommendation, and
modifying the hypothesis based on the relevant data and on the feedback data by one or more of adding an updated criteria to the one or more starting criteria and removing one starting criteria from the one or more starting criteria,
wherein the recommendation engine is further configured to provide, during a second period of time that is subsequent to the first period of time, an updated recommendation to a second plurality of customers, the updated recommendation comprising the particular output based on the modified hypothesis.",Machine learning
Machine learning image processing,"A machine learning image processing system performs natural language processing (NLP) and auto-tagging for an image matching process. The system facilitates an interactive process, e.g., through a mobile application, to obtain an image and supplemental user input from a user to execute an image search. The supplemental user input may be provided from a user as speech or text, and NLP is performed on the supplemental user input to determine user intent and additional search attributes for the image search. Using the user intent and the additional search attributes, the system performs image matching on stored images that are tagged with attributes through an auto-tagging process.","1. A machine learning image processing system comprising:
a data repository storing images and tags for each image, wherein the tags for each image describe attributes of an object in each image;
a network interface to connect the machine learning image processing system to at least one network;
at least one processor to execute machine readable instructions stored on at least one non-transitory computer readable medium;
at least one data storage to store a plurality of image attribute machine learning classifiers, wherein the plurality of image attribute machine learning classifiers comprise convolutional neural networks trained to identify the attributes;
wherein the machine readable instructions comprise machine readable instructions for an auto-tagging subsystem, and the at least one processor is to execute the machine readable instructions for the auto-tagging subsystem to:
apply each image stored in the data repository to the plurality of image attribute machine learning classifiers;
determine predictions for a plurality of image attribute categories from outputs of the plurality of image attribute machine learning classifiers;
determine the attributes of the object in each image stored in the data repository from the predictions; and
tag each image stored in the data repository with the determined attributes for the object in each image,
wherein the machine readable instructions comprise machine readable instructions for an image matching subsystem, and the at least one processor is to execute the machine readable instructions for the image matching subsystem to:
receive, via the network interface, a target image from a mobile application connected to the machine learning image processing system via the at least one network;
receive, via the network interface, supplemental user input associated with the target image from the mobile application connected to the machine learning image processing system via the at least one network;
apply the target image to the plurality of image attribute machine learning classifiers;
determine predictions for the plurality of image attribute categories from applying the target image to the plurality of image attribute machine learning classifiers;
determine target image attributes for an object in the target image from the predictions determined by the plurality of image attribute machine learning classifiers;
apply the supplemental user input to a natural language processing model to determine at least one supplemental image search attribute;
identify a matching subset of the images stored in the data repository that match the target image based on image search attributes determined from the target image attributes and the at least one supplemental image search attribute; and transmit, via the network interface, the matching subset of images to the mobile application for display by the mobile application.",Machine learning
Active machine learning,"Technologies are described herein for active machine learning. An active machine learning method can include initiating active machine learning through an active machine learning system configured to train an auxiliary machine learning model to produce at least one new labeled observation, refining a capacity of a target machine learning model based on the active machine learning, and retraining the auxiliary machine learning model with the at least one new labeled observation subsequent to refining the capacity of the target machine learning model. Additionally, the target machine learning model is a limited-capacity machine learning model according to the description provided herein.","1. A method comprising:
initiating active machine learning through an active machine learning system configured to train an auxiliary machine learning model;
evaluating an unlabeled observation using the auxiliary machine learning model to generate a first score;
evaluating the unlabeled observation using a target machine learning model to generate a second score;
comparing the first score to the second score to calculate a magnitude of a difference between the first score and the second score;
identifying a machine learning feature using the magnitude;
updating the target machine learning model based at least on a refinement using the machine learning feature, wherein the target machine learning model includes a limited-capacity machine learning model;
retraining the auxiliary machine learning model with at least one new labeled observation subsequent to updating the target machine learning model, wherein the retrained version of the auxiliary machine learning model produces the at least one new labeled observation using the machine learning feature from the unlabeled observation subsequent to refining the capacity of the target machine learning model; and
providing the updated target machine learning model to a computing device capable of computation of device labels using the updated target machine learning model.",Machine learning
Machine learning with partial inversion,"An example embodiment may involve a machine learning model representing relationships between a dependent variable and a plurality of n independent variables. The dependent variable may be a function of the n independent variables, where the n independent variables are measurable characteristics of computing devices, and where the dependent variable is a predicted behavior of the computing devices. The embodiment may also involve obtaining a target value of the dependent variable, and separating the n independent variables into nâˆ’1 independent variables with fixed values and a particular independent variable with an unfixed value. The embodiment may also involve performing a partial inversion of the function to produce a value of the particular independent variable such that, when the function is applied to the value of the particular independent variable and the nâˆ’1 independent variables with fixed values, the dependent variable is within a pre-defined range of the target value.","1. A system comprising:
a machine learning model representing relationships between a dependent variable and a plurality of independent variables, including a first independent variable and a second independent variable, wherein the dependent variable is a function of the plurality of independent variables, wherein the first independent variable comprises an identifying characteristic of users of one or more computer-based services offered by a managed network, wherein the second independent variable comprises a type of communication directed to the users of the one or more computer-based services offered by a managed network, and wherein the dependent variable is a likelihood of a respective user performing a specific action in response to the communication when interacting with the one or more computer-based services; and
a computing device including a processor and memory, wherein execution, by the processor, of program instructions stored in the memory causes the computing device to perform operations comprising:
training the machine learning model to output the function of the dependent variable using a training data set, wherein the training data set comprises historical data for the plurality of independent variables and the dependent variable;
obtaining a target value of the dependent variable;
fixing the first independent variable at a fixed value;
performing a partial inversion of the function, using the fixed value of the first independent value and the target value of the dependent variable as inputs, to solve for the second independent variable to obtain a partially inverted function to produce one or more values of the second independent variable such that, when the partially inverted function is applied to the first independent variable having the fixed value and any of the one or more values of the second independent variable, the dependent variable is within a pre-defined range of the target value of the dependent variable, wherein solving for the second independent variable to obtain the partially inverted function comprises:
determining a plurality of candidate values of the second independent variable:
determining, for each of the plurality of candidate values of the second independent variable, a respective output value of the dependent variable when the fixed value of the first independent variable and the candidate value of the second independent variable are applied as inputs to the function and comparing the respective output value of the dependent variable to the target value of the dependent variable; and
outputting one or more of the plurality of candidate values of the second independent variable that, when input to the function with the fixed value of the first independent variable, result in the respective output value of the dependent variable being within the pre-defined range of the target value of the dependent variable;
generating a notification based on the produced one or more values of the second independent variable; and
communicating the notification to one or more of the users of the one or more computer-based services offered by a managed network.",Machine learning
Method and apparatus for machine learning,A disclosed machine learning method includes: calculating a first output error between a label and an output in a case where dropout in which values are replaced with 0 is executed for a last layer of a first channel among plural channels in a parallel neural network; calculating a second output error between the label and an output in a case where the dropout is not executed for the last layer of the first channel; and identifying at least one channel from the plural channels based on a difference between the first output error and the second output error to update parameters of the identified channel.,"1. A non-transitory computer-readable storage medium storing a program that causes a computer to execute a process, the process comprising:
calculating a first output error between a label and an output in a case where dropout in which values are replaced with 0 is executed for a last layer of a first channel among a plurality of channels in a parallel neural network;
calculating a second output error between the label and an output in a case where the dropout is not executed for the last layer of the first channel; and
identifying at least one channel from the plurality of channels based on a difference between the first output error and the second output error to update parameters of the identified channel.",Machine learning
Code completion with machine learning,"A code completion tool uses machine learning models to more precisely predict the likelihood of a method invocation completing a code fragment that follows one or more method invocations of a same class in a same document during program development. In one aspect, the machine learning model is a n-order Markov chain model that is trained on features that represent characteristics of the context of method invocations of a class in commonly-used programs from a sampled population.","1. A system comprising:
one or more processors; and a memory;
one or more programs, wherein the one or more programs are stored in memory and configured to be executed by the one or more processors, the one or more programs including instructions for:
generating a data structure representing a syntactic structure and a semantic model of a program;
extracting a plurality of features from the data structure to construct one or more feature vectors, the plurality of features including characteristics of a context of a method invocation found within a program, wherein the plurality of features includes a spatial position of a method invocation in the program; and
training a sequential model for a first class from the one or more feature vectors to predict a next method invocation for code completion following one or more preceding method invocations of a same class as the first class, wherein the sequential model includes one or more sequences of method invocations of a same class with probabilities for transitioning between each method invocation in the one or more sequences,
wherein the sequential model is an n-order Markov chain model.",Machine learning
Anomaly detection with machine learning,"The technology disclosed relates to machine learning based anomaly detection. In particular, it relates to constructing activity models on per-tenant and per-user basis using an online streaming machine learner that transforms an unsupervised learning problem into a supervised learning problem by fixing a target label and learning a regressor without a constant or intercept. Further, it relates to detecting anomalies in near real-time streams of security-related events of one or more tenants by transforming the events in categorized features and requiring a loss function analyzer to correlate, essentially through an origin, the categorized features with a target feature artificially labeled as a constant. It further includes determining an anomaly score for a production event based on calculated likelihood coefficients of categorized feature-value pairs and a prevalencist probability value of the production event comprising the coded features-value pairs.","1. A method of detecting an anomaly event that has not frequently been observed in an ongoing event stream of security-related events of one or more organizations, the method including:
evaluating a plurality of production events with production space IDs, including for a production event:
transforming features of the production event into categorical bins of a hash-space;
applying a hash function to the production space ID and the features of the production event as transformed to retrieve likelihood coefficients for the transformed features of the production event and a standard candle for the production space ID;
calculating an anomaly score;
when the anomaly score represents a detected anomaly event, accessing history associated with the production space ID to construct a contrast between feature-event pairs of the detected anomaly event and non-anomalous feature-value pairs of prior events for the production space ID; and
invoking one or more security actions including at least one of a quarantine, and an encryption, to be performed when anomalies are detected;
wherein the likelihood coefficients had been calculated by space ID and a standard candle and mapped into the hash-space using a loosely supervised machine learning of observed features in security-related events using a loss function analyzer and recording the standard candle.",Machine learning
Machine learning classifiers,"In an implementation, a non-transitory machine-readable storage medium stores instructions that when executed by a processor, cause the processor to allocate classifier data structures to persistent memory, read a number of categories from a set of training data, and populate the classifier data structures with training data including training-based, category and word probabilities calculated based on the training data.","1. A method of implementing a machine learning classifier, the method comprising:
allocating in persistent memory, a training structure comprising an array of categories, a category data structure for each category in the array, and a global data structure;
reading the categories of the array from training data;
for each category:
reading training statements from the training data;
splitting each training statement into an array of words;
incrementing a category word counter for each word;
calculating a category statement probability and storing it in the category data structure;
calculating a category word probability for each word and storing it in the category data structure;
calculating a global word probability for each word and storing it in the global data structure.",Machine learning
Machine learning model monitoring,"Technologies for monitoring performance of a machine learning model include receiving, by an unsupervised anomaly detection function, digital time series data for a feature metric; where the feature metric is computed for a feature that is extracted from an online system over a time interval; where the machine learning model is to produce model output that relates to one or more users' use of the online system; using the unsupervised anomaly detection function, detecting anomalies in the digital time series data; labeling a subset of the detected anomalies in response to a deviation of a time-series prediction model from a predicted baseline model exceeding a predicted deviation criterion; creating digital output that identifies the feature as associated with the labeled subset of the detected anomalies; causing, in response to the digital output, a modification of the machine learning model.","1. A method for monitoring performance of a machine learning model, the method comprising:
receiving, by an unsupervised anomaly detection function, digital time series data for a feature metric;
wherein the feature metric is computed for a feature that is extracted from an online system over a time interval;
wherein the feature represents one or more user interactions responsive to one or more program execution decisions of the online system;
wherein the machine learning model is to produce model output that relates to one or more users' use of the online system;
using the unsupervised anomaly detection function, detecting anomalies in the digital time series data;
labeling a subset of the detected anomalies in response to a deviation of a time-series prediction model from a predicted baseline model exceeding a predicted deviation criterion;
creating digital output that identifies the feature as associated with the labeled subset of the detected anomalies; and
causing, in response to the digital output, a modification of the machine learning model,
wherein the method is performed by one or more computing devices.",Machine learning
Machine learning dialect identification,"Technology is disclosed for creating and tuning classifiers for language dialects and for generating dialect-specific language modules. A computing device can receive an initial training data set as a current training data set. The selection process for the initial training data set can be achieved by receiving one or more initial content items, establishing dialect parameters of each of the initial content items, and sorting each of the initial content items into one or more dialect groups based on the established dialect parameters. The computing device can generate, based on the initial training data set, a dialect classifier configured to detect language dialects of content items to be classified. The computing device can augment the current training data set with additional training data by applying the dialect classifier to candidate content items. The computing device can then update the dialect classifier based on the augmented current training data set.","1. A method, comprising:
selecting, by a computing device, an initial training data set as a current training data set, wherein the initial training data set is selected by:
receiving one or more initial content items; and
establishing dialect parameters for two or more of the initial content items, wherein the dialect parameters identify a first of the two or more of the initial content items as being composed in a first dialect and identify a second of the two or more of the initial content items as being composed in a second dialect;
generating, by the computing device and based on the initial training data set and corresponding dialect parameters, a dialect classifier configured to detect language dialects of content items to be classified as being in one of two or more dialects, the two or more dialects including at least the first dialect and the second dialect;
augmenting, by the computing device, the current training data set with additional training data by applying the dialect classifier to candidate content items, wherein at least one of the candidate content items that is in the augmented current training data set was not included in the initial training data set;
updating the dialect classifier based on the augmented current training data set; and
returning the updated dialect classifier, wherein the updated dialect classifier is configured to identify additional content items that are not in the initial training data and are not in the augmented current training data set as being in one of the two or more dialects.",Machine learning
WEIGHT GENERATION IN MACHINE LEARNING,"Technologies are generally described for systems, devices and methods relating to a machine learning environment. In some examples, a processor may identify a training distribution of a training data. The processor may identify information about a test distribution of a test data. The processor may identify a coordinate of the training data and the test data. The processor may determine, for the coordinate, differences between the test distribution and the training distribution. The processor may determine weights based on the differences. The weights may be adapted to cause the training distribution to conform to the test distribution when the weights are applied to the training distribution.","1. A method to improve predictive capability of a machine learning system, the method comprising:
receiving, by a computer, training data that includes one or more points;
identifying, by the computer, a training distribution of the one or more points of the training data;
receiving, by the computer, test data that includes one or more points;
identifying, by the computer, information about a test distribution of the one or more points of the test data;
identifying, by the computer, one or more coordinates for the one or more points of the training data and the one or more points of the test data;
determining, for each identified coordinate and by the computer differences between the one or more points of the test data and the one or more points of the training data;
determining, by the computer, weights for the one or more points of the training data based on the determined differences, wherein the weights are adapted to cause the training distribution to conform to the test distribution in response to the weights being applied to the training distribution;
generating, by the computer, a weighted function based on the determined weights and the training data; and
generating, by the computer, a first output based on an application of an input to the generated weighted function, wherein the first output is different than a second output generated by an application of the input to a non-weighted function, wherein the first output and the second output respectively correspond to a first predictive capability and a second predictive capability of the machine learning system, and wherein the first predictive capability is greater than the second predictive capability.",Machine learning
"Machine learning apparatus, laser apparatus and machine learning method","A machine learning apparatus includes: a state amount observation unit that observes a state amount of a laser apparatus including output data from an optical output detection unit, an optical output characteristic recording unit that records history of a driving current and optical output characteristics, and a driving condition/state amount recording unit that records history of a LD unit driving condition data and the state amount; an operation result acquisition unit that acquires a prediction result of characteristics and measurement result of optical output characteristics of the LD unit; a learning unit that learns the LD unit driving condition data with the state amount and the results of the LD unit driving condition data; and a decision-making unit that decides, from a learning result, the LD unit driving condition data.","1. A machine learning apparatus for a laser apparatus that is provided with: a plurality of LD units including at least one or more laser diode modules; a power source unit that individually supplies a driving current to each LD unit; and a control unit that independently controls, for each LD unit, the driving current injected from the power source unit to each LD unit, and emits a laser light via a laser optical system by using a laser light from the plurality of LD units as a laser light source or as an excitation light source for laser oscillation, the machine learning apparatus being configured to learn LD unit driving condition data including output command data instructed to the power source unit,
the machine learning apparatus comprising:
a state amount observation unit that observes a state amount of the laser apparatus including output data from the power source unit, at least one or more optical output detection units that measure an optical output of the laser light, an optical output characteristic recording unit that records history of a measurement result of optical output characteristics of each LD unit obtained from the driving current output from the power source unit and the optical output, and a driving condition/state amount recording unit configured to record history of the LD unit driving condition data including at least a prediction result of a remaining life of each LD unit as a prediction result of characteristics of each LD unit output from the machine learning apparatus and history of the state amount of the laser apparatus;
an operation result acquisition unit that acquires, as results of the LD unit driving condition data, a difference between the prediction result of the characteristics of each LD unit including at least the prediction result of the remaining life of each LD unit recorded in the driving condition/state amount recording unit and the measurement result of the optical output characteristics of each LD unit and/or an estimation result from the measurement result;
a learning unit that receives an output from the state amount observation unit and an output from the operation result acquisition unit, and learns the LD unit driving condition data including at least the prediction result of the remaining life as the prediction result of the characteristics of each LD unit in association with the state amount of the laser apparatus and the results of the LD unit driving condition data; and
a decision-making unit that decides, by referring to a learning result of the learning unit, an LD unit driving condition data including at least a prediction result of the remaining life as the prediction result of the characteristics of each LD unit.",Machine learning
"Machine learning device, robot control system, and machine learning method","A machine learning device that acquires state information from a robot control inspection system. The system has a robot hand to hold a workpiece or camera. The state information includes a flaw detection position of the workpiece, a movement route of the robot hand, an imaging point of the workpiece, and the number of imaging by the camera. A reward calculator calculates a reward value in reinforcement learning based on flaw detection information including the flaw detection position. A value function updater updates an action value function by performing the reinforcement learning based on the reward value, the state information, and the action.","1. A machine learning device for use with, and configured to perform reinforcement learning with respect to a robot control system,
the robot control system comprising:
an illumination means that irradiates a surface to be inspected of an object to be inspected with illumination light;
an imaging means that images the surface to be inspected;
a robot that includes a robot hand;
a control unit that, while moving the robot hand gripping the object to be inspected or the imaging means, along a movement route including a plurality of imaging points set on the surface to be inspected so that the surface to be inspected is entirely covered by a plurality of images imaged by the imaging means, causes the imaging means to image the imaging points set on the surface to be inspected; and
a flaw inspection unit that detects a flaw on the surface to be inspected on the basis of the image obtained by imaging the surface to be inspected by the imaging means,
the machine learning device comprising:
an action information output unit that outputs action information including adjustment information of the imaging region including the imaging points, to the control unit;
a state information acquisition unit that acquires state information from the control unit and the flaw inspection unit resulting from a number N of images, obtained by imaging the surface to be inspected by the imaging means by moving the robot hand gripping the object to be inspected or the imaging means by the control unit, based on the action information, the state information including the number N and flaw detection information, the flaw detection information including a flaw detection position of the surface to be inspected, detected by the flaw inspection unit, with respect to each of a plurality of objects to be inspected prepared in advance;
a reward output unit that outputs a reward value in the reinforcement learning based on the number N and the flaw detection information including the flaw detection positions included in the state information; and
a value function updating unit that updates an action value function based on the reward value, the state information, and the action information.",Machine learning
Efficient machine learning method,"A computerized efficient machine learning method for classification of data and new class discovery inputs labeled data and unlabeled data into a computer memory for a computerized machine tool to perform (a) initial supervised learning using the labeled data to generate a classifier, (b) semi-supervised learning using the labeled data, the classifier and the unlabeled data to generate an updated classifier and high confidence data, (c) active learning using the updated classifier and the unlabeled data to generate a data label request and receive new class labeled data to generate augmented labeled data, (d) new class discovery using the updated classifier and the data label request to generate data of potential new classes and receive labels for potential new class data to generate new class labeled data, and (e) supervised learning using the high confidence data, the labeled data and the augmented labeled data to generate an output classifier.","1. A computerized efficient machine learning method for classification of data, comprising the steps of:
a) inputting labeled data and unlabeled data into a computer memory;
b) performing by a computer program of a computerized machine learning tool an initial supervised learning using the labeled data to generate a classifier;
c) performing by a computer program of the computerized machine learning tool a semi-supervised learning guided by a model assumption using the labeled data, the classifier and the unlabeled data to generate an updated classifier and high confidence data;
d) after step c), performing by a computer program of the computerized machine learning tool an active learning using the updated classifier and the unlabeled data to generate a data label request and receive labels for requested data to label a subset of the unlabeled data and generate augmented labeled data containing labels for high confidence data to update the model assumption for semi-supervised learning if the high confidence data is not verified by the labels for high confidence data, wherein the data label request is processed by an external labeler to generate the labels for requested data; and
e) performing by a computer program of the computerized machine learning tool a supervised learning using the high confidence data, the labeled data and the augmented labeled data to generate an output classifier.",Machine learning
Machine learning document processing,"A machine learning document processing system performs natural language processing (NLP) and machine learning to determine a subset of documents from a document dataset based on the structural features and semantic features. The system facilitates an interactive process, e.g., through a client application, to receive user input from a user to identify documents with a specific document feature category. The user input may be provided from a user as speech or text, and NLP is performed on the user input to determine user intent, the document features, and document feature category. Using the user intent and the additional document feature category, the system identifies subsets of the document dataset that matches the document feature category for display.","1. A machine learning document processing system comprising:
a data repository storing documents;
a network interface to connect the machine learning document processing system to at least one network;
at least one processor to execute machine readable instructions stored on at least one non-transitory computer readable medium;
at least one data storage to store a plurality of document feature machine learning classifiers;
wherein the machine readable instructions comprise machine readable instructions for a document decoder, and the at least one processor is to execute the machine readable instructions for the document decoder to, for each document in the data repository:
apply the document in the data repository to a semantic feature machine learning classifier and a structural feature machine learning classifier;
determine a semantic feature category from the output of the semantic feature machine learning classifier;
determine a structural feature category from the output of the structural feature machine learning classifier; and
determine a document feature category for each document in the data repository based on the semantic feature category and the structural feature category;
wherein the machine readable instructions comprise machine readable instructions for a document interpreter, and the at least one processor is to execute the machine readable instructions for the document interpreter to:
receive, via the network interface, a user input from a client application connected to the machine learning document interpreter via the at least one network;
determine at least a query document feature category based on the user input;
identify a matching subset of the documents stored in the data repository classified by the document decoder based on the determined query document feature category; and
transmit, via the network interface, the matching subset of the documents to the client application for display by the client application.",Machine learning
Machine learning for input fuzzing,Provided are methods and systems for automatically generating input grammars for grammar-based fuzzing by utilizing machine-learning techniques and sample inputs. Neural-network-based statistical learning techniques are used for the automatic generation of input grammars. Recurrent neural networks are used for learning a statistical input model that is also generative in that the model is used to generate new inputs based on the probability distribution of the learnt model.,"1. A method for generating input for testing an application, the method comprising:
receiving as input, at a server, (i) a learnt distribution model of Portable Document Format (PDF) object characters, (ii) a probability of fuzzing a character, and (iii) a threshold probability for modifying predicted characters;
sampling, at the server, the learnt distribution model to generate (i) a candidate next character in a sequence of characters in a PDF object and (ii) a probability of the candidate next character being an actual next character in the sequence;
responsive to determining that the probability of the candidate next character being the actual next character in the sequence is greater than the threshold probability for modifying predicted characters, sampling, at the server, the learnt distribution model to generate a new candidate next character in the sequence; and
generating an input for testing the application using the new candidate next character in the sequence.",Machine learning
Controller and machine learning device,"A machine learning device of a controller observes, as state variables that express a current state of an environment, feeding amount data indicating a feeding amount per unit cycle of a tool and vibration amount data indicating a vibration amount of a cutting part of the tool when the cutting part of the tool passes through the workpiece. In addition, the machine learning device acquires determination data indicating a propriety determination result of the vibration amount of the cutting part of the tool when the cutting part of the tool passes through the workpiece. Then, the machine learning device learns the feeding amount per unit cycle of the tool when the cutting part of the tool passes through the workpiece in association with the vibration amount data, using the state variables and the determination data.","1. A controller that determines a rotational speed of a tool and a feeding amount per unit cycle of a tool when a cutting part of the tool passes through and exits a workpiece in crosscut grooving, the controller comprising:
a processor for controlling the tool and performing a machine learning algorithm including:
controlling, by the processor, a first motor to rotate the tool at the rotational speed, and a second motor to move the cutting part of the tool along an axis to pass through and exit the workpiece at the feeding amount per unit cycle;
observing, by the processor, state variables that express a current operational state of an environment in which a learning target exists, the state variables including a feeding amount data indicating the feeding amount per unit cycle of the tool when the cutting part of the tool is controlled by the processor to exit the workpiece, and a rotational speed amount data indicating the rotational speed of the tool when the cutting part of the tool is controlled by the processor to exit the workpiece, a tool data indicating information on the tool, machining shape data indicating information on a machining shape, and a vibration amount data indicating a vibration amount of the cutting part of the tool when the cutting part of the tool is controlled by the processor to exit the workpiece;
acquiring, by the processor, determination data indicating a comparison between:
i) the vibration amount of the cutting part of the tool due to a current rapid change in the deflection of the tool when the processor controls the first motor and the second motor to rotate and move the cutting part of the tool to exit the workpiece, and
ii) a previous vibration amount of the cutting part of the tool due to a previous rapid change in the deflection of the tool when the processor previously controlled the first motor and the second motor to rotate and move the cutting part of the tool to make a previous exit from the workpiece;
computing, by the processor, a reward based on the comparison, the reward being positive when the comparison indicates that the vibration amount decreased as compared to the previous vibration amount, and the reward being negative when the comparison indicates that the vibration amount increased as compared to the previous vibration amount;
learning, by the processor, based on the state variables, the determination data and the reward, an adjusted rotational speed of the tool and an adjusted feeding amount per unit cycle of the tool which maintains a subsequent vibration due to a subsequent rapid change in the deflection of the tool when the cutting part of the tool exits the workpiece to within an allowable range in association with the tool data, the machining shape data, and the vibration amount data, using the state variables and the determination data;
controlling, by the processor, the first motor to rotate the tool at the adjusted rotational speed, and the second motor to move the cutting part of the tool along the axis to make a subsequent exit from the workpiece at the adjusted feeding amount per unit cycle; and
repeating, by the processor, the machine learning algorithm to maximize a total of the rewards to optimize the adjusted rotational speed and the adjusted feeding amount per unit cycle.",Machine learning
Mixed machine learning architecture,"A preprocessing module of a neural network has a first input and second input. The module generates multiple, different first latent vector representations of its first input, and multiple, different second latent vector representations of its second input. The module then models pairwise interactions between every unique pairwise combination of the first and second latent vector representations. The module then produces an intermediate output by combining the results of the modeled pairwise interactions.","1. A method comprising:
by a computing device, accessing a neural network having a module comprising embedding-pooling blocks, the module having a first input and a second input; and
by the computing device, using the module to:
process the first input to generate a plurality of first latent vector representations of the first input by submitting the first input to each of a first plurality of the embedding-pooling blocks, the plurality of first latent vector representations being different from each other and being respectively generated by the first plurality of the embedding-pooling blocks, wherein a number of the first plurality of the embedding-pooling blocks is determined based on a density of the first input;
process the second input to generate a plurality of second latent vector representations of the second input by submitting the second input to each of a second plurality of the embedding-pooling blocks, the plurality of second latent vector representations being different from each other and being respectively generated by the second plurality of the embedding-pooling blocks, wherein a number of the second plurality of the embedding-pooling blocks is determined based on a density of the second input;
determine unique pairwise combinations of latent vector representations, wherein each unique pairwise combination comprises latent vector representations selected from (1) the plurality of first latent vector representations of the first input and (2) the plurality of second latent vector representations of the second input;
model pairwise interactions between the unique pairwise combinations of latent vector representations; and
produce an intermediate output by combining results of the modeled pairwise interactions.",Machine learning
Processing machine learning attributes,"Systems and methods for processing machine learning attributes are disclosed. An example method includes: identifying a user transaction associated with a set of transaction attributes and a first transaction status; selecting, based on a risk evaluation model, a first plurality of transaction attributes from the set of transaction attributes; modifying a first value of a first transaction attribute in the first plurality of transaction attributes to produce a first modified plurality of transaction attributes; determining, based on the risk evaluation model, that the first modified plurality of transaction attributes identify a second transaction status different from the first transaction status; and in response to the determining, identifying the first transaction attribute as a risk attribute associated with the user transaction.","1. A system, comprising:
a non-transitory memory; and
one or more hardware processors coupled to the non-transitory memory and configured to execute instructions to cause the system to perform operations comprising:
receiving, from a user device via a merchant device, a request to perform an electronic transaction between a user of the user device and a merchant associated with the merchant device, wherein the request comprises a plurality of attribute values corresponding to a plurality of transaction attributes and associated with the electronic transaction;
determining, using a risk evaluation model, that the request is associated with a first risk above a predefined amount;
denying the request based on the first risk being above the predefined amount;
determining, from the plurality of transaction attributes and based on analyzing the risk evaluation model, a set of machine learning attributes used by the risk evaluation model for evaluating electronic transactions;
generating a plurality of modified transaction requests by iteratively selecting different subsets of the set of machine learning attributes and modifying attribute values corresponding to the different subsets of the set of machine learning attributes;
determining, using the risk evaluation model, at least one machine learning attribute from the set of machine learning attributes that contributes to the first risk being above the predefined amount based on the plurality of modified transaction requests;
presenting an indication that the request is denied and the at least one machine learning attribute that contributed to the first risk associated with the request being above the predefined amount;
prompting, on a user interface of the user device, the user for additional information based on the at least one machine learning attribute; and
processing the electronic transaction based on the additional information.",Machine learning
Machine learning method,"A method for using machine learning to solve problems having either a ""positive"" result (the event occurred) or a ""negative"" result (the event did not occur), in which the probability of a positive result is very low and the consequences of the positive result are significant. Training data is obtained and a subset of that data is distilled for application to a machine learning system. The training data includes some records corresponding to the positive result, some nearest neighbors from the records corresponding to the negative result, and some other records corresponding to the negative result. The machine learning system uses a co-evolution approach to obtain a rule set for predicting results after a number of cycles. The machine system uses a fitness function derived for use with the type of problem, such as a fitness function based on the sensitivity and positive predictive value of the rules. The rules are validated using the entire set of training data.","1. A computer-executable method for using machine learning to predict an outcome, the method comprising:
defining a first outcome associated with a first range of medical costs at least as great as a cost threshold;
defining a second outcome associated with a second range of medical costs less than the cost threshold, wherein the second outcome is more likely than the first outcome; and
processing training data with a machine learning system, wherein said training data is a subset of a data set and is recorded in a computer-readable medium, and wherein the act of processing the training data includes:
selecting a first subset of the training data, the first subset corresponding to the first outcome;
selecting a second subset of the training data, the second subset corresponding to the second outcome and consisting of a set of nearby neighbors to the first outcome; and
selecting a third subset of the training data, the third subset corresponding to the second outcome, wherein the third subset does not consist of nearby neighbors to the first outcome; and
using a plurality of software-based, computer-executable machine learners to develop from the first, second and third subsets one or more sets of computer-executable rules usable to predict the first outcome or the second outcome.",Machine learning
Policy Evolution With Machine Learning,"A method for constructing a classifier which maps an input vector to one of a plurality of pre-defined classes, the method steps includes receiving a set of training examples as input, wherein each training example is an exemplary input vector belonging to one of the pre-defined classes, learning a plurality of functions, wherein each function maps the exemplary input vectors to a numerical value, and determining a class for the input vector by combining numerical outputs of the functions determined for the input vector.","1. A computer-implemented classification method, the method comprising:
receiving a plurality of training examples, wherein each training example is an input vector belonging to one class of a set of N classes, wherein N is at least 2;
learning a set of N functions, wherein each function corresponds to a respective class of the N classes and is characterized by the input vectors belonging to the respective class,
wherein the learning comprises for each class:
determining a sequence of N values that is unique with respect to the other sequences for the other classes;
generating points for each input vector belonging to the class, the points comprising the corresponding input vector and a value of the sequence; and
generating a corresponding one of the N functions for the class by fitting a curve through the points;
inputting a new input vector to each of the N functions to generate N outputs; and
classifying the new input vector based on a combination of the N outputs.",Machine learning
Customizable machine learning models,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for customizable machine learning models. In some implementations, data is received, including (i) example data sets and (ii) data specifying one or more criteria to be assessed. A set of multiple models is trained, where each model in the set of models is trained using a training data set comprising a different subset of the example data sets. Output of the models is obtained for various example data sets, and a combination of n-grams is selected based on the outputs. The example data sets are used to train a classifier to evaluate input data with respect to the specified one or more criteria based on whether the input data includes the n-grams in the selected combination of n-grams.","1. A computer-implemented method comprising:
receiving, by a computing system comprising one or more computers, (i) example data sets that each include information about a different individual in an organization and (ii) data specifying one or more criteria to be assessed;
for each of multiple n-grams extracted from text of the example data sets, determining, by the computing system, a correlation measure based on how many of the example data sets that satisfy the specified one or more criteria include the n-gram relative to a total number of the example data sets that satisfy the specified one or more criteria, wherein determining the correlation measures for the multiple n-grams comprises:
determining, as the correlation measure for a particular n-gram, a score based on (i) a first count of the example data sets that include the particular n-gram and satisfy the specified one or more criteria and (ii) a second count of the example data sets that include particular n-gram and do not satisfy the specified one or more criteria;
identifying, by the computing system, a subset of the n-grams selected based on the correlation measures;
for each of different combinations of n-grams in the identified subset selected based on the correlation measures:
training, by the computing system, a set of multiple models that are each configured to classify input data based on whether the n-grams in the combination are present in the input data, wherein each model in the set of models is respectively trained using a training data set comprising a different subset of the example data sets; and
obtaining, by the computing system and for each model in each of the sets of models, output that the model generates for a test data set comprising example data sets different from those of the training data set with which the model was trained;
selecting, by the computing system, one of the combinations of n-grams based on the outputs; and
using, by the computing system, the example data sets to train a classifier to evaluate input data with respect to the specified one or more criteria based on whether the input data includes the n-grams in the selected combination of n-grams.",Machine learning
Training machine learning models,"In one respect, there is provided a system that may include a processor and a memory. The memory may be configured to store instructions that results in operations when executed by the processor. The operations may include: training a machine learning model by at least processing a training set with the machine learning model, the training set including at least one synthetic image that is generated by applying one or more modifications to a non-synthetic image; determining, based at least on a result of the processing of the mixed training set, that the machine learning model is unable to classify images having a specific modification; and training the machine learning model with additional training data that includes one or more additional synthetic images having the specific modification. Related methods and articles of manufacture are also disclosed.","1. A system, comprising:
at least one data processor; and
at least one memory storing instructions which, when executed by the at least one data processor, result in operations comprising:
generating a training set to include a first non-synthetic image and a first synthetic image, the first synthetic image generated by at least reorienting an object in a three dimensional scene depicted in the first non-synthetic image, the object being reoriented by at least mapping a first plurality of coordinates of the object in the three dimensional scene to a second plurality of coordinates of the object in a two dimensional image plane of the non-synthetic image;
balancing a first quantity of non-synthetic images included the training set relative to a second quantity of synthetic images included in the training set, the balancing includes generating, based on the first non-synthetic image, a second non-synthetic image to include in the training set;
training a machine learning model by at least processing the training set with the machine learning model, the training set including the first quantity of non-synthetic images and the second quantity of synthetic images;
determining, based at least on a result of the processing of the training set, that the machine learning model is unable to classify a threshold quantity of images having a first modification but is able to classify a threshold quantity of images having a second modification; and
in response to the machine learning model being unable to classify the threshold quantity of images having the first modification and able to classify the threshold quantity of images having the second modification, training the machine learning model with additional training data that includes one or more additional synthetic images having the first modification but not the second modification.",Machine learning
Explainers for machine learning classifiers,"A transformed data set corresponding to a machine learning classifier's training data set is generated. Each transformed record contains a modified version of a corresponding training record, as well as the prediction made for the training record by the classifier. A set of explanatory rules is minded from the transformed data set, with each rule indicating a relationship between the prediction and one or more features corresponding to the training records. From among the rule set, a particular matching rule is selected to provide an easy-to-understand explanation for a prediction made by the classifier for an observation record which is not part of the training set.","1. A system, comprising:
one or more computing devices of a machine learning service implemented at a provider network;
wherein the one or more computing devices are configured to:
receive a first request, via a programmatic interface from a client, to generate a classification model with respect to an input data set comprising a plurality of observation records, wherein individual ones of the observation records comprise respective values of (a) a plurality of input variables and (b) an output variable;
train, using a selected classification algorithm, the classification model based at least in part on an analysis of a training set of observation records of the input data set;
generate, corresponding to at least a subset of observation records of the training set, a transformed data set comprising a plurality of intermediate data records, wherein a particular intermediate data record of the transformed data set comprises: (a) a binarized feature derived from a corresponding non-binary attribute value of a particular observation record of the training set and (b) a prediction produced by the classification model for the output variable with respect to the particular observation record;
identify, using a selected rule mining algorithm on the transformed data set, a plurality of explanatory rules, wherein a first explanatory rule of the plurality of explanatory rules indicates an implied relationship between a prediction result of the classification model and a predicate on a particular attribute of the observation records;
rank the plurality of explanatory rules based on one or more ranking metrics; and
in response to a request for an explanation of a particular prediction produced by the classification model with respect to a second observation record, provide a representation of a particular explanatory rule, wherein the particular explanatory rule is selected from the plurality of explanatory rules based at least in part on (a) a rank of the particular explanatory rule and (b) a result of a comparison between a predicate of the particular explanatory rule and the second observation record.",Machine learning
Interoperable machine learning platform,"An interoperable platform that provides a way to automatically compose and execute even complex workflows without writing code is described. A set of pre-built functional building blocks can be provided. The building blocks perform data transformation and machine learning functions. The functional blocks have few well known plug types. The building blocks can be composed to build complex compositions. Interoperability between data formats, metadata schema and interfaces to machine learning (ML) functions and trained machine learning models can be provided with no loss of information. A cloud runtime environment can be provided in which the composed workflows can be hosted as REST API to run in production.","1. A computing device, comprising:
at least one processor, and a memory connected to the at least one processor, wherein the at least one memory and the at least one processor are respectively configured to store and execute instructions for causing the computing device to perform operations, the operations comprising:
receiving one or more modules of a machine learning workflow;
composing the one or more received modules of the machine learning workflow into at least a portion of a machine learning application; and
processing a machine learning dataset with the composed machine learning application, the processing of the machine learning dataset including:
automatically interfacing the dataset, at runtime, between a first execution environment configured to execute machine learning code in a first programming language and a second execution environment configured to execute code written in a second programming language; and
interfacing metadata schema, at runtime, between the first execution environment configured to execute the machine learning code in the first programming language and the second execution environment configured to execute the code written in the second programming language.",Machine learning
Machine learning visualization,"A unique user interface for improving machine learning algorithms is described herein. The user interface comprises an icon with multiple visual indicators displaying the machine learning confidence score. When a mouse hovers over the icon, a set of icons are displayed to accept the teaching user's input. In addition, the words that drove the machine learning confidence score are highlighted with formatting so that the teaching user can understand what drove the machine learning confidence score.","1. A special purpose computer implemented method of visualizing a machine learning confidence score, the method comprising:
processing a textual description through a machine learning model to derive the machine learning confidence score, wherein the machine learning model uses natural language processing to convert the textual description into word stems that are used by the machine learning model to calculate the machine learning confidence score, and then searches the textual description for the word stems that comprised a highest impact in the machine learning confidence score, adding formatting instructions to bold or increase a font size at least one word associated with the word stems that comprised the highest impact on the machine learning confidence score;
displaying on a display screen the textual description with the formatting instructions to indicate reasoning used by the machine learning model in a determination of the machine learning confidence score, wherein the textual description with the formatting instructions is only displayed on the display screen when a mouse location is over a variable icon that varies depending on a magnitude of the machine learning confidence score; and
accepting input from a user correcting the machine learning confidence score, said input used to teach the machine learning model.",Machine learning
Method and apparatus for machine learning,"A machine learning apparatus determines an order in which numerical values in an input dataset are to be entered to a neural network for data classification, based on a reference pattern that includes an array of reference values to provide a criterion for ordering the numerical values. The machine learning apparatus then calculates an output value of the neural network whose input-layer neural units respectively receive the numerical values arranged in the determined order. The machine learning apparatus further calculates an input error at the input-layer neural units, based on a difference between the calculated output value and a correct classification result indicated by a training label. The machine learning apparatus updates the reference values in the reference pattern, based on the input error at the input-layer neural units.","1. A non-transitory computer-readable storage medium storing a learning program that causes a computer to perform a procedure comprising:
obtaining an input dataset including a set of numerical values and a training label indicating a correct classification result corresponding to the input dataset;
determining an input order in which the numerical values in the input dataset are to be entered to a neural network for data classification, based on a reference pattern that includes an array of reference values to provide a criterion for ordering the numerical values;
calculating an output value of the neural network whose input-layer neural units respectively receive the numerical values arranged in the input order;
calculating an input error at the input-layer neural units of the neural network, based on a difference between the calculated output value and the correct classification result indicated by the training label; and
updating the reference values in the reference pattern, based on the input error at the input-layer neural units.",Machine learning
Controller and machine learning device,"A machine learning includes a state observation unit that observes, as state variables representing a current state of an environment, PID control parameter data indicating the a parameter of the PID control during machining, machining condition data indicating a machining condition of the machining, and machining environment data relating to a machining environment of the machining, a determination data acquisition unit that acquires, as determination data, tool life determination data indicating an appropriateness determination result relating to depletion of the life of a tool during the machining, and cycle time determination data indicating an appropriateness determination result relating to the cycle time of the machining, and a learning unit that learns the machining condition and the machining environment of the machining, and the parameter of the PID control in association with each other.","1. A numerical controller which, during machining performed by controlling, on the basis of a machining program, a machine having a spindle including a tool and an axis that drives the spindle, executes PID control for controlling a movement rate of the axis so that a spindle load of the spindle becomes constant, the numerical controller comprising:
a machine learning device that learns a parameter of the PID control in relation to a machining condition and a machining environment of the machining, wherein the machine learning device includes:
a state observation unit that observes, as state variables representing a current state of an environment, PID control parameter data indicating the parameter of the PID control during the machining, machining condition data indicating the machining condition of the machining, and machining environment data relating to the machining environment of the machining;
a determination data acquisition unit that acquires, as determination data, tool life determination data indicating an appropriateness determination result relating to depletion of a life of the tool during the machining, and cycle time determination data indicating an appropriateness determination result relating to a cycle time of the machining; and
a learning unit that uses the state variables and the determination data to learn the machining condition and the machining environment of the machining, and the parameter of the PID control in association with each other, and
wherein the learning unit includes:
a reward calculation unit that determines a reward relating to the appropriateness determination result; and
a value function updating unit that uses the reward to update a function representing a value of the parameter of the PID control in relation to the machining condition and the machining environment of the machining, and
the reward calculation unit issues a steadily higher award as depletion of the life of the tool decreases and the cycle time of the machining shortens.",Machine learning
Adversarial quantum machine learning,"In this disclosure, a number of ways that quantum information can be used to help make quantum classifiers more secure or private are disclosed. In particular embodiments, a form of robust principal component analysis is disclosed that can tolerate noise intentionally introduced to a quantum training set. Under some circumstances, this algorithm can provide an exponential speedup relative to other methods. Also disclosed is an example quantum approach for bagging and boosting that can use quantum superposition over the classifiers or splits of the training set to aggregate over many more models than would be possible classically. Further, example forms of k-means clustering are disclosed that can be used to prevent even a powerful adversary from even learning whether a participant even contributed data to the clustering algorithm.","1. A method, comprising:
programming a quantum computing device to implement quantum circuits that perform a machine learning technique using one or more qubits of the quantum computing device, wherein the machine learning technique employs principal component analysis based on at least one median estimate stored as a quantum bit string;
providing an initial group of one or more training sets to the quantum circuits;
allowing the quantum computing device to continue the machine learning process using one or more additional training sets; and
applying a set of one or more security measures to datasets output from the quantum computing device to detect or resist the presence of an adversarial attack on the quantum circuits.",Machine learning
Split machine learning systems,"Split machine learning systems can be used to generate an output for an input. When the input is received, a determination is made as to whether the input is within a first, second, or third range of values. If the input is within the first range, the output is generated using a first machine learning system. If the input is within the second range, the output is generated using a second machine learning system. If the input is within the third range, the output is generated using the first and second machine learning systems.","1. A method of using split machine learning systems to generate an output for an input, wherein the input includes a plurality of profile parameters of a structure formed on a semiconductor wafer, wherein the output is a simulated diffraction signal, which characterizes light diffracted from the structure, the method comprising:
a) receiving the input;
b) determining if the input is within a first, second, or third range of values, wherein the ranges are separate and distinct ranges each containing at least one value;
c) if the input is within the first range, generating the output using a first machine learning system;
d) if the input is within the second range, generating the output using a second machine learning system; and
e) if the input is within the third range, generating the output using the first and second machine learning systems.",Machine learning
Cryptographically secure machine learning,"Embodiments are directed towards classifying data. A machine learning (ML) engine may select an ML model that may employ a cryptographic multi-party computation (MPC) protocol based on model preferences, including a parameter model, provided by a client. A randomness engine may be employed to provide random values and other random values based on the MPC protocol such that the random values may be provided to the client and the other random values may be provided to an answer engine. Input values that correspond to fields in the parameter model may be provided by the client such that the input values may be based on the MPC protocol and the random values. The answer engine may be employed to provide partial results to the question based on the ML model, the input values, and the MPC protocol that may be provided to the client.","1. A method for classifying data over a network using one or more processors, included in one or more network computers, to perform actions, comprising:
employing a machine learning (ML) engine to perform actions, including:
selecting an ML model that employs a cryptographic multi-party computation (MPC) protocol based on model preferences provided by a client, wherein the provided model preferences include both a question and a parameter model, and wherein the parameter model includes one or more model objects of the ML model, and wherein the ML engine uses the parameter model to define one or more input values that are compatible with the ML model;
employing a randomness engine to perform actions, including:
providing one or more random values and one or more other random values based on the cryptographic MPC protocol, wherein the one or more random values are provided to the client and the one or more other random values are provided to an answer engine;
distributing a first instance of the randomness engine and a first random information datastore to the client, wherein the one or more random values are provided from the first random information datastore; and
distributing a second instance of the randomness engine and a second random information datastore to the answer engine, wherein the one or more other random values are provided from the second random information datastore; and
employing the answer engine to perform further actions, including:
synchronizing the first random information datastore and second random information datastore to maintain a correlation between the one or more random values and the one or more other random values;
receiving, from the client, a data model having model objects that include the one or more input values that correspond to one or more fields of the one or more model objects in the parameter model, wherein the one or more input values are based on the cryptographic MPC protocol and the one or more random values;
determining compliance of the data model with one or more requirements of the ML model based on a comparison of the data model to the parameter model;
in response to the data model complying with the one or more requirements of the ML model, providing one or more partial results to the question based on the ML model, the one or more input values, and the cryptographic MPC protocol; and
providing the one or more partial results to the client, wherein a ML client engine provides one or more answers to the question based on the one or more partial results.",Machine learning
Machine learning system interface,"Some embodiments include an experiment management interface for a machine learning system. The experiment management interface can manage one or more workflow runs related to building or testing machine learning models. The experiment management interface can receive an experiment initialization command to create a new experiment associated with a new workflow. A workflow can be represented by an interdependency graph of one or more data processing operators. The experiment management interface enables definition of the new workflow from scratch or by cloning and modifying an existing workflow. The workflow can define a summary format for its inputs and outputs. In some embodiments, the experiment management interface can automatically generate a comparative visualization at the conclusion of running the new workflow based on an input schema or an output schema of the new workflow.","1. A computer-implemented method for promoting the design and execution of machine learning processes by enabling re-use of existing workflows, the method comprising:
generating an experiment management user interface to present existing workflows that ran on a machine learning system, wherein each existing workflow is represented by an interdependency graph of one or more data processing operators, the interdependency graph defines a pipeline of the data processing operators that converts an input dataset specified by an input schema of the existing workflow into an output specified by an output schema of the existing workflow;
receiving from a user, via the experiment management user interface and with the machine learning system, an experiment initialization command to create a new experiment associated with a new workflow, wherein the experiment initialization command includes a selection of an existing workflow in the machine learning system;
receiving from the user, one or more modifications to the existing workflow via the experiment management user interface, comprising:
adding at least one data processing operator to the one or more data processing operators of the existing workflow; or
removing at least one data processing operator from the one or more data processing operators of the existing workflow;
generating, based on the received modifications, an updated pipeline of the one or more data processing operators including the added at least one data processing operator or excluding the removed at least one data processing operator, wherein the generating the updated pipeline is based on input and output schemas of the one or more data processing operators; and
causing execution of the generated new workflow by a dynamic pool of computing devices, based on the updated pipeline, by causing at least a first of the one or more data processing operators to execute on a first computing device, of the dynamic pool of computing devices, and causing at least a second of the one or more data processing operators to execute on a second computing device, of the dynamic pool of computing devices, that is different from the first computing device, and wherein causing the execution involves facilitating one or more of: load-balancing, resource consumption minimization, avoiding bottlenecks, avoiding errors, avoiding inconsistencies, or any combination thereof; and
generating a visualization to facilitate analysis of the new experiment based on an input schema or an output schema of the new workflow.",Machine learning
Machine learning apparatus,"According to an embodiment, a machine learning apparatus includes an interlayer accelerator. The interlayer accelerator includes interlayer units that generate, based on (a) an input vector of a first layer included in a neural network that includes three or more layers and (b) a learning weight matrix of the first layer, an input vector of a second layer next to the first layer. Each of the interlayer units includes a coupled oscillator array. The coupled oscillator array includes oscillators that oscillate at frequencies corresponding to differences between elements of the input vector of the first layer and elements of a row vector that is one row of the learning weight matrix, and combines oscillated signals generated by the oscillators to obtain a calculated signal.","1. A machine learning apparatus, comprising:
an interlayer accelerator that comprises a plurality of interlayer units that generate, based on (a) an input vector of a first layer included in a neural network that includes three or more layers and (b) a learning weight matrix of the first layer, an input vector of a second layer next to the first layer; and
a magnetoresistive random access memory that stores the learning weight matrix, wherein
each of the plurality of interlayer units comprises:
a coupled oscillator array that includes a plurality of oscillators that oscillate at frequencies corresponding to differences between a plurality of elements of the input vector of the first layer and a plurality of elements of a row vector that is one row of the learning weight matrix, and combines oscillated signals generated by the plurality of oscillators to obtain a calculated signal; and
an activation function applier that applies an activation function to the calculated signal to generate one element of the input vector of the second layer,
the plurality of oscillators include a spin torque oscillator, and
the spin torque oscillator is surrounded by eight magnetoresistive random access memory cells.",Machine learning
Feature extraction for machine learning,"A device may receive a first command, included in a set of commands, to set a configuration parameter associated with performing feature extraction. The device may receive a second command, included in the set of commands, to set a corresponding value for the configuration parameter. The configuration parameter and the corresponding value may correspond to a particular feature metric that is to be extracted. The device may configure, based on the configuration parameter and the corresponding value, feature extraction for a corpus of documents. The device may perform, based on configuring feature extraction for the corpus, feature extraction on the corpus to determine the particular feature metric. The device may generate a feature vector based on performing the feature extraction. The feature vector may include the particular feature metric. The feature vector may include a feature identifier identifying the particular feature metric. The device may provide the feature vector.","1. A device, comprising:
one or more processors to:
provide a user interface,
the user interface including one or more user interface elements identifying a set of commands of a feature extraction language;
receive, via the user interface, a first command, included in the set of commands, to set a configuration parameter associated with performing feature extraction,
the configuration parameter representing data used to define a manner in which the feature extraction is to be performed for a corpus of documents;
receive, via the user interface, a second command, included in the set of commands, to set a corresponding value for the configuration parameter,
the configuration parameter and the corresponding value corresponding to a particular feature metric that is to be extracted,
the corresponding value specifying a particular type of data to be extracted from the corpus;
configure, based on the configuration parameter and the corresponding value, the feature extraction for the corpus;
perform, based on configuring the feature extraction for the corpus, the feature extraction on the corpus to determine the particular feature metric,
the particular feature metric identifying data extracted from the corpus based on the configuration parameter and the corresponding value;
generate a feature vector based on performing the feature extraction,
the feature vector including the particular feature metric,
the feature vector including a feature identifier identifying the particular feature metric, and
the feature identifier including an expression, the expression including:
data associated with the first command,
data associated with the second command, and
a logical operator identifying a relationship between the first command and the second command; and
provide the feature vector, as input, to a first machine learning application operating on a first recipient device.",Machine learning
Machine-learning digital assistants,"A method of improving response times associated with responding to requests submitted at one or more front-end systems is disclosed. An utterance is listened for at an intelligent virtual assistant included in the one or more front-end systems. At least one of an intent, a context, and a classification is inferred from the utterance. One or more back-end system commands are generated based on the inferring. The one or more back-end system commands are selected based on machine-learned mappings of the at least one of the intent, the context, and the classification to machine-learned organization-specific pathways into the one or more back-end systems. The one or more back-end system commands are distributed across the one or more back-end systems. A response to the utterance is communicated for presentation via the intelligent virtual assistant, the response including an aggregation of the one or more results received.","1. A system comprising:
one or more computer processors;
one or more computer memories;
one or more usability modules incorporated into the one or more computer memories, the one or more usability modules configuring the one or more computer processors to perform operations for improving response times associated with responding to requests submitted at one or more front-end systems, the operations comprising:
listening for an utterance at an intelligent virtual assistant included in the one or more front-end systems, the utterance including a voice utterance or a chat utterance;
inferring an intent, a context, and a classification from the utterance based on a vocabulary that is specific to an organizational environment in which the one or more front-end systems and the one or more back-end systems have been deployed;
generating a request for a task to be performed at a node in a machine-learned bi-directional workflow data structure, the generating of the request based on the inferring of the intent, the context, and the classification;
generating a plurality of back-end system commands for performing the task based on machine-learned pathways in the bi-directional workflow data structure between the node and the back-end systems, the machine-learned pathways identified based on an application of a first machine-learned algorithm and an application of a second machine-learned algorithm, the first machine-learned algorithm identifying a first set of the pathways in the bi-directional workflow data structure going from the node into the one or more back-end systems, the second machine-learned algorithm identifying pathways in the bi-directional workflow data structure going from the one or more back-end systems to the node;
distributing the one or more back-end system commands across the one or more back-end systems using the machine-learned pathways;
receiving, using the machine-learned pathways, one or more results corresponding to execution of the plurality of back-end system commands by the one or more back-end systems; and
generating a response to the utterance for presentation via the intelligent virtual assistant, the response including an aggregation of the one or more results and a next predicted task, the next predicted task identified based on a correspondence between the utterance and an additional node in the bi-directional workflow data structure.",Machine learning
Machine learning techniques,"Improved techniques for training a machine learning (ML) model are discussed herein. Training the ML model can be based on a subset of examples. In particular, the training can include identifying a reference region associated with an area of the image representing an object, and selecting, based at least in part on a first confidence score associated with a first bounding box, a first hard example for inclusion in the subset of examples. In some cases, the first confidence score and the first bounding box can be associated with a first portion of the feature map. Next, the training can include determining that a first degree of alignment of the first bounding box to the reference region is above a threshold degree of alignment, and in response, replacing the first hard example with a second hard example.","1. A computer-implemented method comprising:
receiving an image representing an object;
identifying a reference region indicative of an area representing the object in the image;
selecting a subset of portions of the image; and
training, based at least in part on the subset of portions, a machine-learning (ML) model to output a feature map, a portion of the feature map associated with classification information, region of interest (ROI) information, and confidence information,
wherein selecting the subset of portions of the image comprises:
determining a first portion of the feature map based at least in part on the first portion having first confidence information that meets or exceeds a threshold confidence; and
determining to include, based at least in part on ROI information associated with the first portion, a second portion of the feature map in the subset of portions of the image, wherein the second portion is different than the first portion.",Machine learning
Method and apparatus for machine learning,Provided is a method of machine learning for a convolutional neural network (CNN). The method includes: receiving input target data; determining whether to initiate incremental learning on the basis of a difference between a statistical characteristic of the target data with respect to the CNN and a statistical characteristic of previously used training data with respect to the CNN; determining a set of kernels with a high degree of mutual similarity in each convolution layer included in the CNN when the incremental learning is determined to be initiated; and updating a weight between nodes to which kernels included in the set of kernels with a high degree of mutual similarity are applied.,"1. A method of machine learning for a convolutional neural network (CNN), the method comprising:
receiving input target data;
determining whether to initiate incremental learning on the basis of a difference between a statistical characteristic of the target data with respect to the CNN and a statistical characteristic of previously used training data with respect to the CNN:
determining a set of kernels with a high degree of mutual similarity in each of convolution layers included in the CNN when the incremental learning is determined to be initiated;
updating each kernel included in the set of kernels by calculating a random matrix with each of the kernels included in the set of kernels with the high degree of mutual similarity; and
updating a weight between nodes to which kernels included in the set of kernels with a high degree of mutual similarity are applied,
wherein the determining of the set of kernels with the high degree of mutual similarity comprises:
constructing concatenated kernel matrices from kernels used in a convolution operation of the node;
measuring similarity between the concatenated kernel matrices based on an absolute value of a difference between the concatenated kernel matrices; and
determining a pair of the kernels with the smallest absolute value of the difference between the concatenated kernel matrices, wherein the pair of the kernels belongs to the set of the kernels,
wherein determining of whether to initiate the incremental learning comprises when the target data is a data array consisting of a plurality of pieces of data, it is determined that the incremental learning is initiated when the number of pieces of data, which are included in the data array and satisfy a condition in which a maximum value among output values of the CNN is less than the predetermined reference value, is greater than or equal to a reference number,
when it is determined that the incremental learning is initiated determining a set of weight vectors with a high degree of mutual similarity in each fully connected layer included in the CNN; and
updating a weight between nodes to which weight vectors included in the set of weight vectors with a high degree of mutual similarity;
wherein the determining of the set of weight vectors with a high degree of mutual similarity comprises measuring at least one pair of the weight vectors with a high degree of mutual similarity by measuring a distance or similarity between weight vectors.",Machine learning
Nested machine learning architecture,"In one embodiment, a method includes a preprocessing stage of a neural network model, where the preprocessing stage includes first and second preprocessing modules. Each of the two modules has first input that may receive a dense input and a second input that may receive a sparse input. Each module generates latent vector representations of their respective first and second inputs, and combine the latent vectors with the original first input to define an intermediate output. The intermediate output of the first module is fed into the first input of the second module.","1. A method for generating a personalized prediction or ranking for a task using a neural network model, comprising:
by a computing device, receiving a first data and a second data associated with a user;
by the computing device, using the neural network model to process the first data and the second data, the neural network model having a plurality of modules, including a first module and a second module, wherein the processing comprises:
providing the first data and the second data to the first module for processing by the first module, wherein the first data includes user information associated with the user, the second data includes semantic information related to the task, and the processing by the first module comprises:
receiving a first input and a second input, wherein the first input comprises the first data and the second input comprises the second data;
generating a first latent vector representation of the first input and a second latent vector representation of the second input;
modeling a pairwise interaction between the first latent vector representation and the second latent vector representation; and
generating an output based on a combination of the pairwise interaction and the first input; and
after generating the output of the first module by processing the first data and the second data using the first module, providing the output of the first module and the second data to the second module for processing by the second module, wherein the processing by the second module comprises:
receiving a first input of the second module and a second input of the second module, wherein the first input of the second module comprises the output of the first module and the second input of the second module comprises the second data;
generating a second-module first latent vector representation of the first input of the second module and a second-module second latent vector representation of the second input of the second module;
modeling a second-module pairwise interaction between the second-module first latent vector representation and the second-module second latent vector representation; and
generating a second-module output based on a combination of the second-module pairwise interaction and the first input of the second module.",Machine learning
MACHINE LEARNING HYPERPARAMETER ESTIMATION,"A method of determining hyperparameters (HP) of a classifier (1) in a machine learning system (10) iteratively produces an estimate of a target hyperparameter vector. The method comprises the steps of selecting from the random sample the hyperparameter vector producing the best result in the present and any previous iterations, and updating the estimate of the target hyperparameter vector by using said selected hyperparameter vector. The random sample may be restricted by using the hyperparameter vector producing the best result in the present and any previous iterations.","1. A method of determining hyperparameters of a classifier in a machine learning system by iteratively producing an estimate of a target hyperparameter vector, each iteration comprising the steps of:
drawing a random sample of hyperparameter vectors from a set of possible hyperparameter vectors,
updating the estimate of the target hyperparameter vector by using the random sample, and
selecting, from the random sample of hyperparameter vectors, a hyperparameter vector producing a best result in the present and any previous iterations, and wherein the step of updating the estimate of the target hyperparameter vector uses said hyperparameter vector producing the best result.",Machine learning
Adaptive machine learning platform,"Some examples include receiving data from a source external to an adaptive machine learning platform that includes at least one machine learning component. Some implementations may execute a machine learning component to generate a machine learning component output. The machine learning component output may be generated at least in part based on the received data. A command may be generated based at least in part on the machine learning component output, and the command may be communicated to an action plugin. The action plugin manager may be used to configure one or more parameters of an action representing an automated workflow to be executed by the action plugin in response to receiving the command.","1. A method comprising:
under control of one or more processors configured with executable instructions,
receiving data from a source external to an adaptive machine learning platform that includes multiple machine learning components, wherein the received data comprises at least one of customer experience data, manufacturer data, retailer data, demonstration device sales data, demonstration device usage data, financial data, return data, or data warehouse data;
executing a first machine learning component of the multiple machine learning components to generate a machine learning component output, wherein the machine learning component output is generated at least in part based on the received data;
generating, based at least in part on the machine learning component output of the first machine learning component, a command configured to cause an action plugin to perform an action representing an automated workflow that is associated with the first machine learning component;
communicating the command to the action plugin, wherein, in response to the action plugin receiving the command, one or more parameters of the action representing the automated workflow to be executed by the action plugin are configurable via an action plugin manager;
causing the action representing the automated workflow to be performed based at least in part on the one or more parameters configured via the action plugin manager in response to communicating the command; and
storing the machine learning component output of the first machine learning component, wherein the stored machine learning component output is accessible as input data to at least a second machine learning component of the multiple machine learning components, the second machine learning component being different than the first machine learning component.",Machine learning
NEURAL NETWORK AND METHOD OF NEURAL NETWORK TRAINING,"A neural network includes a plurality of inputs for receiving input signals, and synapses connected to the inputs and having corrective weights. The network additionally includes distributors. Each distributor is connected to one of the inputs for receiving the respective input signal and selects one or more corrective weights in correlation with the input value. The network also includes neurons. Each neuron has an output connected with at least one of the inputs via one synapse and generates a neuron sum by summing corrective weights selected from each synapse connected to the respective neuron. Furthermore, the network includes a weight correction calculator that receives a desired output signal, determines a deviation of the neuron sum from the desired output signal value, and modifies respective corrective weights using the determined deviation. Adding up the modified corrective weights to determine the neuron sum minimizes the subject deviation for training the neural network.","1. A neural network comprising:
a plurality of inputs of the neural network, each input configured to receive an input signal having an input value;
a plurality of synapses, wherein each synapse is connected to one of the plurality of inputs and includes a plurality of corrective weights, wherein each corrective weight is defined by a weight value;
a set of distributors, wherein each distributor is operatively connected to one of the plurality of inputs for receiving the respective input signal and is configured to select one or more corrective weights from the plurality of corrective weights in correlation with the input value;
a set of neurons, wherein each neuron has at least one output and is connected with at least one of the plurality of inputs via one of the plurality of synapses, and wherein each neuron is configured to add up the weight values of the corrective weights selected from each synapse connected to the respective neuron and thereby generate a neuron sum; and
a weight correction calculator configured to receive a desired output signal having a value, determine a deviation of the neuron sum from the desired output signal value, and modify respective corrective weight values using the determined deviation, such that adding up the modified corrective weight values to determine the neuron sum minimizes the deviation of the neuron sum from the desired output signal value to thereby train the neural network.",Neural network
Inspection neural network for assessing neural network reliability,"A system employs an inspection neural network (INN) to inspect data generated during an inference process of a primary neural network (PNN) to generate an indication of reliability for an output generated by the PNN. The system includes a sensor configured to capture sensor data. Sensor data captured by the sensor is provided to a data analyzer to generate an output using the PNN. An analyzer inspector is configured to capture inspection data associated with the generation of the output by the data analyzer, and use the INN to generate an indication of reliability for the PNN's output based on the inspection data. The INN is trained using a set of training data that is distinct from the training data used to train the PNN.","1. A computer implemented method, comprising:
receiving input data for a primary neural network (PNN), the input data captured by one or more sensors, wherein the PNN is trained using a first set of training data to reduce a value of a loss function;
generating, from the PNN, an output based at least in part on the input data;
capturing inspection data associated with the generation of the output;
generating, from an inspection neural network (INN), an indication of reliability for the output from the PNN based at least in part on the inspection data, wherein the INN is trained using a second set of training data generated from applying the PNN to a third set of training data, wherein the third set of training data is different from the first set of training data used to train the PNN; and
transmitting the output and the indication of reliability to a controller.",Neural network
Training a neural network using another neural network,"In an example embodiment, a first DCNN is trained to output a value for a first metric by inputting a plurality of sample documents to the first DCNN, with each of the sample documents having been labeled with a value for the first metric. Then a plurality of possible transformations of a first input document are fed to the first DCNN, obtaining a value for the first metric for each of the plurality of possible transformations. A first transformation is selected from the plurality of possible transformations based on the values for the first metric for each of the plurality of possible transformations. Then a second DCNN is trained to output a transformation for a document by inputting the selected first transformation to the second DCNN. The second input document is fed to the second DCNN, obtaining a second transformation of the second input document.","1. A computerized method of training and utilizing deep convolutional neural networks (DCNNs), the method comprising:
training a first DCNN to output a value for a first metric by inputting a plurality of sample documents to the first DCNN, each of the sample documents having been labeled with a value for the first metric;
feeding a plurality of possible transformations of a first input document to the first DCNN, the first DCNN producing as output a value for the first metric for each of the plurality of possible transformations;
selecting, by an optimization problem solving component separate from the first DCNN, a first transformation from the plurality of possible transformations based on the values for the first metric for each of the plurality of possible transformations output by the first DCNN;
training a second DCNN to perform a transformation of a document by inputting the selected first transformation and the corresponding value for the first metric to the second DCNN; and
feeding the first input document in its entirety, and exactly as it was fed to the first DCNN, to the second DCNN, obtaining a second transformation of the first input document.",Neural network
Cascaded neural networks using test ouput from the first neural network to train the second neural network,A method includes: training a first neural network using a first training dataset; inputting each test data of a first test dataset to the first neural network; calculating output data of the first neural network for each test data of the first test dataset; composing a second training dataset of training data from the first test dataset that causes the first neural network to output data within a first range; and training a second neural network using the second training dataset.,"1. A method, implemented by a computer, comprising:
training a first neural network using a first training dataset;
inputting each test data of a first test dataset to the first neural network;
calculating output data of the first neural network for each test data of the first test dataset;
composing a second training dataset of training data from the first test dataset that causes the first neural network to output data within a first range; and
training a second neural network using the second training dataset.",Neural network
NEURAL NETWORK METHOD AND APPARATUS,"
A lightened neural network method and apparatus. The neural network apparatus includes a processor configured to generate a neural network with a plurality of layers including plural nodes by applying lightened weighted connections between neighboring nodes in neighboring layers of the neural network to interpret input data applied to the neural network, wherein lightened weighted connections of at least one of the plurality of layers includes weighted connections that have values equal to zero for respective non-zero values whose absolute values are less than an absolute value of a non-zero value. The lightened weighted connections also include weighted connections that have values whose absolute values are no greater than an absolute value of another non-zero value, the lightened weighted connections being lightened weighted connections of trained final weighted connections of a trained neural network whose absolute maximum values are greater than the absolute value of the other non-zero value.

","1. A processor-implemented method for lightening at least one layer of a neural network with a plurality of layers comprising plural nodes and having weighted connections between neighboring nodes in neighboring layers,
wherein the method comprises the steps of:
performing a truncation operation and a cutoff operation to obtain a lightweight range of lightened weighted connections, the truncation operation comprising setting values of weighted connections less than a first positive non-zero value and greater than a first negative non-zero value to zero, and the cutoff operation comprising setting values of weighted connections greater than a second positive non-zero value and less than a second negative non-zero value to said second positive non-zero value and said second negative non-zero value, respectively, and wherein lightened weighted connections, corresponding to weighted connections having a value between the first positive non-zero value and the second positive non-zero value or a value between the second negative non-zero value and the first negative non-zero value, have maintained values corresponding to the value of corresponding weighted connections;
shifting a distribution range of the lightweight range of lightened weighted connections toward zero based on the first positive non-zero value and the first negative non-zero value; and
determining a minimum number of bits for representation of elements in a finite set corresponding to the lightweight range, and representing the lightened weighted connections based on the determined minimum number of bits, thereby reducing a bit size for each lightened weighted connection,
wherein the first positive non-zero value, the second positive non-zero value, the first negative non-zero value and/or the second negative non-zero value are determined by iteratively adjusting said first positive non-zero value, second positive non-zero value, first negative non-zero value and/or second negative non-zero value until a performance index of the neural network having the lightened weighted connections meets a preset criterion.",Neural network
Memristive nanofiber neural networks,"Disclosed are various embodiments of memristive neural networks comprising neural nodes. Memristive nanofibers are used to form artificial synapses in the neural networks. Each memristive nanofiber may couple one or more neural nodes to one or more other neural nodes. In one case, a memristive neural network includes a first neural node, a second neural node, and a memristive fiber that couples the first neural node to the second neural node. The memristive fiber comprises a conductive core and a memristive shell, where the conductive core forms a communications path between the first neural node and the second neural node and the memristive shell forms a memristor synapse between the first neural node and the second neural node.","1. A memristive neural network, comprising:
a first neural node;
a second neural node; and
a memristive fiber that couples the first neural node to the second neural node, wherein the memristive fiber comprises a conductive core and a memristive shell, wherein the conductive core forms a communications path between the first neural node and the second neural node, wherein the memristive shell forms a memristor synapse between the first neural node and the second neural node.",Neural network
Neural networks for transforming signals,"A method for transforms input signals, by first defining a model for transforming the input signals, wherein the model is specified by constraints and a set of model parameters. An iterative inference procedure is derived from the model and the set of model parameters and unfolded into a set of layers, wherein there is one layer for each iteration of the procedure, and wherein a same set of network parameters is used by all layers. A neural network is formed by untying the set of network parameters such that there is one set of network parameters for each layer and each set of network parameters is separately maintainable and separately applicable to the corresponding layer. The neural network is trained to obtain a trained neural network, and then input signals are transformed using the trained neural network to obtain output signals.","1. A method for transforming input signals, comprising:
selecting from a memory a neural network, wherein the neural network includes a set of layers, wherein an activation function of each layer is equivalent to a corresponding iteration of an iterative inference procedure of a generative model, wherein the same set of network parameters is reused by all layers, such that dimensions, ordering, and meaning of parameters are the same for all layers, wherein the iterative inference procedure includes one or combination of a variational inference, a belief propagation, and multiplicative updates, and wherein values of the set of network parameters of at least one layer are different from values of the set of network parameters of at least one other layer; and
transforming the input signals using the neural network to obtain output signals, wherein the steps are performed in a processor operatively connected to the memory.",Neural network
Object detection with neural network,"According to an example aspect of the present invention, there is provided an apparatus comprising at least one processing core, at least one memory including computer program code, the at least one memory and the computer program code being configured to, with the at least one processing core, cause the apparatus at least to provide an input data item to a first convolutional layer of an artificial neural network comprising a set of convolutional layers, process the input data item in the set of convolutional layers, define, in a feature map output from a last convolutional layer of the set of convolutional layers, a first feature map patch and a second feature map patch, and provide the first feature map patch to a first classifier and the second feature map patch to a second classifier.","1. An apparatus comprising at least one processing core, at least one memory including computer program code, the at least one memory and the computer program code being configured to, with the at least one processing core, cause the apparatus at least to:
provide scaled versions of an input data item to a first convolutional layer of an artificial neural network comprising a set of convolutional layers;
process the input data item in the set of convolutional layers, wherein processing the input data item in the set of convolutional layers comprises performing local contrast normalization after a third convolutional layer;
define, in a feature map output from a last convolutional layer of the set of convolutional layers, a first feature map patch and a second feature map patch; and
provide the first feature map patch to a first classifier and the second feature map patch to a second classifier.",Neural network
Interface neural network,"An operation method of a neural network, a training method, and a signal processing apparatus are provided. The operation method includes receiving an output signal from a first neural network, and converting a first feature included in the output signal to a second feature configured to be input to a second neural network, based on a conversion rule controlling conversion between a feature to be output from the first neural network and a feature to be input to the second neural network. The operation method further includes generating an input signal to be input to the second neural network, based on the second feature, and transmitting the input signal to the second neural network.","1. An operation method of an interface neural network, the operation method comprising:
receiving an output signal from a first neural network;
converting a first feature vector of the output signal to a second feature vector of an input signal to be input to a second neural network, based on parameters of the interface neural network controlling conversion between a feature vector to be output from the first neural network and a feature vector to be input to the second neural network;
generating the input signal to be input to the second neural network, the input signal comprising the second feature vector; and
transmitting the input signal to the second neural network.",Neural network
Neural network processing system,"A neural network processing system includes at least one synapse and a neuron circuit. The synapse receives an input signal and has an external weighted value and an internal weighted value, and the internal weighted value has a variation caused by an external stimulus. When the variation of the internal weighted value accumulates to a threshold value, the external weighted value varies and the input signal is multiplied by the external weighted value of the synapse to generate a weighted signal. A neuron circuit is connected with the synapse to receive the weighted signal transmitted by the synapse, and calculates and outputs the weighted signal. The present invention can simultaneously accelerate the prediction and learning functions of the deep learning and realize a hardware neural network with high precision and real-time learning.","1. A neural network processing system comprising:
at least one synapse receiving at least one input signal and having an external weighted value and an internal weighted value, and said internal weighted value has a variation based on said external weighted value and an external stimulus, and when said variation of said internal weighted value accumulates to a threshold value, said external weighted value varies and said at least one input signal is multiplied by said external weighted value of said at least one synapse to generate at least one weighted signal; and
a neuron circuit connected with said at least one synapse to receive said weighted signal transmitted by said at least one synapse, and processing said at least one weighted signal to output a result signal
wherein said external weighted value has a random variation of binary states, and a cumulative distribution function (CDF) of said internal weighted value is used to determine a probability of said random variation;
wherein a formula of calculating said internal weighted value to obtain a probability of switching said external weighted value is expressed by Hj=fa(Î£i=1IXiÃ—Wext,i,j)=fa(Ij), Ok=fa(Î£j=1JHJÃ—Wext,j,k)=fa(Ik), Î´k=(Tkâˆ’Ok)Ã—faâ€²(Ik), Î´jÎ£k=1k(Î´kÃ—Wext,j,k)Ã—faâ€²(Ij), wint,i,jnew=wint,i,joldâˆ’Î·Ã—XiÃ—Î´j, wint,j,knew=wint,j,koldâˆ’Î·Ã—HjÃ—Î´k, and Pext,sw=[CDF(Wintnew)âˆ’CDF(Wintold)]/[1âˆ’CDF(Wintold)], wherein Hj is an output value of a neuron of a jth hidden layer, and Ok is an output value of a neuron of a kth output layer, and Xi is an output value of a ith input layer, and wext,i,j is said external weighted value between said ith input layer and said neuron of said jth hidden layer, wext,j,k is said external weighted value between saaid jth hidden layer and a kth output neuron, fa is a activation function, Ij is a sum of weighted products of neurons of said jth hidden layer, Ik is a sum of weighted products of neurons of said kth output layer, Tk is a target output value of said kth output layer, faâ€² is a differentiation item of said activation function, Î´k is an error amount of said kth output layer, Î´j is an error amount of said jth hidden layer, wint,i,j is said internal weighted value between said ith input layer and said neuron of said jth hidden layer, wint,j,k is said internal weighted value between said jth hidden layer and said kth output neuron, Î· is a learning speed, wold is said internal weighted value before update, wnew is said internal weighted value after update, wint,i,jold is said internal weighted value before update between said ith input layer and said neuron of said jth hidden layer, wint,i,jnew is said internal weighted value after update between said ith input layer and said neuron of said jth hidden layer, wint,j,kold is said internal weighted value before update between said jth hidden layer and said kth output neuron, wint,j,knew is said internal weighted value after update between said jth hidden layer and said kth output neuron, Pext,sw is a probability of switching said external weighted value to 1 or 0, and CDF is said cumulative distribution function determined by said internal weighted value.",Neural network
Neural network method and apparatus,"Provided are a neural network method and an apparatus, the method including obtaining a set of floating point data processed in a layer included in a neural network, determining a weighted entropy based on data values included in the set of floating point data, adjusting quantization levels assigned to the data values based on the weighted entropy, and quantizing the data values included in the set of floating point data in accordance with the adjusted quantization levels.","1. A processor-implemented neural network method, the method comprising:
obtaining a set of floating point data processed in a layer included in a neural network;
determining a weighted entropy based on data values included in the set of floating point data;
adjusting quantization levels assigned to the data values based on the weighted entropy;
quantizing the data values included in the set of floating point data in accordance with the adjusted quantization levels;
implementing the neural network using the quantized data values and based on input data provided to the neural network; and
indicating a result of the implementation,
wherein, the set of floating point data includes a set of weights, and the determining of the weighted entropy comprises:
grouping the set of weights into a plurality of clusters;
determining respective relative frequencies for each of the grouped clusters by dividing a total number of weights included in each of the respective grouped clusters by a total number of weights included in the set of weights;
determining respective representative importances of each of the grouped clusters based on sizes of weights included in each of the grouped clusters; and
determining the weighted entropy based on the respective relative frequencies and the respective representative importances,
wherein the respective representative importances are average values of importances corresponding to the weights included in each of the grouped clusters, each of the importances being quadratically proportional to the size of corresponding weight, and
wherein, the set of floating point data includes a set of activations, activation quantization levels assigned, using an entropy-based logarithm data representation-based quantization method, to data values corresponding to the set of activations are adjusted based on an activation weighted entropy, and the data values corresponding to the set of activations are quantized in accordance with the adjusted activation quantization levels.",Neural network
Line qualification with neural networks,A method tests a subscriber line. The method includes determining values of electrical line features from electrical measurements on the subscriber line and processing a portion of the values of the electrical features with a neural network. The neural network predicts whether the line qualifies to support one or more preselected data services from the portion of the values.,"1. A method of testing a subscriber line, comprising:
determining values of electrical line features from electrical measurements on the subscriber line; and
processing a portion of the values of the electrical features with a neural network that predicts whether the line qualifies to support one or more preselected data services from the portion of the values,
wherein the measurements are one-ended electrical measurements, and further comprising
compressing a vector whose components are the electrical features into a subspace; and
predicting whether the line qualifies based on the compressed vector.",Neural network
Compute optimizations for neural networks,"One embodiment provides for a compute apparatus to perform machine learning operations, the apparatus comprising a decode unit to decode a single instruction into a decoded instruction that specifies multiple operands including an input value and a quantized weight value associated with a neural network and an arithmetic logic unit including a barrel shifter, an adder, and an accumulator register, wherein to execute the decoded instruction, the barrel shifter is to shift the input value by the quantized weight value to generate a shifted input value and the adder is to add the shifted input value to a value stored in the accumulator register and update the value stored in the accumulator register.","1. A compute apparatus to perform machine learning operations, the apparatus comprising:
a decode unit to decode a single instruction into a decoded instruction that specifies multiple operands including an input value and a quantized weight value associated with a neural network, wherein the quantized weight value is an exponent value for a neural network weight and the neural network weight is constrained to a power of a base value; and
an arithmetic logic unit including a barrel shifter, an adder, and an accumulator register, wherein to execute the decoded instruction, the barrel shifter is to shift the input value by the quantized weight value to generate a shifted input value and the adder is to add the shifted input value to a value stored in the accumulator register and update the value stored in the accumulator register.",Neural network
Image classification neural networks,"A neural network system that includes: multiple subnetworks that includes: a first subnetwork including multiple first modules, each first module including: a pass-through convolutional layer configured to process the subnetwork input for the first subnetwork to generate a pass-through output; an average pooling stack of neural network layers that collectively processes the subnetwork input for the first subnetwork to generate an average pooling output; a first stack of convolutional neural network layers configured to collectively process the subnetwork input for the first subnetwork to generate a first stack output; a second stack of convolutional neural network layers that are configured to collectively process the subnetwork input for the first subnetwork to generate a second stack output; and a concatenation layer configured to concatenate the pass-through output, the average pooling output, the first stack output, and the second stack output to generate a first module output for the first module.","1. A neural network system implemented by one or more computers, wherein the neural network system is configured to receive an image and to generate a classification output for the input image, and wherein the neural network system comprises:
a plurality of subnetworks arranged in a stack on top of each other, wherein each subnetwork is configured to process a subnetwork input to generate a subnetwork output and to provide the subnetwork output as input to another subnetwork above the subnetwork in the stack, and wherein the plurality of subnetworks includes:
a first subnetwork comprising a plurality of first modules, each first module comprising:
a first pass-through convolutional layer configured to process the subnetwork input for the first subnetwork to generate a first pass-through output;
a first average pooling stack of neural network layers, wherein the layers in the first average pooling stack are configured to collectively process the subnetwork input for the first subnetwork to generate a first average pooling output;
a first stack of convolutional neural network layers, wherein the layers in the first stack are configured to collectively process the subnetwork input for the first subnetwork to generate a first stack output;
a second stack of convolutional neural network layers, wherein the second stack comprises a 1Ã—1 convolutional layer immediately followed by a 3Ã—3 convolutional layer immediately followed by a 3Ã—3 convolutional layer, and wherein the layers in the second stack are configured to collectively process the subnetwork input for the first subnetwork to generate a second stack output; and
a first concatenation layer configured to concatenate the first pass-through output, the first average pooling output, the first stack output, and the second stack output to generate a first module output for the first module; and
a second subnetwork comprising second modules, each second module comprising:
a second pass-through convolutional layer configured to process the subnetwork input for the second subnetwork to generate a second pass-through output;
a second average pooling stack of neural network layers, wherein the layers in the second average pooling stack are configured to collectively process the subnetwork input for the second subnetwork to generate a second average pooling output;
a third stack of convolutional neural network layers, wherein the third stack includes a 1Ã—1 convolutional layer followed by a 1Ã—7 convolutional layer followed by a 1Ã—7 convolutional layer, and wherein the layers in the third stack are configured to collectively process the subnetwork input for the second subnetwork to generate a third stack output;
a fourth stack of convolutional neural network layers, wherein the layers in the fourth stack are configured to collectively process the subnetwork input for the second subnetwork to generate a fourth stack output; and
a second concatenation layer configured to concatenate the second pass-through output, the second average pooling output, the third stack output, and the fourth stack output to generate a second module output for the second module.",Neural network
Dueling deep neural networks,"Systems, methods, and apparatus, including computer programs encoded on a computer storage medium, for selecting an actions from a set of actions to be performed by an agent interacting with an environment. In one aspect, the system includes a dueling deep neural network. The dueling deep neural network includes a value subnetwork, an advantage subnetwork, and a combining layer. The value subnetwork processes a representation of an observation to generate a value estimate. The advantage subnetwork processes the representation of the observation to generate an advantage estimate for each action in the set of actions. The combining layer combines the value estimate and the respective advantage estimate for each action to generate a respective Q value for the action. The system selects an action to be performed by the agent in response to the observation using the respective Q values for the actions in the set of actions.","1. A system for selecting actions from a set of actions to be performed by an agent interacting with an environment, the system comprising:
a dueling deep neural network implemented by one or more computers, the dueling deep neural network comprising:
a value subnetwork configured to:
receive a representation of an observation characterizing a current state of the environment; and
process the representation of the observation to generate a value estimate of the current state, the value estimate being an estimate of an expected return resulting from the environment being in the current state, independent of the action selected;
an advantage subnetwork configured to:
receive the representation of the observation;
receive data identifying the set of actions that can be performed by the agent in response to the observation; and
for each particular action in the set of actions, process the representation of the observation and the particular action to generate a respective advantage estimate for the particular action that characterizes a difference in (i) a return that would be received if the agent performed the particular action when the environment is in the current state and (ii) a return that would be received if the agent performed another action from the set of actions when the environment is in the current state; and
a combining layer configured to, for each action in the set of actions, process the value estimate and the respective advantage estimate for the action to generate a respective Q value for the current state and the action, wherein the respective Q value is an estimate of an expected return resulting from the agent performing the action when the environment is in the current state.",Neural network
Adaptive and interchangeable neural networks,"Methods and systems that allow neural network systems to maintain or increase operational accuracy while being able to operate in various settings. A set of training data is collected over each of at least two different settings. Each setting has a set of characteristics. Examples of setting characteristic types can be time, geographical location, and/or weather condition. Each set of training data is used to train a neural network resulting in a set of coefficients. For each setting, the setting characteristics are associated with the corresponding neural network having the resulting coefficients and neural network structure. A neural network, having the coefficients and neural network structure resulted after training using the training data collected over a setting, would yield optimal results when operated in/under the setting. A database management system can store information relating to, for example, the setting characteristics, neural network coefficients, and/or neural network structures.","1. A method of controlling a machine, the method comprising:
storing at least two sets of neural network coefficients, each being different from the others;
associating each of the at least two sets of neural network coefficients with one or more characteristics of a setting;
receiving first data from one or more input devices of the machine;
selecting one from the at least two sets of neural network coefficients based on the first data and the one or more characteristics of settings;
instantiating a neural network with the selected one from the at least two sets of neural network coefficients; and
controlling an aspect of the machine using an output from the instantiated neural network.",Neural network
Adaptation of a trained neural network,"A method, computer program product, and apparatus for adapting a trained neural network having one or more batch normalization layers are provided. The method includes adapting only the one or more batch normalization layers using adaptation data. The method also includes adapting the whole of the neural network having the one or more adapted batch normalization layers, using the adaptation data.","1. A computer-implemented method for adapting a trained neural network having one or more batch normalization layers, the method comprising:
adapting only the one or more batch normalization layers using adaptation data; and
adapting the whole of the neural network having the one or more adapted batch normalization layers, using the adaptation data,
wherein each batch normalization layer has a mean, a variance, a scale parameter, and a shift parameter,
wherein the neural network having the one or more batch normalization layers was already trained using training data derived from source domain data, the adaptation data is derived from target domain data; and the scale parameter is adjusted according to the following equation:
##EQU##
where {circumflex over (Î³)} denotes an adjusted scale parameter; Î² denotes the scale parameter of the batch normalization layer before adapted, Ïƒadaptation2 denotes a final value of running variance recalculated using the adaptation data, Ïƒbase2 denotes a final value of running variance of the batch normalization layer before adapted, and E denotes a predefined value.",Neural network
Neural network packing,"A column packing for use in the scrubbing of gases by aqueous liquid, the packing comprising material that is electrically conductive and material that is non-conductive with such materials being intimately mixed with each other, such that the packing as a whole provides the gas-liquid surface for absorption and the conductive material in particular serves as a bipolar electrode for electrolysis.","1. A column and a column packing for use in the scrubbing of gases by an aqueous liquid, said packing comprising electrically conductive pieces and non-conductive pieces, with such pieces being randomly intermixed and spaced between two electrical contacts, such that the packing as a whole provides the gas-liquid interface for absorption and the conductive pieces in particular serve as a bipolar electrode for electrolysis, the proportion of conductive to non-conductive pieces being sufficient to form strands or clumps of conductive pieces but less than the lowest proportion which causes an electrical shunt between the electrical contracts, said column having top and bottom ends on each of which said electrical contacts are positioned.",Neural network
Autoassociative-heteroassociative neural network,"An efficient neural network computing technique capable of synthesizing two sets of output signal data from a single input signal data set. The method and device of the invention involves a unique integration of autoassociative and heteroassociative neural network mappings, the autoassociative neural network mapping enabling a quality metric for assessing the generalization or prediction accuracy of the heteroassociative neural network mapping.","1. A robustness quantifiable autoassociative-heteroassociative neural network capable of synthesizing two sets of output signal data from a single input signal data set for predicting numerical fluctuations including stock market fluctuations, said network comprising:
an encoding subnetwork comprising:
a plurality of input signal receiving layers and nodes communicating numerical fluctuation data input signals to a projection space of said neural network, said input signals being from a source external to said neural network,
a plurality of encoding nodes within said neural network input signal receiving layers forming one representative input signal;
a decoding subnetwork connected to said projection space comprising a plurality of output signal transmitting layers communicating output signal data from said projection space of said neural network to an output;
a plurality of decoding nodes within said output signal transmitting layers jointly transforming said numerical fluctuation input signal data set to a first predicted data set such as future stock market performance and a second data set replicating said input signal data set;
a mean square error backpropagation neural network training algorithm;
a source of training data such as historic stock market performance connected to said encoding subnetwork and applied to said mean square error backpropagation neural network training algorithm as a single set on said encoding subnetwork and generating two data sets from said decoding network; and
an input signal data set and said second data set from said decoding subnetwork comparator block, said comparator block comparing accuracy of replication of said second data set from said decoding subnetwork to said input signal data set indicating robustness of said neural network.",Neural network
Deep-freezer with neural network,"Deep-freezing apparatus for foodstuffs comprising a deep-freezing compartment, signal display means, sensor means adapted to detect the temperature inside the foodstuffs stored in said deep-freezing compartment, processing means for the signals generated by said sensor means, wherein the apparatus further comprises a neural network adapted to receive the signals issued by a temperature sensor situated inside the foodstuff being deep-frozen and by an information on the time elapsed from the beginning of the deep-freezing process, and further adapted to provide a signal representative of the residual time needed to reach a pre-set (deep-freezing) temperature, as well as processing means adapted to receive the signal output by said neural network and provide in response an information representative of the predicted time needed for a pre-determined temperature to be reached on said first temperature sensor.","1. Deep-freezing apparatus for foodstuffs comprising:
a deep-freezing compartment,
a sensor device adapted to detect a temperature inside the foodstuffs stored in said deep-freezing compartment,
a processing device for signals generated by said sensor device,
a display device for displaying information worked out and output by said processing device,
a neural network adapted to receive the signals issued by said processing device for the signals from said temperature sensor device,
and by a plurality of information pieces on a duration of successive time intervals, each one of which measures the time elapsing for going from a respective first temperature down to a respective second temperature, and is adapted to provide information relating to an estimated time (tr) that is still needed for the temperature inside the foodstuff being deep-frozen to reach down to a pre-set, still lower temperature value (Tc), at which there corresponds the state transition phase of the same foodstuff,
and in that said processing device is adapted to receive said time information (tr) from said neural network and provide in response thereto an information representative to the predicted time (tn) needed for a pre-determined temperature lying below said state transition temperature (Tc), and measurable by said sensor device, to be eventually reached.",Neural network
Neural network for reinforcement learning,"A neural model for reinforcement-learning and for action-selection includes a plurality of channels, a population of input neurons in each of the channels, a population of output neurons in each of the channels, each population of input neurons in each of the channels coupled to each population of output neurons in each of the channels, and a population of reward neurons in each of the channels. Each channel of a population of reward neurons receives input from an environmental input, and is coupled only to output neurons in a channel that the reward neuron is part of. If the environmental input for a channel is positive, the corresponding channel of a population of output neurons are rewarded and have their responses reinforced, otherwise the corresponding channel of a population of output neurons are punished and have their responses attenuated.","1. A neural network for reinforcement-learning and for action-selection comprising:
a plurality of channels;
a population of input neurons in each of the channels;
a population of output neurons in each of the channels, each population of input neurons in each of the channels coupled to each population of output neurons in each of the channels by first synapses; and
a population of reward neurons in each of the channels, wherein each population of reward neurons receives input from an environmental input, and wherein each channel of reward neurons is coupled only to output neurons in a channel that the reward neuron is part of by second synapses;
wherein if the environmental input for a channel is positive, the corresponding channel of a population of output neurons are rewarded and have their responses reinforced;
wherein if the environmental input for a channel is negative, the corresponding channel of a population of output neurons are punished and have their responses attenuated; and
wherein the neural network comprises memristors.",Neural network
Organizing neural networks,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for organizing trained and untrained neural networks. In one aspect, a neural network device includes a collection of node assemblies interconnected by between-assembly links, each node assembly itself comprising a network of nodes interconnected by a plurality of within-assembly links, wherein each of the between-assembly links and the within-assembly links have an associated weight, each weight embodying a strength of connection between the nodes joined by the associated link, the nodes within each assembly being more likely to be connected to other nodes within that assembly than to be connected to nodes within others of the node assemblies.","1. A neural network device implemented in hardware or in a combination of hardware and software, the neural network device comprising:
a collection of node assemblies interconnected by a plurality of between-assembly links, each node assembly itself comprising a network of nodes interconnected by a plurality of within-assembly links, wherein each of the between-assembly links and the within-assembly links have an associated weight, each weight embodying a strength of connection between the nodes joined by the associated link, the nodes within each assembly being more likely to be connected to other nodes within that assembly than to be connected to nodes within others of the node assemblies, wherein an average weight of the within-assembly links within each respective node assembly increases as the number of within-assembly links within the respective node assembly increases at least for numbers of within-assembly links that are less than or equal to the number of nodes within the respective node assembly.",Neural network
Reducing computations in a neural network,Reducing computations in a neural network may include determining a group including a plurality of convolution kernels of a convolution stage of a neural network. The convolution kernels of the group are similar to one another. A base convolution kernel for the group may be determined. Scaling factors for a plurality of input feature maps processed by the group may be calculated. The convolution stage of the neural network may be modified to calculate a composite input feature map using the scaling factors and apply the base convolution kernel to the composite input feature map.,"1. A method, comprising:
determining, using a processor, a group comprising a plurality of convolution kernels of a convolution stage of a neural network in which the convolution kernels of the group are similar to one another, the determining of the group by:
determining a similarity metric between a first convolution kernel and a second convolution kernel of the plurality of convolution kernels by calculating an element-wise ratio of the first convolution kernel to the second convolution kernel and calculating a standard deviation for the element-wise ratio of the first convolution kernel and the second convolution kernel; and
including the first convolution kernel and the second convolution kernel in the group responsive to determining that the similarity metric meets a similarity criterion;
determining a base convolution kernel for the group;
determining scaling factors based on the base convolution kernel for the group for a plurality of input feature maps processed by the group, each input feature map corresponding to a scaling factor and a convolution kernel; and
modifying the convolution stage of the neural network so that, when the neural network is executed, the convolution stage of the neural network performs:
scaling each of a plurality of input feature maps using the scaling factors,
generating a composite input feature map as a sum of the scaled input feature maps,
applying the base convolution kernel to the composite input feature map, and
summing the composite input feature map with any of a second plurality of input feature maps which are processed by a convolution kernel other than the base convolution kernel.",Neural network
Neural network optimization mechanism,"An apparatus to facilitate optimization of a neural network (NN) is disclosed. The apparatus includes optimization logic to define a NN topology having one or more macro layers, adjust the one or more macro layers to adapt to input and output components of the NN and train the NN based on the one or more macro layers.","1. An apparatus to facilitate optimization of a neural network (NN), comprising:
a general purpose graphics processing unit to:
define a NN topology as having one or more macro layers;
adjust the one or more macro layers to adapt to input and output components of the NN;
replace a first topology of the NN with the one or more macro layers;
wrap the one or more macro layers within a macro stub layer; and
train the NN based on the one or more macro layers and the macro stub layer.",Neural network
Neural network classifier,"Approaches for classifying training samples with minimal error in a neural network using a low complexity neural network classifier, are described. In one example, for the neural network, an upper bound on the Vapnik-Chervonenkis (VC) dimension is determined. Thereafter, an empirical error function corresponding to the neural network is determined. A modified error function based on the upper bound on the VC dimension and the empirical error function is generated, and used for training the neural network.","1. A method for training a neural network, the method comprising:
determining, for the neural network, an upper bound on the Vapnik-Chervonenkis (VC) dimension;
determining an empirical error function corresponding to the neural network;
generating, for the neural network, a modified error function based on the upper bound on the VC dimension and the empirical error function; and
training the neural network by minimizing the modified error function, wherein the modified error function is represented as:
##EQU##
wherein,
M is the number of training patterns in the neural network;
C represents a hyper-parameter; and
neti represents net input to an output node of the neural network.",Neural network
Systolic convolutional neural network,"A circuit and method are provided for performing convolutional neural network computations for a neural network. The circuit includes a transposing buffer configured to receive actuation feature vectors along a first dimension and to output feature component vectors along a second dimension, a weight buffer configured to store kernel weight vectors along a first dimension and further configured to output kernel component vectors along a second dimension, and a systolic array configured to receive the kernel weight vectors along a first dimension and to receive the feature component vectors along a second dimension. The systolic array includes an array of multiply and accumulate (MAC) processing cells. Each processing cell is associated with an output value. The actuation feature vectors may be shifted into the transposing buffer along the first dimension and output feature component vectors may shifted out of the transposing buffer along the second dimension, providing efficient dataflow.","1. A circuit for performing convolutional neural network computations for a neural network, the circuit comprising:
a transposing buffer configured to store elements of a feature map, each element having a plurality of components, the transposing buffer including a two-dimensional array of registers that allow register-to-register shifting in both dimensions and the transposing buffer configured to receive actuation feature vectors along a first dimension of the transposing buffer and to output feature component vectors along a second dimension of the transposing buffer, each actuation feature vector including a plurality of components of one or more elements of the feature map and each feature component vector including a single component from each of a plurality of elements of the feature map;
a weight buffer configured to store kernel weight vectors along a first dimension of the weight buffer and further configured to output kernel component vectors along a second dimension of the weight buffer; and
a systolic array configured to receive the kernel weight vectors along a first dimension of the systolic array and to receive the feature component vectors along a second dimension of the systolic array,
where the systolic array comprises an array of multiply and accumulate (MAC) processing cells.",Neural network
Neural networks for intelligent control,"A method and system for implementing a neuro-controller. One example of a neuro-controller is a brain-like stochastic search. Another example is a neuro-controller for controlling a hypersonic aircraft. Using a variety of learning techniques, the method and system provide adaptable control of external devices (e.g., airplanes, plants, factories, and financial systems).","1. A computer program product, comprising:
a computer storage medium and a computer program code mechanism embedded in the computer storage medium for causing a neural network to control an external device, the computer program code mechanism comprising:
a first computer code device configured to represent U(u,X) in computer-readable form, where U(u,X) is a family of max problems;
a second computer code device configured to implement a learning algorithm; and
a third computer code device configured to utilize the second computer code device on the computer-readable form of U(u,X) to learn to find u which maximizes U(u,X).",Neural network
Training of a physical neural network,"Physical neural network systems and methods are disclosed. A physical neural network can be configured utilizing molecular technology, wherein said physical neural network comprises a plurality of molecular conductors, which form neural network connections thereof. A training mechanism can be provided for training said physical neural network to accomplish a particular neural network task based on a neural network training rule. The neural network connections are formed between pre-synaptic and post-synaptic components of said physical neural network. The neural network generally includes dynamic and modifiable connections for adaptive signal processing. The neural network training mechanism can be based, for example, on the Anti-Hebbian and Hebbian (AHAH) rule and/or other plasticity rules.","1. A physical neural network system based on nanotechnology, comprising:
an electromechanical-based physical neural network having at least one synaptic component, wherein said at least one synaptic component of said electromechanical-based physical neural network comprises a dielectric medium with a plurality of nanoconductors disposed in said dielectric medium;
a mechanism for modifying a strength of said at least one synaptic component, whereby said plurality of nanoconductors suspended in said dielectric medium and free to move about are subject to a dielectrophoretic force resulting from an exposure to a time-varying electric field, wherein said dielectrophoretic force includes a dipole-induced force that is utilized to attract or repel said plurality of nanoconductors to an electrode gap formed between at least one pre-synaptic electrode and at least one post-synaptic electrode that form said at least one synaptic component of said electromechanical-based physical neural network; and
a feedback circuit for training said electromechanical-based physical neural network by providing a feedback signal to said physical neural network for modifying said at least one synaptic component of said electromechanical-based physical neural network to accomplish a particular neural network task based on a neural network training rule, thereby permitting said electromechanical-based physical neural network to provide for a physical process capable of self-assembly and self-repair and wherein said electromechanical-based physical neural network functions as a physical mechanism that emulates said neural network training rule.",Neural network
Neural networks decoder,"A method of training a neural network to perform decoding of a time-varying signal comprising a sequence of input symbols, which is coded by a coder such that each coded output symbol depends on more than one input symbol, characterized by repetitively: providing a plurality of successive input symbols to the neural network and to the coder, comparing the network outputs with the input signals; and adapting the network parameters to reduce the differences therebetween.","1. A computer implemented method, stored in a computer readable medium, for training a neural network to perform decoding of a time-varying signal comprising a sequence of input symbols, which is coded by a convolutional coder such that each coded output symbol depends on more than one input symbol, characterized by repetitively:
providing a plurality of successive input symbols to the neural network and to the convolutional coder,
comparing outputs of the neural network with the input symbols; and
adapting parameters of the neural network to reduce differences between the neural network outputs and the input symbols;
repeating the providing, comparing and adapting steps until the differences are reduced below a threshold and the neural network substantially operates as a decoder of the convolutional coder;
wherein the input symbol is provided to the decoder with the plurality of output symbols.",Neural network
OPTICAL NEURAL NETWORK,An input layer outputs light having a relatively narrow emission angle distribution to a middle layer as an output signal if the signal level of input signal is relatively high and outputs light having a relatively broad emission angle distribution to the middle layer as the output signal if the signal level of input signal is relatively low. The middle layer outputs light having a relatively narrow emission angle distribution as an output signal to an output layer if the signal level of the output signal from input layer is relatively high and outputs light having a relatively broad emission angle distribution to the output layer as an output signal if the signal level of the output signal from the input layer is relatively low.,"1. An optical neural network comprising:
an input semiconductor layer receiving an input signal and outputs a first output signal on the basis of the received input signal;
a middle semiconductor layer provided to face the input semiconductor layer, receiving the first output signal from the input semiconductor layer and outputting a second output signal on the basis of the received first output signal; and
an output semiconductor layer provided to face the middle semiconductor layer, receiving the second output signal from the middle semiconductor layer and outputting a final output signal on the basis of the received second output signal,
wherein:
the input semiconductor layer outputs light having a first emission angle distribution to the middle semiconductor layer as the first output signal if the signal level of the input signal is a first level and outputs light having a second emission angle distribution which is broader than the first emission angle distribution to the middle semiconductor layer as the first output signal if the signal level of the input signal is a second level which is lower than the first level; and
the middle semiconductor layer outputs light having a third emission angle distribution to the output semiconductor layer as the second output signal if the signal level of the first output signal is a third level and outputs light having a fourth emission angle distribution which is broader than the third emission angle distribution to the output semiconductor layer as the second output signal if the signal level of the first output signal is a fourth level which is lower than the third level.",Neural network
NEURAL NETWORK SYSTEM,"A neural network system that can minimize circuit resources for constituting a self-learning mechanism and be reconfigured into network configurations suitable for various purposes includes a neural network engine that operates in a first and a second operation mode and performs an operation representing a characteristic determined by setting network configuration information and weight information with respect to the network configuration, and a von Neumann-type microprocessor that is connected to the neural network engine and performs a cooperative operation in accordance with the first or the second operation mode together with the neural network engine. The von Neumann-type microprocessor recalculates the weight information or remakes the configuration information as a cooperative operation according to the first operation mode, and sets or updates the configuration information or the weight information set in the neural network engine, as a cooperative operation according to the second operation mode.","1. A neural network system comprising:
a neural network engine that operates in a first operation mode and a second operation mode and performs an operation representing a characteristic determined by setting network configuration information indicating a network configuration to be formed and weight information indicating a weight with respect to the network configuration; and
a von Neumann-type microprocessor that performs a cooperative operation in accordance with the first operation mode or the second operation mode together with said neural network engine, said von Neumann-type microprocessor being connected to said neural network engine,
wherein said neural network engine includes:
a neural processing element that performs neural signal processing;
a routing switch;
a memory containing control information of said neural processing element;
a memory containing control information of said routing switch;
an interconnect, and
said von Neumann-type microprocessor recalculates the weight information or remake the network configuration information as a cooperative operation according to the first operation mode; and
sets or updates the network configuration information or the weight information set in said neural network engine, as a cooperative operation according to the second operation mode.",Neural network
Scheduling neural network processing,"A computer-implemented method includes receiving a batch of neural network inputs to be processed using a neural network on a hardware circuit. The neural network has multiple layers arranged in a directed graph and each layer has a respective set of parameters. The method includes determining a partitioning of the neural network layers into a sequence of superlayers. Each superlayer is a partition of the directed graph that includes one or more layers. The method includes processing the batch of inputs using the hardware circuit, which includes, for each superlayer in the sequence: i) loading the respective set of parameters for the layers in the superlayer into memory of the hardware circuit, and ii) for each input in the batch, processing the input through each of the layers in the superlayer using the parameters in the memory of the hardware circuit to generate a superlayer output for the input.","1. A method, comprising:
receiving a batch of neural network inputs to be processed using a neural network on a hardware circuit, the neural network having a plurality of layers arranged in a directed graph, each layer having a respective set of parameters;
determining a partitioning of the neural network layers into a sequence of superlayers, each superlayer being a partition of the directed graph that includes one or more layers, and wherein a memory of the hardware circuit has a threshold storage capacity, and determining the partitioning of the neural network layers into a sequence of superlayers, comprises:
partitioning the neural network layers into a sequence of superlayers based on the threshold storage capacity of the memory of the hardware circuit;
processing the batch of neural network inputs using the hardware circuit, comprising, for each superlayer in the sequence:
loading the respective set of parameters for the layers in the superlayer into the memory of the hardware circuit; and
for each neural network input in the batch:
processing a superlayer input corresponding to the neural network input through each of the layers in the superlayer using the parameters in the memory of the hardware circuit to generate a superlayer output for the neural network input.",Neural network
Neural network crossbar stack,A circuit for performing neural network computations for a neural network is described. The circuit includes plurality of neural network layers each including a crossbar arrays. The plurality of crossbar arrays are formed in a common substrate in a stacked configuration. Each crossbar array includes a set of crosspoint devices. A respective electrical property of each of the crosspoint devices is adjustable to represent a weight value that is stored for each respective crosspoint device. A processing unit is configured to adjust the respective electrical properties of each of the crosspoint devices by pre-loading each of the crosspoint devices with a tuning signal. A value of the turning signal for each crosspoint device is a function of the weight value represented by each respective crosspoint device.,"1. A circuit for performing neural network computations for a neural network comprising a plurality of neural network layers, the circuit comprising:
a matrix computation unit comprising a respective crossbar array for each layer of the plurality of neural network layers, wherein the respective crossbar arrays for the plurality of neural network layers are formed in a common substrate in a stacked configuration, each crossbar array comprising:
a set of crosspoint devices, wherein a respective electrical property of each of the crosspoint devices is adjustable to represent a weight value that is stored for each respective crosspoint device;
a first set of nanowires, each nanowire of the first set of nanowires being configured to receive an activation input; and
a second set of nanowires, each nanowire of the second set of nanowires being connected to each nanowire of the first set of nanowires by a respective crosspoint device of the set of crosspoint devices, wherein each nanowire of the second set of nanowires is configured to output a value that is a function of signals received from each nanowire of the first set of nanowires and the respective electrical properties of the respective crosspoint devices;
a processing unit configured to adjust the respective electrical properties of each of the crosspoint devices by pre-loading each of the crosspoint devices with a tuning signal, wherein a value of the turning signal for each crosspoint device is a function of the weight value represented by each respective crosspoint device;
a shift-add circuit configured to sum the plurality of activated values;
a sum-in register configured to store the summed plurality of activated values; and
summation circuitry communicatively coupled to the matrix computation unit and the sum-in register, where the summation circuitry is configured to output a sum of a product and the summed plurality of activated values.",Neural network
Presence detection with neural networks,"In a disclosed method, a computing device receiver, from a wireless receiver (RX), first data indicative of channel properties of a first communication link between the wireless receiver (RX) in a first device and a wireless transmitter (TX) in a second device. The first device and the second device are located in a building. The computing device further executes a neural network to process the first data to distinguish humans from stationary objects within the building and detect presence of the human in the building. The computing device transmits result data indicative of the presence to at least one of the first device or the second device.","1. A method comprising:
receiving, by a computing device from a first wireless receiver (RX), first channel properties for a first communication link between the first wireless receiver and a wireless transmitter (TX) that existed during a time period;
receiving, by the computing device, from a second wireless RX, second channel properties for a second communication link between the second wireless RX and the wireless TX that existed during the time period, wherein the first and the second wireless RXs and the wireless TX are located in a building having multiple rooms, and wherein the first and second channel properties represent wireless signal propagation characteristics comprising a combination of scattering, fading, and power decay of wireless signals;
separating, by the computing device, the first channel properties associated with a first group of frequencies from the second channel properties associated with a second group of frequencies, to generate a first input variable and a second input variable, respectively;
processing, by the computing device executing a multi-layered neural network, the first input variable and the second input variable to generate at least a first intermediate output value and a second intermediate output value, wherein processing comprises applying a plurality of weights, from a trained classifier, to the first input variable and the second input variable;
summing, by the computing device, the first intermediate output value and the second intermediate output value to generate a final output value;
performing, by the computing device, presence detection through:
applying a detection activation function to the final output value to generate a detection decision that indicates presence of a human within the building; and
applying a locator function to the final output value to detect a relative location of the human within the building, wherein the relative location is with respect to a closest of the first wireless RX, the second wireless RX, and the wireless TX.",Neural network
Convolutional neural network,"A convolutional neural network (CNN) for an image processing system comprises an image cache responsive to a request to read a block of NÃ—M pixels extending from a specified location within an input map to provide a block of NÃ—M pixels at an output port. A convolution engine reads blocks of pixels from the output port, combines blocks of pixels with a corresponding set of weights to provide a product, and subjects the product to an activation function to provide an output pixel value. The image cache comprises a plurality of interleaved memories capable of simultaneously providing the NÃ—M pixels at the output port in a single clock cycle. A controller provides a set of weights to the convolution engine before processing an input map, causes the convolution engine to scan across the input map by incrementing a specified location for successive blocks of pixels and generates an output map within the image cache by writing output pixel values to successive locations within the image cache.","1. A convolutional neural network (CNN) for an image processing system comprising:
an image cache comprising an input port and an output port, said image cache being responsive to a request to read a block of NÃ—M pixels extending from a specified location within an input map to provide said block of NÃ—M pixels at said output port;
a convolution engine being arranged to read at least one block of NÃ—M pixels from said image cache output port, to combine said at least one block of NÃ—M pixels with a corresponding set of weights to provide a product, and to subject said product to an activation function to provide an output pixel value;
said image cache being configured to write output pixel values to a specified write address via said image cache input port;
said image cache comprising a plurality of interleaved memories, each memory storing a block of pixel values at a given memory address, the image cache being arranged to determine for a block of NÃ—M pixels to be read from said image cache: a respective one address within each of said interleaved memories in which said pixels of said block of NÃ—M pixels are stored; a respective memory of said plurality of interleaved memories within which each pixel of said block of NÃ—M pixels is stored; and a respective offset for each pixel of said block of NÃ—M pixels within each memory address, so that said image cache can simultaneously provide said NÃ—M pixels at said output port in a single clock cycle; and
a controller arranged to provide a set of weights to said convolution engine before processing at least one input map, to cause said convolution engine to process said at least one input map by specifying locations for successive blocks of NÃ—M pixels and to generate an output map within said image cache by writing said output pixel values to successive locations within said image cache, wherein said weights are stored as 8-bit floating point number with an exponent bias greater that 7D.",Neural network
Neural network training system,"In order for the feature extractors to operate with sufficient accuracy, a high degree of training is required. In this situation, a neural network implementing the feature extractor may be trained by providing it with images having known correspondence. A 3D model of a city may be utilized in order to train a neural network for location detection. 3D models are sophisticated and allow manipulation of viewer perspective and ambient features such as day/night sky variations, weather variations, and occlusion placement. Various manipulations may be executed in order to generate vast numbers of image pairs having known correspondence despite having variations. These image pairs with known correspondence may be utilized to train the neural network to be able to generate feature maps from query images and identify correspondence between query image feature maps and reference feature maps. This training can be accomplished without requiring the capture of real images with known correspondence. Capture of real images with known correspondence is cumbersome, time and resource-intensive, and difficult to manage.","1. A method for training a neural network implementation of a feature extractor and a correspondence matcher comprising the steps of:
set rendering parameters in a 3D model wherein said rendering parameters include at least position and orientation;
rendering a frame according to said rendering parameters to establish a current image;
processing ray-tracing information of said frame to solve a ground-truth correspondence map of said current image with a neighbor image;
incrementing said rendering parameters; and
providing said ground-truth correspondence map as a first training input to said neural network.",Neural network
Convolutional neural networks,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for keyword spotting. One of the methods includes training, by a keyword detection system, a convolutional neural network for keyword detection by providing a two-dimensional set of input values to the convolutional neural network, the input values including a first dimension in time and a second dimension in frequency, and performing convolutional multiplication on the two-dimensional set of input values for a filter using a frequency stride greater than one to generate a feature map.","1. A non-transitory computer readable storage medium storing instructions executable by a data processing apparatus and upon such execution cause the data processing apparatus to perform operations comprising:
training, by a keyword detection system, a convolutional neural network that comprises a first convolutional layer and a second convolutional layer, the convolutional neural network being trained for keyword detection of one or more key phrases by:
providing a two-dimensional set of input values to the convolutional neural network, the input values including values across a first dimension in time and values across a second dimension in frequency;
selecting, for use with the first convolutional layer only of the convolutional neural network, a filter having (i) a time span that extends over all of the input values in the first dimension, and (ii) a frequency span that extends over less than all of the input values in the second dimension;
performing convolutional multiplication on the two-dimensional set of input values for the filter that has (i) the time span that extends over all of the input values in the first dimension, and (ii) the frequency span that extends over of less than all of the input values in the second dimension, using (i) a frequency stride greater than one, and (ii) a time stride equal to one, to generate a first output comprising a feature map; generating, by the second convolutional layer of the convolutional neural network, using the feature map, a second output;
generating, by a linear low rank layer, using the second output, a third output;
generating, by a deep neural network, using the third output, a fourth output;
generating, by a softmax layer, using the fourth output, a final output; and
updating, using the final output, a set of weight values for the filter without pooling values in the feature map in frequency; and
after training the convolutional neural network for keyword detection, providing, by the keyword detection system, the trained convolutional neural network to a device for use by the device during keyword detection of the one or more key phrases.",Neural network
Spiking neural network,"Broadly speaking, embodiments of the present technique provide a neuron for a spiking neural network, where the neuron is formed of at least one Correlated Electron Random Access Memory (CeRAM) element or Correlated Electron Switch (CES) element.","1. A spiking neuron for a spiking neural network, the spiking neuron comprising:
a correlated electron switch (CES) element for implementing a thresholding function of the spiking neuron;
an accumulator circuit for summing current signals received by the spiking neuron to provide an accumulated current signal;
a further CES element for storing the accumulated current signal as a compliance current;
wherein the CES element stores a threshold current value corresponding to a compliance current, the spiking neuron further including:
a comparator circuit for comparing the accumulated current signal with the threshold current value stored by the CES element and outputting a spike signal if the accumulated current signal is greater than or equal to the threshold current value, the comparator circuit including:
a first mirror circuit for mirroring the accumulated current signal stored in the further CES element; and
a second mirror circuit for mirroring the threshold current value stored by the CES element.",Neural network
Numerical representation for neural networks,"Techniques in advanced deep learning provide improvements in one or more of accuracy, performance, and energy efficiency. An array of processing elements comprising a portion of a neural network accelerator performs flow-based computations on wavelets of data. Each processing element has a respective compute element and a respective routing element. Each compute element has a respective floating-point unit enabled to optionally and/or selectively perform floating-point operations in accordance with a programmable exponent bias and/or various floating-point computation variations. In some circumstances, the programmable exponent bias and/or the floating-point computation variations enable neural network processing with improved accuracy, decreased training time, decreased inference latency, and/or increased energy efficiency.","1. An apparatus comprising:
a programmable processor enabled to execute instructions, the instructions comprising floating-point instructions, and first and second load control resource instructions;
a first control resource of the programmable processor enabled to receive an exponent bias responsive to an operation corresponding to the first load control resource instruction;
a floating-point unit of the programmable processor enabled to perform floating-point operations corresponding to the floating-point instructions, the performing floating-point operations being in accordance with floating-point operands and floating-point results each comprising a respective biased exponent, the performing floating-point operations being responsive to the exponent bias, and compatible, according to the exponent bias, with interpreting the biased exponents of the floating-point operands and producing the biased exponents of the floating-point results; and
wherein
the apparatus further comprises a second control resource of the programmable processor enabled to receive an exponent size specifier responsive to an operation corresponding to the second load control resource instruction,
the floating-point unit is further enabled to perform the floating-point operations responsive to the exponent size specifier specifying one of a plurality of floating-point formats, and a first of the plurality of floating-point formats comprises a biased exponent of N bits and a mantissa of M bits, and a second of the plurality of floating-point formats comprises a biased exponent of (N+delta) bits and a mantissa of (M-delta) bits, and
the performing floating-point operations is further compatible, according to the exponent size specifier, with interpreting the biased exponents of the floating-point operands and producing the biased exponents of the floating-point results.",Neural network
Method of training a neural network,"A state vector (SVt) is determined with elements that characterize a financial market (101). Taking into account predetermined evaluation variables, an evaluation (Vt) is determined (102) for the state vector (SVt). In addition, a chronologically following state vector (SVt+1) is determined (103) and evaluated (Vt+1). On the basis of the two evaluations (Vt, Vt+1), weights (wi) of the neural network (NN) are adapted (104) using a reinforcement learning method (DELTAwi).","1. A method for training a neural network, comprising the following steps that are iteratively executed:
determining a state vector that has elements that characterize a financial market;
determining for the state vector, an evaluation relating to predetermined evaluation variables; and
adapting, using a reinforcement learning method, weights of the neural network, at least based on the evaluation of this state vector and based on a determined evaluation of at least one following state vector.",Neural network
Artificial neural network,An artificial neural network comprises at least one input layer with a predetermined number of input nodes and at least one output layer with a predetermined number of output nodes or also at least one intermediate hidden layer with a predetermined number of nodes between the input and the output layer. At least the nodes of the output layer and/or of the hidden layer and/or also of the input layer carry out a non linear transformation of a first non linear transformation of the input data for computing an output value to be fed as an input value to a following layer or the output data if the output layer is considered.,"1. A neural network comprising:
a plurality of nodes forming at least two layers, a first such layer being an input layer and a last such layer being an output layer, said input layer nodes and said output layer nodes being communicably connected;
wherein, in operation, input data from a database is input to said input layer, and the results of processing said data are output from the output layer, the output layer nodes forming output channels;
wherein each node of the output layer outputs a transformation into output data of the input data received from the input layer, said transformation comprising:
a first transformation step comprising at least one sub-step summing the input data received from the input nodes to said output nodes by weighting the said input data, and
a second transformation step which nonlinearly transforms the results of the first transformation step,
wherein in each output node said first transformation step comprises two substeps:
a first sub-step being a nonlinear transformation function of the input data received by the output nodes from the input nodes, and
the second sub-step being said summing step of said input data which has been nonlinearly transformed in said first sub-step, and
wherein said neural network is implemented in a computer having a processor and memory, said computer arranged to input the data from the database to the input layer, perform the summing and transformations of data at each node of the output layer and provide the output data to a user.",Neural network
NEURAL NETWORK IMAGE REPRESENTATION,"A method for representing an input image includes the steps of applying a trained neural network on the input image, selecting a plurality of feature maps, determining a location of each of the plurality of feature maps in an image space of the input image, defining a plurality of interest points of the input image, and employing the plurality of interest points for representing the input image for performing a visual task. The plurality of feature maps are selected of an output of at least one selected layer of the trained neural network according to values attributed to the plurality of feature maps by the trained neural network. The plurality of interest points of the input image are defined based on the locations corresponding to the plurality of feature maps.","1. A method for representing an input image, the method comprising the following procedures:
applying a trained neural network on said input image;
selecting a plurality of feature maps of an output of at least one selected layer of said trained neural network according to values attributed to said plurality of feature maps by said trained neural network;
for each of said plurality of feature maps, determining a location corresponding thereto in an image space of said input image;
defining a plurality of interest points of said input image, based on said locations corresponding to said plurality of feature maps; and
representing said input image as a graph according to said plurality of interest points and according to geometric relations between interest points of said plurality of interest points;
employing said graph for performing said visual task;
wherein said graph comprises a plurality of vertices and edges; and
wherein said graph maintains data respective of said geometric relations between said interest points.",Neural network
Visualizing convolutional neural networks,"Convolutional neural networks can be visualized. For example, a graphical user interface (GUI) can include a matrix of symbols indicating feature-map values that represent a likelihood of a particular feature being present or absent in an input to a convolutional neural network. The GUI can also include a node-link diagram representing a feed forward neural network that forms part of the convolutional neural network. The node-link diagram can include a first row of symbols representing an input layer to the feed forward neural network, a second row of symbols representing a hidden layer of the feed forward neural network, and a third row of symbols representing an output layer of the feed forward neural network. Lines between the rows of symbols can represent connections between nodes in the input layer, the hidden layer, and the output layer of the feed forward neural network.","1. A system for visualizing convolutional neural networks, the system comprising:
a processing device; and
a memory device on which instructions executable by the processing device are stored for causing the processing device to:
generate a matrix of symbols to be positioned in a graphical user interface, each symbol in the matrix indicating a feature-map value that represents a likelihood of a particular feature being present or absent at a location in an input to a convolutional neural network, each column in the matrix having feature-map values generated by convolving the input to the convolutional neural network with a respective filter for identifying a specific feature in the input;
generate a node-link diagram to be positioned in the graphical user interface, wherein the node-link diagram represents a feed forward neural network that forms part of the convolutional neural network and comprises:
a first row of symbols representing an input layer to the feed forward neural network, wherein the input layer is also a maxpooling layer of the convolutional neural network, the first row of symbols is color coded to represent activation values for nodes in the input layer, and each symbol in the first row of symbols is vertically aligned with a respective column in the matrix of symbols and indicates a maximum value in the respective column;
a second row of symbols representing a hidden layer of the feed forward neural network, the second row of symbols being color coded to represent activation values of nodes in the hidden layer;
a third row of symbols representing an output layer of the feed forward neural network, the third row of symbols being color coded to represent activation values of nodes in the output layer; and
lines between the first row of symbols, the second row of symbols, and the third row of symbols, the lines representing connections between nodes in the input layer, the hidden layer, and the output layer of the feed forward neural network;
generate the graphical user interface at least in part by positioning the matrix of symbols above and adjacent to the node-link diagram in the graphical user interface; and
transmit a display communication to a display device for causing the display device to output the graphical user interface.",Neural network
Neural network verification,"Systems and methods associated with neural network verification are disclosed. One example method may be embodied on a non-transitory computer-readable medium storing computer-executable instructions. The instructions, when executed by a computer, may cause the computer to train a neural network with a training data set to perform a predefined task. The instructions may also cause the computer to train the neural network with a sentinel data set. The sentinel data set may cause the neural network to provide an identification signal in response to a predefined query set. The instructions may also cause the computer to verify whether a suspicious service operates an unauthorized copy of the neural network. The suspicious service may be verified by extracting the identification signal from responses the suspicious service provides to the predefined query set.","1. A non-transitory computer-readable medium storing computer-executable instructions that when executed by a computer cause the computer to:
train a neural network with a training data set to perform a predefined task;
train the neural network with a sentinel data set, where the sentinel data set causes the neural network to provide an identification signal in response to a predefined query set;
provide the predefined query set, the identification signal, and access to the neural network to a trusted third party, where the trusted third party confirms whether the predefined query set, the identification signal, and the neural network conflict with an archived query set, an archived identification signal, and an archived neural network, and where the trusted third party confirms whether the neural network provides the identification signal in response to the predefined query set;
verify whether a suspicious service operates an unauthorized copy of the neural network by extracting the identification signal from responses the suspicious service provides upon receiving the predefined query set; and
receive a confirmation from the trusted third party when the suspicious service operates the unauthorized copy of the neural network.",Neural network
Category learning neural networks,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for determining a clustering of images into a plurality of semantic categories. In one aspect, a method comprises: training a categorization neural network, comprising, at each of a plurality of iterations: processing an image depicting an object using the categorization neural network to generate (i) a current prediction for whether the image depicts an object or a background region, and (ii) a current embedding of the image; determining a plurality of current cluster centers based on the current values of the categorization neural network parameters, wherein each cluster center represents a respective semantic category; and determining a gradient of an objective function that includes a classification loss and a clustering loss, wherein the clustering loss depends on a similarity between the current embedding of the image and the current cluster centers.","1. A method, comprising:
training a categorization neural network to determine trained values of the categorization neural network parameters from initial values of the categorization neural network parameters, comprising, at each of a plurality of iterations:
processing a given image depicting an object using the categorization neural network in accordance with current values of categorization neural network parameters to generate an output comprising: (i) a current prediction for whether the given image depicts an object or a background region, and (ii) a current embedding of the given image;
determining a plurality of current cluster centers based on the current values of the categorization neural network parameters, wherein each cluster center represents a respective semantic category;
determining a gradient of an objective function that includes a classification loss and a clustering loss, wherein the classification loss depends on the current prediction for whether the given image depicts an object or a background region, and wherein the clustering loss depends on a similarity between the current embedding of the given image and the current cluster centers; and
determining an update to the current values of the categorization neural network parameters from the gradient;
determining a plurality of final cluster centers based on the trained values of the categorization neural network parameters;
for each of a plurality of target images, processing the target image using the categorization neural network in accordance with the trained values of the categorization neural network parameters to generate a final embedding of the target image; and
determining a clustering of the target images into a plurality of semantic categories using the final embeddings of the target images and the final cluster centers.",Neural network
Neural network for program synthesis,"Described are systems, methods, and computer-readable media for program generation in a domain-specific language based on input-output examples. In accordance with various embodiments, a neural-network-based program generation model conditioned on an encoded set of input-output examples is used to generate a program tree by iteratively expanding a partial program tree, beginning with a root node and ending when all leaf nodes are terminal.","1. A method comprising:
for a given domain-specific language that defines a plurality of symbols and a plurality of production rules, providing an input-output encoder and a program-generation model comprising a neural network, the input-output encoder and the neural network having been trained on a plurality of programs within the domain-specific language and a plurality of respective training sets of input-output examples associated with the programs, wherein, for each of the plurality of programs and its associated training set, each input-output example of the training set comprises a pair of an input to the program and a corresponding output produced by the program from the input;
providing a test set of input-output examples for a target program;
using one or more hardware processors to perform operations for generating the target program based on the test set of input-output examples, the operations comprising:
encoding the test set of input-output examples using the input-output encoder;
conditioning the program-generation model on the encoded set of input-output examples; and
using the neural network to generate a program tree representing the target program by iteratively expanding a partial program tree, beginning with a root node and ending when all leaf nodes are terminal, based on a computed probability distribution for a set of valid expansions, wherein leaves in the program tree and the partial program tree represent symbols in the domain-specific language and wherein non-leaf interior nodes in the program tree and the partial program tree represent production rules in the domain-specific language.",Neural network
Neural networks for biometric recognition,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training an encoder neural network having multiple encoder neural network parameters. The encoder neural network is configured to process a biometric data sample in accordance with current values of encoder neural network parameters to generate as output an embedded representation of the biometric data sample. The embedded representation includes: (i) an inter-class embedded representation, and (ii) an intra-class embedded representation that is different than the inter-class embedded representation.","1. A method for training an encoder neural network having a plurality of encoder neural network parameters and being configured to process a biometric data sample in accordance with current values of encoder neural network parameters to generate as output an embedded representation of the biometric data sample, wherein the embedded representation of the biometric data sample defines a set of features representing the biometric data sample, the method comprising:
obtaining a positive biometric data sample characterizing an identity of a first person and a negative biometric data sample characterizing an identity of a second person, wherein the identity of the first person is different than the identity of the second person;
processing the positive biometric data sample and the negative biometric data sample using the encoder neural network and in accordance with the current values of the encoder neural network parameters to generate: (i) an embedded representation of the positive biometric data sample that defines a set of features representing the positive biometric data sample, and (ii) an embedded representation of the negative biometric sample that defines a set of features representing the negative biometric data sample;
determining a gradient of a loss function with respect to the encoder neural network parameters, wherein the loss function includes a first term that encourages a similarity between: (i) a specified proper subset of the set of features defined by the embedded representation of the positive biometric data sample and (ii) a corresponding specified proper subset of the set of features defined by the embedded representation of the negative biometric data sample, to be less than a similarity between: (i) the set of features defined by the embedded representation of the positive biometric data sample and (ii) the set of features defined by the embedded representation of the negative biometric data sample; and
adjusting the current values of the encoder neural network parameters using the gradient of the loss function.",Neural network
Audio processing with neural networks,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for audio processing using neural networks. One of the systems includes multiple neural network layers, wherein the neural network system is configured to receive time domain features of an audio sample and to process the time domain features to generate a neural network output for the audio sample, the plurality of neural network layers comprising: a frequency-transform (F-T) layer that is configured to apply a transformation defined by a set of F-T layer parameters that transforms a window of time domain features into frequency domain features; and one or more other neural network layers having respective layer parameters, wherein the one or more neural network layers are configured to process frequency domain features to generate a neural network output.","1. A method for training a neural network that includes a plurality of neural network layers on training data,
wherein the neural network is configured to receive as input time domain features of an audio sample and to process the time domain features to generate a neural network output from the audio sample,
wherein the neural network comprises (i) a frequency-transform (F-T) layer and (ii) one or more other neural network layers having respective layer parameters, wherein the F-T layer is configured to apply a transformation defined by a set of F-T layer parameters that transforms a window of time domain features that are received as input by the neural network into frequency domain features that are processed by the one or more other neural network layers to generate the neural network output, and
wherein the method comprises:
obtaining training data comprising, for each of a plurality of training audio samples, time domain features of the training audio sample and a known output for the training audio sample; and
training the neural network on the training data to adjust the values of the parameters of the other neural network layers and to adjust the values of the F-T layer parameters to update the transformation applied by the F-T layer that transforms time-domain features that are input to the neural network to frequency-domain features that are processed by the one or more other neural network layers to generate neural network outputs.",Neural network
Neural network processor,"Each processor of the SIMD array performs the computations for a respective neuron of a neural network. As part of this computation, each processor of the SIMD array multiplies an input to a weight and accumulates the result for its assigned neuron each (MAC) instruction cycle. A table in a first memory is used to store which input is fed to each processor of the SIMD array. A crossbar is used to route a specific input to each processor each MAC cycle. A second memory is used to provide the appropriate weight to each processor that corresponds the input being routed to that processor.","1. A device comprising:
an integrated circuit including:
an N-way single-instruction multiple data (SIMD) array of a plurality of processors where each processor of the array includes a multiply-accumulate unit having a respective accumulator; and
a crossbar to provide a respective selected neural network input value to each of the N processors, the N number of selected neural network input values selected from M number of input values, wherein M is greater than N.",Neural network
Reconfigurable neural network,"A reconfigurable neural network is disclosed. The neural network includes a plurality of switches each having at least two conductive leads, wherein data flow direction of the conductive leads of the switches is programmed to select one of the conductive leads as input switch lead and select another one of the conductive leads as an output switch lead. A plurality of processing elements each having a plurality of leads connected to the switches, wherein the processing elements and the switches are interconnected in one-dimension manner. Each of the processing elements comprising: (a) a serial-in-parallel-out accumulator having a first input coupled to one of the interconnected switches and generating a first output; (b) an activation function for transforming the first output of the serial-in-parallel-out accumulator and generating a second output; and (c) a parallel-in-serial-out shift register for shifting out the second output of the activation function serially to one of the interconnected switches.","1. A neural network circuit, comprising:                                   
first switching means having a first lead, a second lead and a third lead; 
second switching means having a first lead, a second lead and a third lead, said second lead of said second switching means coupled to said second     lead of said first switching means;                                       
a first processing element (PE) having a first input lead, a second input   lead and an output lead, said first input lead of said first PE coupled to said third lead of said first switching means, said second input lead      coupled to said third lead of said second switching means, wherein said    first PE outputting at said output lead of said first PE a serial digital  output signal that is weighted sum function of a first serial digital      input signal received at said first input lead of said first PE;          
third switching means having a first lead, a second lead and a third lead,  said first lead of said third switching means coupled to said first input  lead of said first PE;                                                    
fourth switching means having a first lead, a second lead, and a third      lead, said first lead of said fourth switching means coupled to said       output lead of said first PE, said second lead of said fourth switching    means coupled to said second lead of said third switching means; and      
a second PE having a first input lead, a second input lead and an output    lead, said first input lead coupled to said third lead of said fourth      switching means, said second input lead of said second PE coupled to said  third lead of said fourth switching means, wherein said second PE          outputting at said output lead of said second PE a serial digital output   signal that is a weighted sum function of a serial digital input signal    received at said first input lead of said second PE, wherein:             
said first switching means being selectably configured to provide a signal  received on said first lead of said first switching means to said first    input lead of said first PE or a signal received from said second lead of  said second switching means to said first input lead of said first PE,    
said second switching means being selectably configured to provide a signal received on said first lead of said second switching means to said second  input lead of said first PE or to said second lead of said first switching means,                                                                    
said third switching means being selectably configured to provide a signal  received from said third lead of said first switching means to said first  input lead of said second PE or a signal received from said second lead of said fourth switching means to said first input lead of said second PE,    and                                                                       
said fourth switching means being selectably configured to provide a signal received from said output lead of said first PE to said second lead of     said fourth switching means or to said second input lead of said second    PE.",Neural network
Support Vector Machine,"A method for operating a computer as a support vector machine (SVM) in order to define a decision surface separating two opposing classes of a training set of vectors. The method involves associating a distance parameter with each vector of the SVM's training set. The distance parameter indicates a distance from its associated vector, being in a first class, to the opposite class. A number of approaches to calculating distance parameters are provided. For example, a distance parameter may be calculated as the average of the distances from its associated vector to each of the vectors in the opposite class. The method further involves determining a linearly independent set of support vectors from the training set such that the sum of the distances associated with the linearly independent support vectors is minimized.","1. A computer implemented method for operating a computational device as a support vector machine to discriminate a training set of vectors corresponding to a plurality of digital images into opposing first and second classes of vectors wherein the first class of vectors corresponds to a first class of the digital images and the second class corresponds to a second class of the digital images, the method including the steps of:
associating a distance parameter with each vector of the training set, the distance parameter indicating a distance from its associated vector to the opposite class; and
determining a linearly independent set of support vectors from the training set such that the sum of the distances associated with the linearly independent support vectors is minimized.",Support vector machine
Lagrangian support vector machine,"A Lagrangian support vector machine solves problems having massive data sets (e.g., millions of sample points) by defining an input matrix representing a set of data having an input space with a dimension of n that corresponds to a number of features associated with the data set, generating a support vector machine to solve a system of linear equations corresponding to the input matrix with the system of linear equations defined by a positive definite matrix, and calculating a separating surface with the support vector machine to divide the set of data into two subsets of data","1. A computer-implemented method of classifying numerical data sets comprising the steps of:
defining an input matrix A with m rows and n columns representing a set of m numerical data points having an input space with a dimension of n, wherein n corresponds to a number of features associated with the numerical data set and each row belongs to either class A+ or Aâˆ’, and further wherein the numerical data set represents medical data;
generating a support vector machine by solving a quadratic programming problem corresponding to the input matrix, wherein the quadratic programming problem is defined by a positive definite matrix Q:
##EQU##
wherein H=D[Aâˆ’e], Hâ€² is the transpose of H, I is an identity matrix, e is a vector of ones, D is a diagonal matrix of plus and minus ]'s wherein a value on a diagonal of the D matrix is +1 if the corresponding row of the A matrix is in the class of A+ and âˆ’1 if the correspondina row of the A matrix is in the class of Aâˆ’, and v is a parameter associated with a distance between a pair of parallel bounding planes;
calculating a linear separating surface with the support vector machine by iteratively calculating a value u defined by:
ui+1=Qâˆ’1(e+((Quiâˆ’e)âˆ’Î±ui)+),i=0,1, . . .
wherein
##EQU##
and the + subscript replaces negative components by zeros; and
dividing the set of numerical data into a plurality of subsets of data using the linear separating surface in the n-dimensional x space: xâ€²w=Î³, where xâ€² is the transpose of a vector x, w is orthogonal to the separating surface and Î³ locates the separating surface relative to an origin, wherein the plurality of subsets include at least a good prognostic set of medical data and a poor prognostic set of medical data that are located on opposite sides of the separating surface.",Support vector machine
Support vector machines processing system,"An implementation of SVM functionality improves efficiency, time consumption, and data security, reduces the parameter tuning challenges presented to the inexperienced user, and reduces the computational costs of building SVM models. A system for support vector machine processing comprises data stored in the system, a client application programming interface operable to provide an interface to client software, a build unit operable to build a support vector machine model on at least a portion of the data stored in the system, based on a plurality of model-building parameters, a parameter estimation unit operable to estimate values for at least some of the model-building parameters, and an apply unit operable to apply the support vector machine model using the data stored in the system.","1. A computerized method for support vector machine processing comprising:
storing data;
providing an interface to client software;
building a support vector machine model on at least a portion of the stored data, based on a plurality of model-building parameters;
estimating values for at least some of the model-building parameters; and applying the support vector machine model using the stored data to generate a data mining output;
wherein the parameter estimation is performed by selecting a complexity parameter C method for a linear kernel method by:
selecting a random sample comprising k pairs of training vectors of opposite class, one member of each pair being of positive class and one member of each pair being of negative class;
for each pair, assigning C to have a value such that a margin for the positive class member of the pair is 1 and a margin for the negative class member of the pair is -1;
ordering the assigned values of C; and
selecting as C a k-th highest assigned value of C.",Support vector machine
Spread Kernel Support Vector Machine,"Disclosed is a parallel support vector machine technique for solving problems with a large set of training data where the kernel computation, as well as the kernel cache and the training data, are spread over a number of distributed machines or processors. A plurality of processing nodes are used to train a support vector machine based on a set of training data. Each of the processing nodes selects a local working set of training data based on data local to the processing node, for example a local subset of gradients. Each node transmits selected data related to the working set (e.g., gradients having a maximum value) and receives an identification of a global working set of training data. The processing node optimizes the global working set of training data and updates a portion of the gradients of the global working set of training data. The updating of a portion of the gradients may include generating a portion of a kernel matrix. These steps are repeated until a convergence condition is met. Each of the local processing nodes may store all, or only a portion of, the training data. While the steps of optimizing the global working set of training data, and updating a portion of the gradients of the global working set, are performed in each of the local processing nodes, the function of generating a global working set of training data is performed in a centralized fashion based on the selected data (e.g., gradients of the local working set) received from the individual processing nodes.","1. A method for training a support vector machine, comprising the steps of:
a) selecting, via a processor of a first processing node, a local working set of training data based on local training data stored in a memory of the first processing node;
b) transmitting, via a network interface of the first processing node, certain gradients to a second processing node, the certain gradients selected from gradients of the working set of training data;
c) receiving at the network interface of the first processing node an identification of a global working set of training data;
d) executing, via the processor of the first processing node, a quadratic function stored in a storage device of the first processing node to optimize said global working set of training data;
e) updating gradients of the training data stored in the memory of the first processing node; and
f) repeating said steps a) through e) until a convergence condition is met.",Support vector machine
Gender classification with support vector machines,"A method classifies images of faces according to gender. Training images of male and female faces are supplied to a vector support machine. A small number of support vectors are determined from the training images. The support vectors identify a hyperplane. After training, a test image is supplied to the support vector machine. The test image is classified according to the gender of the test image with respect to the hyperplane.","1. A method for classifying images of faces according to gender, comprising the steps of:
supplying a vector support machine with a plurality of training images, the training images including images of male and female faces;
determining a plurality of support vectors from the training images for identifying a hyperplane;
supplying the support vector machine with a test image;
classifying the gender of the test image with respect to the hyperplane; and
reducing the resolution of the training images and the test image by sub-sampling before supplying the images to the support vector machine;
wherein the test image contains less than 260 pixels.",Support vector machine
SUPPORT VECTOR MACHINE LEARNING SYSTEM AND SUPPORT VECTOR MACHINE LEARNING METHOD,"[Solution] An analysis executing apparatus that performs support vector machine learning, stores a set of learning data including a feature vector and a label encrypted using an additive homomorphic encryption scheme, which are subjected to the support vector machine learning, and performs update processing with a gradient method on the encrypted learning data using an additive homomorphic addition algorithm.","1. A support vector machine learning system that performs support vector machine learning, comprising:
a learning data management apparatus; and
a learning apparatus coupled to the learning data management apparatus, wherein
the learning data management apparatus comprises:
a learning data storage part that stores a set of learning data including a label and a feature vector, the set of learning data being subjected to the support vector machine learning;
an encryption processing part that encrypts the label of the learning data using an additive homomorphic encryption scheme; and
a learning data transmitting part that transmits encrypted learning data including the encrypted label and the feature vector to the learning apparatus, and wherein
the learning apparatus comprises:
a learning data receiving part that receives the encrypted learning data; and
an update processing part that performs update processing with a gradient method on the encrypted learning data using an additive homomorphic addition algorithm.",Support vector machine
SUPPORT VECTOR MACHINE,"A method of building a classification model using a SVM training module comprising, with a processor, computing a mean value of a number of training vectors received by the processor, subtracting the mean value of the number of training vectors from each training vector received by the processor to obtain a number of difference vectors, applying a hash function to each of the difference vectors to obtain a number of hashed vectors, and applying a linear training formula to the hashed vectors to obtain a classifier model. Classifying a sample vector comprises, with a processor, subtracting a mean value of a number of support vector machine training vectors from the sample vector to obtain a sample difference vector, with a processor, applying a hash function to the sample difference vector to obtain a hashed sample vector, and classifying the hashed sample vector using a classifier model.","1. A method of building a classification model using a SVM training module comprising:
with a processor:
computing a mean value of a number of training vectors received by the processor;
subtracting the mean value of the number of training vectors from each training vector received by the processor to obtain a number of difference vectors;
applying a hash function to each of the difference vectors to obtain a number of hashed vectors; and
applying a linear training formula to the hashed vectors to obtain a classifier model.",Support vector machine
Building support vector machines with reduced classifier complexity,"Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem a primal system and method with the following properties has been devised: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (dmax) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(ndmax2) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors.","1. A computerized method for learning for categorizing elements and generating search results, comprising:
building a searchable index of documents in a database wherein the database is searchable to return a result set;
using a classifier to separate documents returned in the result set,
establishing an empty set of basis functions associated with the classifier;
storing, in a computer readable medium, the empty set of basis functions associated with the classifier;
selecting a kernel basis function from a collection of training basis functions located at one or more training points;
adding the selected basis function to the set of basis functions;
incrementing an actual basis functions count;
optimizing one or more parameters associated with the set of basis functions;
comparing a predetermined number of basis functions against the actual basis functions count;
repeating the steps of selecting, adding, incrementing and optimizing until the actual basis functions count reaches the predetermined number of basis functions;
generating a basis set; and
generating a search result set in response to a search request on the database based on the basis set.",Support vector machine
Training a support vector machine with process constraints,"System and method for training a support vector machine (SVM) with process constraints. A model (primal or dual formulation) implemented with an SVM and representing a plant or process with one or more known attributes is provided. One or more process constraints that correspond to the one or more known attributes are specified, and the model trained subject to the one or more process constraints. The model includes one or more inputs and one or more outputs, as well as one or more gains, each a respective partial derivative of an output with respect to a respective input. The process constraints may include any of: one or more gain constraints, each corresponding to a respective gain; one or more Nth order gain constraints; one or more input constraints; and/or one or more output constraints. The trained model may then be used to control or manage the plant or process.","1. A method, comprising:
providing a model, wherein the model comprises a representation of a plant or process implemented with a support vector machine (SVM), wherein the model comprises one or more inputs and one or more outputs, and wherein the plant or process has one or more known attributes;
specifying one or more process constraints that correspond to the one or more known attributes; and
training the model subject to the one or more process constraints.",Support vector machine
SUPPORT VECTOR MACHINE FOR BIOMETRIC DATA PROCESSING,"A system, method and program product for processing biometric data. A biometric data processing system is disclosed that includes: at least one signal acquisition system for collecting biometric input; a feature extraction system for extracting feature vectors from the biometric input; and a support vector machine (SVM) having a plurality of kernel functions, wherein each kernel function is configured for mapping a feature vector to a high dimensional hyperspace structure.","1. A biometric data processing system, comprising:
at least one signal acquisition system for collecting biometric input;
a feature extraction system for extracting feature vectors from the biometric input; and
a support vector machine (SVM) having a plurality of kernel functions,
wherein each kernel function is configured for mapping a feature vector to a high dimensional hyperspace structure,
wherein the high dimensional hyperspace structure is defined as follows:
H=Hyperspace
Î›i=Sub-Universe
Î©w=World
Ï†i,j=Dimension
Ïi,j=Policy
Î´i,j,k=Operator
Al=Cluster
Î±l,i,j=Bin Member
where i=sub-universe number, j=dimension number, k=operator number, l=cluster number, m=world number,
Al={âˆ€l,i,j}
where a cluster is defined as a set of all cluster members, each cluster member being vectored into a high dimensional space,
Î›i={âˆ€i,j}
where a sub-universe is defined as a set of dimensions,
Ï†i,j=âˆƒi,jU{âˆ€i,j,k}
where for each dimension there exists an associated policy and a set of operators, the policy providing an association between operators or heuristics and a dimension,
Î©w={{{âˆ€l,i,j}Îµ{Î›i}}U{Î›i}
where a world automaton specifies the set of all data elements which belong to all clusters within a sub-universe and the universe,
H={âˆ€Î©w}
where a hyperspace automaton defines the universe of a problem domain,
where each kernel function is a parametric function that projects dimensional data onto the high dimensional hyperspace structure, each kernel function including a plurality of multiple independent kernels, where
k(x,xâ€²)=k1(x,xâ€²)+k2(x,xâ€²) . . .
defines a kernel function from a summation of the plurality of multiple independent kernels.",Support vector machine
TASK ASSIGNMENT USING RANKING SUPPORT VECTOR MACHINES,"A method of ranking workers for an incoming task includes recording a list of completed tasks in a computer data structure, extracting first attributes from the list for the tasks that were completed during a pre-determined period, generating a first feature vector for each task and worker from the first extracted attributes, training a Support Vector Machine (SVM) based on the feature vector to output a weight vector, extracting second attributes from an incoming task, generating a second feature vector for each worker based on the second extracted attributes, and ranking the workers using the second feature vectors and the weight vector. The first attributes may be updated during a subsequent period to re-train the SVM on updated first feature vectors to generate an updated weight vector. The workers may be re-ranked based on the second feature vectors and the updated weight vector. Accordingly, the feature vectors are dynamic.","1. A method of ranking workers for an incoming task, the method comprising:
extracting, by a processor, first attributes from a list of tasks that were completed during a pre-determined period;
generating, by a processor, a first feature vector for each task and worker from the first extracted attributes;
training, by a processor, a Support Vector Machine (SVM) based on the first feature vectors to output a weight vector;
extracting, by a processor, second attributes from an incoming task;
generating, by a processor, a second feature vector based on the second extracted attributes; and
ranking, by the processor, the workers for performing the incoming task using the second feature vector and the weight vector,
wherein each first feature vector indicates the number of the completed tasks by each worker having the same attribute.",Support vector machine
Controlling a fabrication tool using support vector machine,"A fabrication tool can be controlled using a support vector machine. A profile model of the structure is obtained. The profile model is defined by profile parameters that characterize the geometric shape of the structure. A set of values for the profile parameters is obtained. A set of simulated diffraction signals is generated using the set of values for the profile parameters, each simulated diffraction signal characterizing the behavior of light diffracted from the structure. The support vector machine is trained using the set of simulated diffraction signals as inputs to the support vector machine and the set of values for the profile parameters as expected outputs of the support vector machine. After the support vector machine has been trained, a fabrication process is performed using the fabrication tool to fabricate the structure on the wafer. A measured diffraction signal off the structure is obtained. The measured diffraction signal is inputted into the trained support vector machine. Values of profile parameters of the structure are obtained as an output from the trained support vector machine. One or more process parameters or equipment settings of the fabrication tool are adjusted based on the obtained values of the profile parameters.","1. A method of controlling a fabrication tool using a support vector machine, the method comprising:
a) obtaining a profile model of a structure, the profile model being defined by profile parameters that characterize the geometric shape of the structure;
b) obtaining a set of values for the profile parameters;
c) generating a set of simulated diffraction signals using the set of values for the profile parameters, each simulated diffraction signal characterizing the behavior of light diffracted from the structure;
d) training the support vector machine using the set of simulated diffraction signals as inputs to the support vector machine and the set of values for the profile parameters as expected outputs of the support vector machine;
e) after d), performing a fabrication process using a first fabrication tool to fabricate the structure on a wafer;
f) obtaining a measured diffraction signal off the structure fabricated on the wafer;
g) inputting the measured diffraction signal into the trained support vector machine;
h) after g), obtaining values of profile parameters of the structure as an output from the trained support vector machine; and
i) after h), adjusting one or more process parameters or equipment settings of the first fabrication tool based on the values of the profile parameters obtained in g).",Support vector machine
Effective multi-class support vector machine classification,"An improved method of classifying examples into multiple categories using a binary support vector machine (SVM) algorithm. In one preferred embodiment, the method includes the following steps: storing a plurality of user-defined categories in a memory of a computer; analyzing a plurality of training examples for each category so as to identify one or more features associated with each category; calculating at least one feature vector for each of the examples; transforming each of the at least one feature vectors so as reflect information about all of the training examples; and building a SVM classifier for each one of the plurality of categories, wherein the process of building a SVM classifier further includes: assigning each of the examples in a first category to a first class and all other examples belonging to other categories to a second class, wherein if any one of the examples belongs to another category as well as the first category, such examples are assigned to the first class only; optimizing at least one tunable parameter of a SVM classifier for the first category, wherein the SVM classifier is trained using the first and second classes; and optimizing a function that converts the output of the binary SVM classifier into a probability of category membership.","1. In a computer-based system, a method of training a multi-category classifier using a binary SVM algorithm, said method comprising:
storing a plurality of user-defined categories in a memory of a computer;
analyzing a plurality of training examples for each category so as to identify one or more features associated with each category;
calculating at least one feature vector for each of said examples;
transforming each of said at least one feature vectors using a first mathematical function so as to provide desired information about each of said training examples; and
building a SVM classifier for each one of said plurality of categories, wherein said process of building a SVM classifier comprises:
assigning each of said examples in a first category to a first class and all other examples belonging to other categories to a second class, wherein if any one of said examples belongs to both said first category and another category, such examples are assigned to the first class only;
optimizing at least one tunable parameter of a SVM classifier for said first categories, wherein said SVM classifier is trained using said first and second classes after the at least one tunable parameter has been optimized; and
optimizing a second mathematical function that converts the output of the binary SVM classifier into a probability of category membership;
calculating a solution for the SVM classifier for the first category using predetermined initial value(s) for said at least one tunable parameter; and testing said solution for said first category to determine if the solution is characterized by either over-generalization or over-memorization;
wherein the SVM classifier is used on real world data, the probability of category membership of the real world data being output to at least one of a user, another system, and another process;
wherein the test to determine whether said SVM classifier solution for said first category is characterized by either over-generalization or over-memorization is based on a difference between a harmonic mean of first and second estimated probabilities on the one hand, and an arithmetic mean of said first and second estimated probabilities on the other hand;
wherein the first estimated probability is indicative of class membership and the second estimated probability is indicative of non-class membership for training examples.",Support vector machine
Forward feature selection for support vector machines,"In one embodiment, the present invention includes a method for training a Support Vector Machine (SVM) on a subset of features (dâ€²) of a feature set having (d) features of a plurality of training instances to obtain a weight per instance, approximating a quality for the d features of the feature set using the weight per instance, ranking the d features of the feature set based on the approximated quality, and selecting a subset (q) of the features of the feature set based on the ranked approximated quality. Other embodiments are described and claimed.","1. A method comprising:
training, using a processor of a computer system, a Support Vector Machine (SVM) on a subset of features (dâ€²) of a feature set having (d) features of a plurality of training instances to obtain a weight per instance ({right arrow over (Î±)}â€²);
approximating, using the processor of the computer system, a quality for the d features of the feature set using the weight per instance;
ranking, using the processor of the computer system, the d features of the feature set based on the approximated quality;
selecting, using the processor of the computer system, a subset (q) of the features of the feature set based on the ranked approximated weights; and
iterating training the SVM, approximating the quality, ranking the d features, and selecting the q subset until the q subset is less than a selected threshold.",Support vector machine
Method and apparatus for improving the efficiency of support vector machines,"A method and apparatus is described for improving the efficiency of any machine that uses an algorithm that maps to a higher dimensional space in which a given set of vectors is used in a test phase. In particular, reduced set vectors are used. These reduced set vectors are different from the vectors in the set and are determined pursuant to an optimization approach other than the eigenvalue computation used for homogeneous quadratic kernels. An illustrative embodiment is described in the context of a support vector machine (SVM).","1. A method for using a support vector machine, the method comprising the steps of:
receiving input data signals; and
using the support vector machine operable on the input data signals for providing an output signal, wherein the support vector machine utilizes reduced set vectors, wherein the reduced set vectors were a priori determined during a training phase using an unconstrained optimization approach other than an eigenvalue computation used for homogeneous quadratic kernels wherein the training phase further comprises the steps of:
receiving elements of a training set;
generating a set of support vectors, the number of support vectors being NS;
selecting a number m of reduced set vectors, where m.ltoreq.NS; and
generating the number m of reduced set vectors using the unconstrained optimization approach.",Support vector machine
GRANULAR SUPPORT VECTOR MACHINE WITH RANDOM GRANULARITY,Methods and systems for granular support vector machines. Granular support vector machines can randomly select samples of datapoints and project the samples of datapoints into a randomly selected subspaces to derive granules. A support vector machine can then be used to identify hyperplane classifiers respectively associated with the granules. The hyperplane classifiers can be used on an unknown datapoint to provide a plurality of predictions which can be aggregated to provide a final prediction associated with the datapoint.,"1. A method comprising:
receiving a training dataset comprising a plurality of tuples and a plurality of attributes for each of the tuples;
deriving a plurality of granules from the training dataset, each granule comprising a plurality of sample tuples and a plurality of sample attributes, wherein for each of the plurality of granules:
the plurality of sample tuples is randomly selected from among the plurality of tuples with replacement; and
the plurality of sample attributes is randomly selected from among the plurality of attributes without replacement;
processing the granules using a support vector machine process to identify a hyperplane classifier associated with each of the granules;
predicting a classification of a new tuple using each of the hyperplane classifiers to produce a plurality of predictions;
aggregating the predictions to derive a decision on a final classification of the new tuple;
validating a first hyperplane classifier associated with a granule by classifying a plurality of tuples from the training dataset which were not included in the granule;
generating a hyperplane classifier effectiveness level based upon the validation of the first hyperplane classifier against tuples from the training dataset which were not included in the granule;
determining whether the hyperplane classifier effectiveness level exceeds a threshold effectiveness level; and
in response to determining that the hyperplane classifier effectiveness level does not exceed the threshold effectiveness level:
removing the first hyperplane classifier; and
requesting a second plurality of sample attributes, each sample attribute in the second plurality of sample attributes different from each sample attribute in the plurality of sample attributes, for use in identifying new hyperplane classifiers.",Support vector machine
System and method for boosting support vector machines,"A system and method for training an SVM in a scalable manner includes boosting the SVM during training. Specifically, individual SVMs in an ensemble of SVMs are trained using small subsets of a training data set, with data that earlier classifiers in the ensemble incorrectly classified being overrepresented in succeeding subsets. In this way, the speed with which the overall SVM is trained is increased and the memory requirements therefor are reduced, even for relatively large training data sets.","1. A computer-implemented method for training a support vector machine (SVM) useful in machine learning, comprising the acts of:
boosting the SVM.",Support vector machine
Method and apparatus for efficient training of support vector machines,"The present invention provides a system and method for building fast and efficient support vector classifiers for large data classification problems which is useful for classifying pages from the World Wide Web and other problems with sparse matrices and large numbers of documents. The method takes advantage of the least squares nature of such problems, employs exact line search in its iterative process and makes use of a conjugate gradient method appropriate to the problem. In one embodiment a support vector classifier useful for classifying a plurality of documents, including textual documents, is built by selecting a plurality of training documents, each training document having suitable numeric attributes which are associated with a training document vector, then initializing a classifier weight vector and a classifier intercept for a classifier boundary, the classifier boundary separating at least two document classes, then determining which training document vectors are suitable support vectors, and then re-computing the classifier weight vector and the classifier intercept for the classifier boundary using the suitable support vectors together with an iteratively reindexed least squares method and a conjugate gradient method with a stopping criterion.","1. A computer-readable storage medium storing a set of instructions for building a support vector classifier useful for classifying a plurality of documents, including textual documents, the set of instructions being executed by a processor, the set of instructions performing the steps of:
selecting a plurality of training documents, each training document having suitable numeric attributes which are associated with a training document vector;
initializing a classifier weight (w) and a classifier intercept (b) for a classifier boundary, the classifier boundary separating at least two document classes;
determining which training document vectors arc suitable support vectors;
re-computing the classifier weight vector and the classifier intercept for the classifier boundary using the suitable support vectors together with an iteratively reindexed least squares method and a conjugate gradient method with a stopping criterion; and
storing the recomputed classifier weight vector and the classifier intercept for the classifier boundary at a data store in communication with the processor.",Support vector machine
Data processing using support vector machines,"
A system and method for enhancing knowledge discovery from data using a learning machine in general and a support vector machine in particular. Training data for a learning machine is pre-processed in order to add meaning thereto. Pre-processing data may involve transforming the data points and/or expanding the data points. By adding meaning to the data, the learning machine is provided with a greater amount of information for processing. With regard to support vector machines in particular, the greater the amount of information that is processed, the better generalizations about the data that may be derived. The learning machine is therefore trained with the pre-processed training data and is tested with test data that is pre-processed in the same manner. The test output from the learning machine is post-processed in order to determine if the knowledge discovered from the test data is desirable. Post-processing involves interpreting the test output into a format that may be compared with the test data. Live data is pre-processed and input into the trained and tested learning machine. The live output from the learning machine may then be post-processed into a computationally derived alphanumerical classifier for interpretation by a human or computer automated process.
","1. A system for providing data analysis services using a support vector machine for analyzing data received from a remote source (1202), the system comprising:
a server (1206) in communication with a distributed network (1204) for receiving a dataset and an account identifier from a remote source (1202), the remote source (1202) also in communication with the distributed network (1204), wherein the server (1206) is further operable for communicating with an institution (1210) for conducting a financial transaction in order to receive payment from an account identified by the account identifier;
one or more storage devices (1110) in communication with the server (1206) for storing the dataset;
a processor (1104a-1104t; 1214a, 1214b) for executing a support vector machine, characterized by the processor (1104a-1104t; 1214a, 1214b) further operable for:
training and testing the support vector machine using a training dataset to provide a trained support vector machine, wherein the training dataset is pre-processed to add dimensionality to the data by expanding each of the plurality of data points;
collecting (226) the dataset received from the remote source (1202);
pre-processing (228) the dataset from the remote source (1202) to add dimensionality to the data;
inputting (230) the dataset from the remote source (1202) into the trained and tested support vector machine to produce (232) an output comprising a recognized pattern within the dataset from the remote source (1202); and
transmitting the output to the remote source (1202) or another remote source after ensuring that payment from the account have been secured.",Support vector machine
Methods and apparatus for building a support vector machine classifier,"Solving a quadratic programming problem involved in training support vector machines by sweeping through a set of training examples, solving small sub-problems of the quadratic programming problem. Each of these sub-problems has an analytic solution, which is faster that the numerical quadratic programming solutions used in the prior art. In one embodiment, training examples with non-optimal Lagrange multipliers are adjusted, one at a time, until all are optimal (e.g. until all examples fulfill the Kuhn-Tucker conditions). In another embodiment, training examples with non-optimal Lagrange multipliers are paired and then adjusted, until all are optimal.","1. A method for operating a machine to train an object classifier based on a set of object training examples represented by feature vectors (x.sub.i) and predetermined classifications (y.sub.i) associated with the each of the object training examples, where the object classifier determines a classification output (O) of an object represented by feature vector (x.sub.input) as: ##EQU14##
where .alpha..sub.i is a Lagrange multiplier associated with the i.sup.th training example, and k is a kernel function, the method comprising steps of:
a) initializing, with the machine, each of the Lagrange multipliers;
b) sweeping through at least some of the training examples of the set of training examples;
c) for each training example of the sweep, correcting, with the machine, a Lagrange multiplier associated with the training example to generate a corrected Lagrange multiplier, if the Lagrange multiplier is not optimal;
d) storing, with the machine, the corrected Lagrange multiplier; and
e) repeating steps (b), (c) and (d) if any Lagrange multiplier was changed during the sweep, thereby training the object classifier.",Support vector machine
Handset identifier using support vector machines,"A system for identifying a handset used over a communication network comprises a composite database including training data for a plurality of handset types, a plurality of support vector machines trained to identify, respectively, at least one of the plurality of handset types, and an interface operable to receive audio testing data for an unidentified handset. Each support vector machine is configured to determine its degree of recognition of the audio testing data. The system also includes a processor configured to identify unidentified handset by determining the support vector machine exhibiting the highest degree of recognition.","1. A method for training a computer-implemented classification system to be able to identify a handset used over a communication network, comprising:
(a) transforming training data for a plurality of handset types into a composite dataset including training feature vectors;
(b) configuring a plurality of classifiers based on said composite dataset, including:
(1) associating one of said classifiers with one of said handset types not previously associated with any other of said classifiers;
(2) training said classifier of (1) to recognize, within said composite dataset, (A) a first class of training feature vectors related to said associated handset type, and (B) a second class of training feature vectors related to other handset types;
(3) repeating (1) and (2) for at least another of said classifiers; and
(c) storing a result of (b) in a computer-readable memory so as to be usable to (i) correlate an unidentified handset's test feature vectors against said trained classifiers, (ii) select one of said classifiers exhibiting the greatest correlation, and (iii) determine said handset type associated with said selected classifier.",Support vector machine
Gradient based training method for a support vector machine,"A training method for a support vector machine, including executing an iterative process on a training set of data to determine parameters defining the machine, the iterative process being executed on the basis of a differentiable form of a primal optimization problem for the parameters, the problem being defined on the basis of the parameters and the data set.","1. A training method for a support vector machine to perform data classification for a relationship between a training set of data, the method executed by a computer system, including executing an iterative process by a processor on the training set of data read from a data input device to determine parameters defining said machine represented by:
y=sgn(wÂ·x+Î²b),
where y is the classification output which is output by a data output device, x is the input data read from the data input device, Î²is 0 or 1, the vector w and bias b, being parameters defining a decision surface, said iterative process being executed by the processor based on a derivative optimization function for said parameters and said data set.",Support vector machine
Method and Apparatus for Transductive Support Vector Machines,"Disclosed is a method for training a transductive support vector machine. The support vector machine is trained based on labeled training data and unlabeled test data. A non-convex objective function which optimizes a hyperplane classifier for classifying the unlabeled test data is decomposed into a convex function and a concave function. A local approximation of the concave function at a hyperplane is calculated, and the approximation of the concave function is combined with the convex function such that the result is a convex problem. The convex problem is then solved to determine an updated hyperplane. This method is performed iteratively until the solution converges.","1. A method for training a transductive support vector machine, based on a non-convex transductive support vector machine objective function, the transductive support vector machine objective function based on two ramp loss functions for unlabeled test data, the method comprising:
decomposing each of the ramp loss functions into a difference of hinge losses, with a computer processor;
using the differences of the hinge losses to decompose the transductive support vector machine objective function into a sum of a concave function and a convex function, with a computer processor; and
determining a hyperplane classifier classifying the unlabeled data into first and second classes with a computer processor by iteratively approximating the concave function.",Support vector machine
CLASSIFICATION USING SUPPORT VECTOR MACHINES AND VARIABLES SELECTION,"A method of deriving a classifier for classifying items using a plurality of variables for characteristics of the items, the method comprising determining a representative subset of the variables for use in said classifier.","1. A method of deriving a classifier for classifying currency bills using a plurality of variables for characteristics of the bills, the method comprising:
determining a representative subset of the plurality of variables for use in said classifier by using a first classifier and the plurality of variables to select the representative subset of the plurality of variables based on a fitness criteria describing a correlation of at least some of the plurality of variables with input data projected on a discriminant axis of the first classifier;
wherein the bills are classified between a reference class which is one billway of a denomination and a second class containing at least 3 other billways of the same denomination.",Support vector machine
Parallel ensemble of support vector machines,"Systems, methods, and computer-readable media for building ensemble members of a Support Vector Machine (SVM) ensemble in parallel and executing processing in parallel on data allocated to each ensemble member are disclosed. The parallel construction and processing of data of each ensemble member allows a single large SVM calculation to be replaced with many smaller SVM calculations performed in parallel, and thus, may reduce the computational resources required to classify datasets.","1. A method for constructing a Support Vector Machine (SVM) ensemble, the method comprising:
determining a subset of data in a training dataset to allocate to an ensemble member of the SVM ensemble; and
building the ensemble member, wherein building the ensemble member comprises:
partitioning a first portion of the subset of data into a first classification and a second portion of the subset of data into a second classification;
determining a first plurality of data clusters among the first portion of the subset of data partitioned into the first classification and a second plurality of data clusters among the second portion of the subset of data partitioned into the second classification;
identifying a first cross-classification cluster matchup, wherein the first cross-classification cluster matchup comprises a first data cluster of the first plurality of data clusters and a second data cluster of the second plurality of data clusters;
identifying a second cross-classification cluster matchup, wherein the second cross-classification cluster matchup comprises a third data cluster of the first plurality of data clusters and a fourth data cluster of the second plurality of data clusters; and
determining a first SVM for the first cross-classification cluster matchup and a second SVM for the second cross-classification cluster matchup,
wherein the first SVM and the second SVM are determined at least partially in parallel.",Support vector machine
Support vector machine prediction method,A computer-implemented method is disclosed for image recognition and other applications. The method employs an SVM model and can reduce false negatives and increase recognition accuracies by raising the sample-to-support-vector ratio.,"1. A computer-implemented method, comprising:
selecting a kernel and kernel parameters for a first Support Vector Machine (SVM) model comprising SVM support vector number of support vectors;
testing the first SVM model on a feature matrix T to produce false positive (FP) data set and false negative (FN) data set by a computer system, wherein the feature matrix T includes n feature vectors of length m, wherein n and m are integer numbers;
copying the feature matrix T to produce a feature matrix T_best comprising T_best sample number of sample points;
checking, by the computer system, if a ratio (T_best sample number)/(SVM support vector number) is above a threshold for the first SVM model on T_best; and
if the ratio is above the threshold, performing SVM predictions using the first SVM model on the feature matrix T_best.",Support vector machine
DEEP NEURAL SUPPORT VECTOR MACHINES,"Aspects of the technology described herein relate to a new type of deep neural network (DNN). The new DNN is described herein as a deep neural support vector machine (DNSVM). Traditional DNNs use the multinomial logistic regression (softmax activation) at the top layer and underlying layers for training. The new DNN instead uses a support vector machine (SVM) as one or more layers, including the top layer. The technology described herein can use one of two training algorithms to train the DNSVM to learn parameters of SVM and DNN in the maximum-margin criteria. The first training method is a frame-level training. In the frame-level training, the new model is shown to be related to the multi-class SVM with DNN features. The second training method is the sequence-level training. The sequence-level training is related to the structured SVM with DNN features and HMM state transition features.","1. An automatic speech recognition (ASR) system comprising:
a processor; and
computer storage memory having computer-executable instructions stored thereon which, when executed by the processor, implement an acoustic model and a language model:
an acoustic sensor configured to convert speech into acoustic information;
the acoustic model (AM) comprising a deep neural support vector machine configured to classify the acoustic information into a plurality of phones; and
the language model (LM) configured to convert the plurality of phones into plausible word sequences.",Support vector machine
CONSTRUCTING AND USING SUPPORT VECTOR MACHINES,"
Methods and systems are described for building and using a support vector machine for classifying a new sample. Training samples of one class or another class are used to build the machine by mapping the angle space to a set of angle vectors and, for each angle vector, finding candidate hyperplanes that are orthogonal to a vector at the angle vector and radiating from an origin point to the hyperplane. An optimal pair of candidate hyperplanes at one of the angle vectors is selected on the basis of the distance between the pair and the number of samples between them. The selection may be based on hard margin or soft margin approaches. A matrix-based implementation is presented. New training samples may be added, removed, or reclassified without requiring recalculation of the entire support vector machine.

","1. A method of classifying a portion of an image or video using a classifier, the classifier being based upon a set of samples (12, 14) in a data space (10), wherein each sample of the set of samples comprises classified image data, wherein the set includes a plurality of samples (12) of one class and a plurality of samples (14) of another class, and wherein the portion of the image or video comprises a new sample in the data space, the method comprising:
training the classifier by:
mapping (102) an angle space to a fixed set of angle vectors and, for each angle vector,
finding (302), for each sample, a candidate hyperplane associated with that angle vector, wherein each candidate hyperplane is a radial distance from an origin point measured along a vector from the origin and normal to the candidate hyperplane, and wherein the direction of the vector is given by the angle vector in the data space;
selecting (220, 304), as an optimal pair of hyperplanes, a pair of the candidate hyperplanes associated with one angle vector of the first set of angle vectors on the basis of a distance between pairs of the candidate hyperplanes parallel to each other and the number of samples between the candidate hyperplanes; and
determining (112, 222, 306) a classification hyperplane as a hyperplane parallel to and between the optimal pair of hyperplanes, and
determining (224, 308) whether the portion of the image or video is in the one class or in the another class based on which side of the classification hyperplane the new sample is located.",Support vector machine
Ensemble weak support vector machines,"In an approach to improving accuracy through weak model aggregation, one or more computer processors generating a plurality of hyperparameter sets, wherein each hyperparameter set in the plurality of hyperparameter sets contains one or more hyperparameters varied to increase over-training in one or more models, wherein over-training includes overfitting or underfitting. The one or more computer processors create a plurality of weak models utilizing a created bootstrap dataset in a plurality of created bootstrap datasets, a corresponding extracted explanatory variable set, and a corresponding hyperparameter set in the generated plurality of hyperparameter sets, wherein each weak model in a created plurality of weak models shares at least the created bootstrap dataset, the extracted explanatory variable set, the generated hyperparameter set, a machine learning technique, or a model architecture. The one or more computer processors predict a classification for an unknown datapoint by aggregating the created plurality of weak models.","1. A computer-implemented method comprising:
generating, by one or more computer processors, a plurality of hyperparameter sets, wherein each hyperparameter set in the plurality of hyperparameter sets contains one or more hyperparameters varied to increase model inaccuracy through over-training in one or more models, over-training includes overfitting or underfitting;
creating, by one or more computer processors, a plurality of weak models utilizing a created bootstrap dataset in a plurality of created bootstrap datasets, a corresponding extracted feature set, and a corresponding hyperparameter set in the generated plurality of hyperparameter sets, wherein each weak model in a created plurality of weak models shares at least the created bootstrap dataset, the extracted feature set, the generated hyperparameter set, a machine learning technique, or a model architecture; and
predicting, by one or more computer processors, a classification for an unknown datapoint by aggregating the created plurality of weak models.",Support vector machine
SUPPORT VECTOR MACHINE COMPUTATION,"A technique solves an SVM problem on table J, defined as the join of two tables T1 and T2, without explicitly joining the tables T1 and T2, in which the table T1 has m rows (piT, uiT), i=1, . . . , m, and the table T2 has n rows (qjT, vjT), j=1, . . . , n. A computer obtains a modified optimization problem from a primal optimization problem in which the modified optimization problem includes minimizew,b,Î·,Î¶Â½âˆ¥ï¸€wâˆ¥ï¸€2+CÂ·Î£i=1mJ(i)Â·Î·i+CÂ·Î£j=1nI(j)Â·Î¶j, subject to yixijTwâˆ’yib+Î·i+Î¶jâ‰§1 ((i,j)âˆˆIJ) and Î·i, Î¶jâ‰§0. The penalty variables are reduced in the modified optimization problem by replacing the penalty variables in a form of Î¾ij for each (i,j)âˆˆIJ with the penalty variables in a form of Î¶ij=Î·i+Î¶j. A compact form of the modified optimization problem is obtained which includes minimizew,b,Î·,Î¶,Ïƒ,Ï„Â½âˆ¥ï¸€wPâˆ¥ï¸€2+Â½âˆ¥ï¸€wUâˆ¥ï¸€2+Â½âˆ¥ï¸€wQâˆ¥ï¸€2+CÂ·Î£i=1mJ(i)Â·Î·i+CÂ·Î£j=1nI(j) Â·Î¶j which is subject to yipiTwPâˆ’yib+Î¾iâˆ’Ïƒkâ‰§0 (iâˆˆIk, k=1, . . . l), qjTwQâˆ’Ï„kâ‰§0 (jâˆˆJk, k=1, . . . l), Ïƒk+zkTwU+Ï„kâ‰§1 (for k=1, . . . l such that Jkâ‰ î‹“), ÏƒkzkTwUâ‰§1 (for k=1, . . . l such that Jk=î‹“), and Î¾iâ‰§0 (i=1, . . . , m). The compact form of the modified optimization problem is solved.","1. A method, by a computer, of solving a support vector machine problem on table J, defined as the join of two tables T1 and T2, without explicitly joining the tables T1 and T2, wherein the table T1 has m rows (piT, uiT), i=1, . . . , m, and the table T2 has n rows (qjT, vjT), j=1, . . . , n, the method comprising:
providing a primal optimization problem over a join of the tables T1 and T2;
obtaining, by the computer, a modified optimization problem from the primal optimization problem;
reducing penalty variables in the modified optimization problem by replacing the penalty variables in a form of Î¾ij for each (i,j)âˆˆIJ with the penalty variables in a form of Î¾ij=Î·i+Î¶j;
obtaining a compact form of the modified optimization problem in which the compact form comprises the penalty variables in the form of Î¾ijÎ·iÎ¶j; and
solving the compact form of the modified optimization problem.",Support vector machine
System and method for a contiguous support vector machine,"A method of classifying features in digitized images includes providing a plurality of feature points in an n-dimensional space, wherein said feature points have been extracted from a digitized medical image, formulating a support vector machine to classify said feature point into one of two sets, wherein each said feature classification vector is transformed by an adjacency matrix defined by those points that are nearest neighbors of said feature, and solving said support vector machine by a linear optimization algorithm to determine a classifying plane that separates the feature vectors into said two sets.","1. A method of classifying features in digitized images comprising the steps of: 
providing a plurality of feature points in an n-dimensional space, wherein said feature points have been extracted from a digitized medical image; 
formulating a support vector machine to classify said feature point into one of two sets, wherein each said feature classification vector is transformed by an adjacency matrix defined by those points that are nearest neighbors of said feature; and 
solving said support vector machine by a linear optimization algorithm to determine a classifying plane that separates the feature vectors into said two sets. ",Support vector machine
Proximal gradient method for huberized support vector machine,"The Support Vector Machine (SVM) has been used in a wide variety of classification problems. The original SVM uses the hinge loss function, which is nondifferentiable and makes the problem difficult to solve in particular for regularized SVMs, such as with l1-norm. The Huberized SVM (HSVM) is considered, which uses a differentiable approximation of the hinge loss function. The Proximal Gradient (PG) method is used to solving binary-class HSVM (BHSVM) and then generalized to multi-class HSVM (MHSVM). Under strong convexity assumptions, the algorithm converges linearly. A finite convergence result about the support of the solution is given, based on which the algorithm is further accelerated by a two-stage method.","1. A computer-implemented method comprising:
receiving, by a processor, medical image data;
training a machine learning model for classifying the medical image data, wherein the machine learning model is generated at least in part by:
replacing a hinge loss term of a support vector machine model of a classifier with a differentiable approximation of the hinge loss term based on a Huber loss function, to produce the machine learning model, wherein a vicinity near a non-smooth point in the hinge loss term of the support vector machine model is approximated by a differentiable curve in the Huber loss function;
applying a proximal gradient method to solve the differentiable support vector machine model, to produce updates for an algorithm for determining a separating hyperplane of the differentiable support vector machine model; and
determining the separating hyperplane using the algorithm;
extracting characteristics of the medical image data to form a feature vector; and
determining one or more medical conditions associated with the medical image data by processing the feature vector using the machine learning model.",Support vector machine
Determining states of an apparatus using support vector machines,"The invention relates to a system and to a method for determining a state of a device by means of a trained support-vector machine. According to the invention, an operating parameter space is divided into classification volumes, at least one of which indicates a normal state and at least one other of which indicates a fault state of the device. A current state of the device can therefore be determined by determining where a current operating parameter point is to be arranged in the operating parameter space. The invention further relates to methods and to variants of the system in order to facilitate a cause evaluation and to determine particularly relevant operating parameters for the fault determination.","1. A system for determining a state of an apparatus, the system comprising:
a capture device configured to capture at least two operating parameters of the apparatus during operation of the apparatus; and
a computing device configured to implement an operating point module, a trained support vector machine (SVM), and an output module,
wherein the operating point module is configured to generate an operating point in an n-dimensional operating parameter space from the at least two captured operating parameters, where n is greater than or equal to two,
wherein the trained SVM is configured and trained to divide the n-dimensional operating parameter space into at least three classification volumes, each of the at least three classification volumes indicating different states of the apparatus,
wherein a first classification volume indicates a normal state of the apparatus, and a second classification volume and a third classification volume indicate different fault states of the apparatus,
wherein the trained SVM is further configured to assign the operating point generated by the operating point module to one classification volume of the at least three classification volumes,
wherein the output module is configured to:
determine a state of the apparatus according to the one classification volume to which the generated operating point is assigned by the trained SVM; and
output an output signal indicating at least the determined state of the apparatus, and
wherein the computing device is further configured to implement an evaluation module, the evaluation module being configured to:
determine a respective normal vector to every plane or hyperplane that separates the first classification volume, which identifies the normal state of the apparatus, from one of the classification volumes that indicate a fault state of the apparatus; and
determine and output, for each of the determined normal vectors, a value of an entry with a greatest absolute value for the normal vector.",Support vector machine
Text classification by weighted proximal support vector machine,"Embodiments of the invention relate to improvements to the support vector machine (SVM) classification model. When text data is significantly unbalanced (i.e., positive and negative labeled data are in disproportion), the classification quality of standard SVM deteriorates. Embodiments of the invention are directed to a weighted proximal SVM (WPSVM) model that achieves substantially the same accuracy as the traditional SVM model while requiring significantly less computational time. A weighted proximal SVM (WPSVM) model in accordance with embodiments of the invention may include a weight for each training error and a method for estimating the weights, which automatically solves the unbalanced data problem. And, instead of solving the optimization problem via the KKT (Karush-Kuhn-Tucker) conditions and the Sherman-Morrison-Woodbury formula, embodiments of the invention use an iterative algorithm to solve an unconstrained optimization problem, which makes WPSVM suitable for classifying relatively high dimensional data.","1. A system for training a text classifier, the system comprising: 
a text data preprocessor that preprocesses raw training text to produce an input matrix; 
a weighting module that generates a weighted matrix by re-weighting the input matrix based on how many training examples are positive and how many training examples are negative; and 
a model-vector generator that iteratively calculates a model vector based on the weighted matrix. ",Support vector machine
Q-METRIC BASED SUPPORT VECTOR MACHINE,"A Support Vector Machine ( 110 ) with a Q-Metric kernel function computer ( 112 ) is provided. The Support Vector Machine ( 110 ) exhibits improved performance for classification and regression. Pattern recognition systems ( 100,900 ) that use the Support Vector Machine ( 110 ) are also provided. A Differential Evolution method of training a Support Vector Machine is also provided.","1. A pattern recognition system comprising:
a sensor for collecting data from a subject to be identified;
a feature vector extractor coupled to said sensor, said feature vector extractor adapted to receive information from said sensor and produce a feature vector characterizing said subject;
a Q-Metric computer adapted to compute Q-Metric distances between said feature vector characterizing said subject and a plurality of exemplar feature vectors;
a Support Vector Machine coupled to said Q-Metric computer wherein said Support Vector Machine is adapted to assign said feature vector characterizing said subject to a classification based on said Q-Metric distances.",Support vector machine
RE-LEARNING METHOD FOR SUPPORT VECTOR MACHINE,"A re-learning method includes: a step of learning an SVM by using a set of training samples for initial learning which have known labels; a step of perturbation-processing the training samples for initial learning; a step of using the perturbation-processed sample as a training sample for addition; and a step of re-learning the learned SVM by using the training sample for initial learning and the training sample for addition. For the training samples for initial learning to be perturbation-processed, a training sample obtained by removing a training sample for initial learning corresponding to a non-support vector, a training sample corresponding to a support vector existing on a soft margin hyperplane, etc., may be used.","1. A re-learning method for a support vector machine, comprising:
a step of learning an SVM by using a set of training samples for initial learning which have known labels;
a step of perturbation-processing the training samples for initial learning;
a step of using the perturbation-processed sample as a training sample for addition; and
a step of re-learning the learned SVM by using the training sample for initial learning and the training sample for addition.",Support vector machine
CLASSIFICATION METHOD BASED ON SUPPORT VECTOR MACHINE,"Provided is a classification method based on a support vector machine, which is effective for a small amount of training data. The classification method based on a support vector machine includes building a first classification model by applying a weight value based on a geometrical distribution of an input feature vector, building a second classification model, based on a classification uncertainty of the input feature vector, and merging the first classification model and the second classification model to perform dual optimization.","1. A classification method based on a support vector machine, the classification method comprising:
(a) building a first classification model by applying a weight value based on a geometrical distribution of an input feature vector;
(b) building a second classification model, based on a classification uncertainty of the input feature vector; and
(c) merging the first classification model and the second classification model to perform dual optimization.",Support vector machine
LEARNING METHOD FOR SUPPORT VECTOR MACHINE,"A plural number of training vectors are randomly selected from a total of unused training vectors, and from among the selected training vectors, a vector having the largest error amount is extracted. Subsequently, the extracted vector is added to the already used training vector so as to update the training vector, and the updated training vector is used to learn the SVM. When the largest error amount becomes smaller than a certain setting value epsilon or when the already used training vector becomes larger than a certain value m, learning of a first phase is stopped. In learning of a second phase, the learning is performed on a predetermined number of or all of the training vectors having a large error amount.","1. A learning method for a support vector machine (hereinafter, SVM), comprising:
a step of selecting two training vectors from two opposite classes to learn an SVM;
a step of arbitrarily selecting a plurality of unused training vectors from a set of previously prepared training vectors to extract an unused training vector having a largest error amount;
a step of adding the extracted unused training vector to an already used training vector to update the training vector;
a step of learning the SVM by using the updated training vector; and
a step of stopping the learning when the number of updated training vectors is equal to or more than a predetermined number or when an error amount of the extracted unused training vector is smaller than a predetermined value.",Support vector machine
System and method for electric load identification and classification employing support vector machine,A method identifies electric load types of a plurality of different electric loads. The method includes providing a support vector machine load feature database of a plurality of different electric load types; sensing a voltage signal and a current signal for each of the different electric loads; determining a load feature vector including at least six steady-state features with a processor from the sensed voltage signal and the sensed current signal; and identifying one of the different electric load types by relating the load feature vector including the at least six steady-state features to the support vector machine load feature database.,"1. A method of identifying electric load types of a plurality of different electric loads, said method comprising:
providing a support vector machine load feature database of a plurality of different electric load types;
sensing a voltage signal and a current signal for each of said different electric loads;
determining a load feature vector including at least six steady-state features with a processor from said sensed voltage signal and said sensed current signal; and
identifying one of said different electric load types by relating the load feature vector including the at least six steady-state features to the support vector machine load feature database.",Support vector machine
User preference techniques for support vector machines in content based image retrieval,"Searching multimedia information which allows determining preferences based on very little amounts of data. The preferences are nonparametrically determined. Each preference is quantized into one of a plurality of bins. By doing the quantization, the distances between positive and negative samples are increased. The quantization amount may change depending on the number of samples which are used. The quantization can be used in a support vector machine or the like.","1. A method comprising:
Receiving a query to search for multimedia content;
Identifying items of multimedia content, the identified items determined to be likely relevant to the received query;
In response to the receiving, providing the identified items of multimedia content;
Obtaining data indicative of positive matches and negative matches from the identified items of multimedia content, a positive match indicative that an identified item likely matches a result to the search query, a negative match indicative that an identified item likely does not match the result to the search query, wherein the positive matches and the negative matches are selected based on distances obtained by mapping the data to a Euclidean space;
Iteratively training a system to provide the result of the search, wherein, in a first iteration, the system is trained based on the positive matches and the negative matchers selected based on the distances obtained by mapping the data to the Euclidean space;
Wherein information that is learned from the training is arranged into a multi-dimensional feature vector having components with labels, wherein a marginal probability of each label for each component of the feature vector is defined as
{P(y=+1|Ï‡1),P(y=âˆ’1|Ï‡1)}.
Where x1 is the l-th component of the feature vector;
Wherein the probabilities
{P(y=+1|Ï‡1),P(y=âˆ’1|Ï‡1)}.
Are estimated by counting a number of matches that fall in each bin
{P(y=+1|Ï‡1), P(y=âˆ’1|Ï‡1)}.
Where the indicator function l(.) takes value one when its argument is true and zero otherwise. l is the number of labeled training data, Xil the l-th component of training vector xi, Î”lk is the size of quantization interval along dimension l centered at reconstruction value rlk;
Obtaining a non-parametric model of statistics based on information divergence.",Support vector machine
ACCELERATION OF SPARSE SUPPORT VECTOR MACHINE TRAINING THROUGH SAFE FEATURE SCREENING,"A system for machine training can comprise one or more data processors and a non-transitory computer-readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform operations including: accessing a dataset comprising data tracking a plurality of features; determining a series of values for a regularization parameter of a sparse support vector machine model, the series including an initial regularization value and a next regularization value; computing an initial solution to the sparse support vector machine model for the initial regularization value; identifying, using the initial solution, inactive features of the sparse support vector machine model for the next regularization value; and computing a next solution to the sparse support vector machine model for the next regularization value, wherein computing the next solution includes excluding the inactive features.","1. A system, comprising:
one or more data processors; and
a non-transitory computer-readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform operations including:
accessing a dataset comprising data tracking a plurality of features;
determining a series of values for a regularization parameter of a sparse support vector machine model, the series including an initial regularization value and a next regularization value, the sparse support vector machine model including a dual form defined by a dual variable;
computing an initial solution to the sparse support vector machine model for the initial regularization value;
identifying, using the initial solution, inactive features of the sparse support vector machine model for the next regularization value, wherein identifying an inactive feature includes determining an upper bound for an inner product of a weighted feature and the dual variable for the next regularization value and identifying the weighted feature as an inactive feature when the upper bound is less than one; and
computing a next solution to the sparse support vector machine model for the next regularization value, wherein computing the next solution includes excluding the inactive features.",Support vector machine
Method and apparatus for performing extraction on an integrated circuit design with support vector machines,"The present invention introduces novel methods of performing integrated circuit layout extraction. In the system of the present invention, a complex extraction problem is first broken down into a set of smaller extraction sub problems. Some of the smaller extraction sub problems may be handled by simple parametric models. However, for the frequent complex extraction sub problems, machine learning is used to build models. Specifically, Support Vector Machines are constructed to extract the desired electrical characteristics. To build the Support Vector Machines, Experimental design is employed to select a set of training points that provide the best information. In one embodiment, the training point set is created by creating a critical input spanning set, adding training points from critical regions in the input space, and adding training points from frequently encountered profile cases. The training point set is then used to train the Support Vector Machine that will extract electrical characteristics for the extraction sub problem.","1. A method of constructing a model for estimating electrical characteristics for an extraction sub problem, said method comprising:
identifying a set of physical parameters of a set of physical measurements that define said extraction sub problem;
selecting a set of training cases for said specific extraction sub problem, each of said training cases including an associated set of said physical measurements;
solving said specific extraction sub problem for each of said training cases using said associated set of physical measurements as an input to an accurate physics based model to generate an associated output; and
training a support vector machine using said associated set of physical measurements and associated output as training data.",Support vector machine
Efficient polynomial mapping of data for use with linear support vector machines,"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for polynomial mapping of data for linear SVMs. In one aspect, a method includes training a linear classifier by receiving feature vectors and generating a condensed representation of a mapped vector corresponding to a polynomial mapping of each feature vector, the condensed representation including an index into a weight vector for each non-zero component of the mapped vector. A linear classifier is trained on the condensed representations. In another aspect, a method includes receiving a feature vector, identifying non-zero components resulting from a polynomial mapping of the feature vector, and mapping the combination of one or more elements of each non-zero component to a weight in a weight vector to determine a set of weights. The feature vector is classified according to a classification score derived by summing the set of weights.","1. A computer-implemented method, comprising:
receiving, in a computing system comprising one or more computers, original training data including a plurality of training feature vectors and a respective decision for each training feature vector, each training feature vector representing a parse state of a parser, each element of each training feature vector indicating a presence or absence of a feature of the parse state, the parser constructing a dependency graph for a sentence having words and punctuation, the parser input comprising tokens corresponding to the words and punctuation of the sentence;
generating, by the computing system, for each training feature vector, a condensed representation of a mapped vector corresponding to a degree-d polynomial mapping of the training feature vector, the mapping defining each component of the mapped vector as either a single element of the training feature vector or a product of up to d elements of the training feature vector, the condensed representation of the mapped vector including an index for each non-zero component of the mapped vector, each index indicating a position in a weight vector of a weight for the corresponding non-zero mapped vector component; and
using a linear support vector machine to train a classifier on the condensed representations, the linear support vector machine configured to receive as training data the condensed representations and decisions for the training feature vectors corresponding to the condensed representations, and determine the weights in the weight vector from the condensed training data, where the classifier corresponds to a given parsing transition and uses the weight vector that was trained for the given parsing transition to generate a score for the given parsing transition, where the score for the given parsing transition is used to determine whether to perform the given parsing transition in building a dependency graph, the dependency graph representing syntactic modifiers for words in the sentence through labeled directed edges.",Support vector machine
Reservoir properties prediction with least square support vector machine,"Subsurface reservoir properties are predicted despite limited availability of well log and multiple seismic attribute data. The prediction is achieved by computer modeling with least square regression based on a support vector machine methodology. The computer modeling includes supervised computerized data training, cross-validation and kernel selection and parameter optimization of the support vector machine. An attributes selection technique based on cross-correlation is adopted to select most appropriate attributes used for the computerized training and prediction in the support vector machine.","1. A computer implemented method of modeling a reservoir property of subsurface reservoir structure by support vector machine processing in the computer of input data available from the reservoir to form measures of the reservoir property at regions of interest in the subsurface reservoir by regression analysis of the available input data, the method comprising the computer processing steps of:
(a) receiving training input data about subsurface attributes from seismic survey data obtained from seismic surveys of the reservoir;
(b) receiving training target data about formation rock characteristics from data obtained from wells in the reservoir;
(c) partitioning the subsurface attributes training data and the formation rock characteristics training target data into a plurality of subsets;
(d) selecting formation attribute parameters for support vector machine modeling by performing the steps of:
(1) cross-validating the subsets of subsurface attributes training data each with the other subsets of the plurality of subsets for a radial based kernel function pair comprising a kernel parameter value and a penalty parameter pair value;
(2) forming an error function for each of the cross-validated subsets;
(3) repeating the steps of cross-validating the subsets of subsurface attributes training data and forming an error function for a plurality of different radial based kernel function pairs;
(e) optimizing the selected formation attribute parameters by determining a minimum error function of the formed error functions for the plurality of different radial based kernel function pairs;
(f) providing the training data, the selected formation attribute parameters, the cross-validated subsets of subsurface attributes training data, and the error functions for the plurality of radial based function kernel pairs as training inputs for support vector machine modeling;
(g) performing support vector machine modeling by regression analysis to determine a minimum error function of the error functions of the provided training inputs;
(h) predicting the reservoir property based on the support vector modeling of the training inputs; and
(i) forming an output display of the predicted reservoir property.",Support vector machine
Classification method of labeled ordered trees using support vector machines,"To achieve classification of semistructured data with a Kernel method for labeled ordered trees, instances having a labeled ordered tree structure are input and their inner product is computed, the result of which is used for classification learning of the instances. In the inner product computation, a sum of matches is computed for descendant nodes of non-leaf nodes of the labeled ordered trees by applying dynamic programming based on correspondence in which order of the nodes is maintained.","1. A data processing method for controlling a computer to classify XML semi-structured data instances in a database, said XML semi-structured data instances each having a vector representable structure having vector representable substructures as attributes, the method comprising: a first step of inputting the vector representable structure instances including their substructure instances; and a second step of successively computing an inner product of successive vector representable structure instances including their substructure instances and storing the successively computed inner products, the method further comprising the steps of: based on structures of positive instances and structures of negative instances stored in memory, obtaining a classification rule for classifying the positive instances and the negative instances and storing the classification rule in memory, each structure of the positive instances being a labeled ordered tree in which a node indication mark is added to a node to be indicated, and each structure of the negative instances being a labeled ordered tree in which a node indication mark is added to a node not to be indicated; adding a node indication mark to a node in the labeled ordered tree to be processed and classifying the labeled ordered tree as positive or negative based on the classification rule stored in the memory; and in response to the classification result, outputting a processing result of the labeled ordered tree with the node indication mark added to a node, to thereby create a classification rule; wherein the first step further comprises, if the given substructures includes lower substructures, computing a sum of matches for the substructures.",Support vector machine
OBJECT CLASSIFICATION WITH CONSTRAINED MULTIPLE INSTANCE SUPPORT VECTOR MACHINE,"This disclosure provides method and systems of classifying a digital image of an object. Specifically, according to one exemplary embodiment, an object classifier is trained using a constrained MI-SVM (multiple instance-support vector machine) approach whereby training images of objects are sampled to generate a collection of image regions associated with an object type and viewpoint, and the classifier is trained to determine an appropriate mid-level representation of the training image which is discriminative.","1. A computer implemented method of classifying a digital image of an object, the method comprising:
a) receiving a digital image of an object to be classified with a processor; and
b) classifying the digital image with a constrained multiple-instance support vector machine (MI-SVM) classifier, the constrained MI-SVM classifier having been automatically trained using a plurality of training images, the training images including a plurality of object types from a plurality of viewpoints, each training image including an image of an object associated with one of the plurality of object types and one of the plurality of object viewpoints, an associated object type label and an associated viewpoint label, the constrained MI-SVM classifier trained by sampling each training image to generate a bag of image regions associated with each training image, discovering a discriminative image region associated with each training image, and generating a collection of discriminative image regions for each of the plurality of object types and each of the plurality of viewpoints,
wherein the constrained MI-SVM classifier is trained using an iterative process that initially selects an initial discriminative image region for a first object type at the plurality of viewpoints and iteratively selects subsequent discriminative image regions of the first object type at the plurality of viewpoints where the selection of a subsequent discriminate image region is constrained by one or more characteristics of selected discriminative image regions associated with other viewpoints of the first object type.",Support vector machine
Pre-processing input data with outlier values for a support vector machine,"A system and method for preprocessing input data to a support vector machine (SVM). The SVM is a system model having parameters that define the representation of the system being modeled, and operates in two modes: run-time and training. A data preprocessor preprocesses received data in accordance with predetermined preprocessing parameters, and outputs preprocessed data. The data preprocessor includes an input buffer for receiving and storing the input data. The input data may include one or more outlier values. A data filter detects and removes any outlier values in the input data, generating corrected input data. The filter may optionally replace the outlier values in the input data. An output device outputs the corrected data from the data filter as preprocessed data. The corrected data may be input to the SVM in training mode to train the SVM, and/or in run-time mode to generate control parameters and/or predictive output information.","1. A data preprocessor for preprocessing input data for a support vector machine, wherein the input data include one or more outlier values, comprising:
an input buffer which is operable to receive and store the input data wherein the input data comprise run-time data;
a data filter which is operable to detect and remove said one or more outlier values, thereby generating corrected input data, wherein said corrected input data comprise corrected run-time data; and
an output device for outputting the corrected input data, said corrected input data comprising the input data to the support vector machine;
wherein the support vector machine comprises a non-linear model having a set of model parameters defining a representation of a system, wherein said model parameters of said support vector machine have been trained to represent said system; and
wherein the support vector machine is operable to receive said corrected run-time data and generate run-time output data, wherein said run-time output data comprise one or both of control parameters for said system and predictive output information for said system.",Support vector machine
Large scale semi-supervised linear support vector machines,"A computerized system and method for large scale semi-supervised learning is provided. The training set comprises a mix of labeled and unlabeled examples. Linear classifiers based on support vector machine principles are built using these examples. One embodiment uses a fast design of a linear transductive support vector machine using multiple switching. In another embodiment, mean field annealing is used to form a very effective semi-supervised support vector machine. For both these embodiments the finite Newton method is used as the base method for achieving fast training.","1. A computerized method for semi-supervised learning for web page classification, comprising:
receiving a set of web pages as training elements;
labeling some of the elements of the set of training elements that are determined to fall within a classification group, the set of training elements thereby having labeled elements and unlabeled elements;
using selected labeled elements and unlabeled elements as examples in a semi-supervised support vector machine implemented using a mean field annealing method, constructing a continuous loss function from a non-continuous loss function, train a linear classifier;
receiving unclassified web pages; and
classifying the unclassified web pages using the trained linear classifier.",Support vector machine
Detection of Textural Defects Using a One Class Support Vector Machine,"Method for detecting textural defects in an image. The image, which may have an irregular visual texture, may be received. The image may be decomposed into a plurality of subbands. The image may be portioned into a plurality of partitions. A plurality of grey-level co-occurrence matrices (GLCMs) may be determined for each partition. A plurality of second-order statistical attributes may be extracted for each GLCM. A feature vector may be constructed for each partition, where the feature vector includes the second order statistical attributes for each GLCM for the partition. Each partition may be classified based on the feature vector for the respective partition. Classification of the partitions may utilize a one-class support vector machine, and may determine if a defect is present in the image.","1. A method for detecting textural defects in an image, the method comprising:
receiving the image, wherein the image comprises an irregular visual texture;
decomposing the image into a plurality of subbands using wavelet frames;
constructing a plurality of feature vectors for the image, wherein the plurality of feature vectors comprise at least one feature vector from each of the subbands;
classifying the image based on the plurality of feature vectors for the image, wherein said classifying utilizes a one-class support vector machine (SVM), wherein said classifying determines if a textural defect is present in the image.",Support vector machine
Method and System for Analysis of Flow Cytometry Data Using Support Vector Machines,"An automated method and system are provided for receiving an input of flow cytometry data and analyzing the data using one or more support vector machines to generate an output in which the flow cytometry data is classified into two or more categories. The one or more support vector machines utilize a kernel that captures distributional data within the input data. Such a distributional kernel is constructed by using a distance function (divergence) between two distributions. In the preferred embodiment, a kernel based upon the Bhattacharyya affinity is used. The distributional kernel is applied to classification of flow cytometry data obtained from patients suspected having myelodysplastic syndrome.","1. A method for analysis and classification of flow cytometry data, the method comprising:
downloading an input dataset comprising a plurality of assays performed on a plurality of samples by flow cytometry analysis into a computer system comprising a processor and a storage device, wherein the processor is programmed to execute at least one support vector machine and performs the steps of:
pre-processing a first portion of the input dataset with one or more feature selection algorithm to select a subset of assays from the plurality of assays;
training a support vector machine comprising a distributional kernel to separate the first portion of the input dataset corresponding to the selected subset of assays into two probability distributions and measuring a discrepancy between the two probability distributions;
testing the trained support vector machine with a second portion of the input dataset using the selected feature subset to determine whether an optimal solution is achieved;
if an optimal solution is not achieved, repeating the steps of training and testing until the optimal solution is reached;
once an optimal solution is reached, inputting a live data set of flow cytometry data into the computer system;
processing the live data set using the selected subset of assays using the trained and tested support vector machine to produce a result comprising a classification of the flow cytometry data into one of two distinct classes; and
generating an output display at a display device with an identification of a flow cytometry data classification.",Support vector machine
